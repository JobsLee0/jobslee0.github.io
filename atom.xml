<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jobslee0.github.io</id>
    <title>Jobs.Lee&apos;s Blog</title>
    <updated>2023-10-09T14:17:35.596Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jobslee0.github.io"/>
    <link rel="self" href="https://jobslee0.github.io/atom.xml"/>
    <subtitle>Jobs.Lee&apos;s Blog</subtitle>
    <logo>https://jobslee0.github.io/images/avatar.png</logo>
    <icon>https://jobslee0.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Jobs.Lee&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[开源大语言模型汇总]]></title>
        <id>https://jobslee0.github.io/post/kai-yuan-da-yu-yan-mo-xing-hui-zong/</id>
        <link href="https://jobslee0.github.io/post/kai-yuan-da-yu-yan-mo-xing-hui-zong/">
        </link>
        <updated>2023-08-05T09:09:24.000Z</updated>
        <content type="html"><![CDATA[<h1 id="开源大语言模型汇总">开源大语言模型汇总</h1>
<p>原文链接：https://mp.weixin.qq.com/s/BQOJNwfkApiZnFveMDBQ-w</p>
<p><strong><strong><strong>#01</strong></strong></strong></p>
<p><strong>Alpaca/LLaMA（Meta/Stanford）</strong></p>
<p>**斯坦福 Alpaca：**一个遵循指令的 LLaMA 模型。</p>
<p>LLaMA 网站：<a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</a></p>
<p>Alpaca 网站：<a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">https://crfm.stanford.edu/2023/03/13/alpaca.html</a></p>
<p>Alpaca GitHub：<a href="https://github.com/tatsu-lab/stanford">https://github.com/tatsu-lab/stanford</a>_alpaca</p>
<p>能否用于商业用途：不能</p>
<p>以下是基于 Meta 的 LLaMA 项目或斯坦福大学的 Alpaca 项目的复制品或相关项目：</p>
<p><strong>Alpaca.cpp</strong></p>
<p>在你的设备上本地快速运行一个类似于 ChatGPT 的模型。下面的录屏并未加速，而是实际运行在一台配有 4GB 权重的 M2 MacBook Air 上。</p>
<p>GitHub：<a href="https://github.com/antimatter15/alpaca.cpp">https://github.com/antimatter15/alpaca.cpp</a></p>
<p><strong>Alpaca-LoRA</strong></p>
<p>这个代码库包含了用低秩适应（LoRA）方法复现斯坦福 Alpaca 结果的代码。我们为树莓派（用于研究）提供了一个与 text-davinci-003 相似质量的 Instruct 模型，并且代码可以轻松地应用于 13b、30b 和 65b 模型。</p>
<p>GitHub：<a href="https://github.com/tloen/alpaca-lora">https://github.com/tloen/alpaca-lora</a></p>
<p>Demo：<a href="https://huggingface.co/spaces/tloen/alpaca-lora">https://huggingface.co/spaces/tloen/alpaca-lora</a></p>
<p><strong>AlpacaGPT4-LoRA-7B-OpenLLaMA</strong></p>
<p>Hugging Face：<a href="https://huggingface.co/LLMs">https://huggingface.co/LLMs</a></p>
<p>LLMs Models：<a href="https://huggingface.co/LLMs">https://huggingface.co/LLMs</a></p>
<p><strong>Baize</strong></p>
<p>Baize 是一个使用低秩适应（LoRA）进行微调的开源聊天模型。它利用了由 ChatGPT 自我对话产生的 100,000 个对话数据。同时，我们还使用了 Alpaca 的数据来提高其性能表现。目前已经发布了 7B、13B 和 30B 的模型。</p>
<p>GitHub：<a href="https://github.com/project-baize/baize">https://github.com/project-baize/baize</a></p>
<p>Paper：<a href="https://arxiv.org/pdf/2304.01196.pdf">https://arxiv.org/pdf/2304.01196.pdf</a></p>
<p><strong>Cabrita</strong></p>
<p>一款葡萄牙语微调的指令型 LLaMA 模型。</p>
<p>GitHub：<a href="https://github.com/22-hours/cabrita">https://github.com/22-hours/cabrita</a></p>
<p><strong>Chinese-LLaMA-Alpaca</strong></p>
<p>为了推动中文 NLP 社区大模型的开放研究，该项目开源了中文 LLaMA 模型和经过指令微调的 Alpaca 大型模型。这些模型在原始 LLaMA 的基础上，扩展了中文词汇表并使用中文数据进行二次预训练，从而进一步提高了对中文基本语义理解的能力。同时，中文 Alpaca 模型还进一步利用中文指令数据进行微调，明显提高了模型对指令理解和执行的能力。具体详情请参阅技术报告（崔、杨、姚，2023）。</p>
<p>GitHub：<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></p>
<p><strong>Chinese-Vicuna</strong></p>
<p>一款基于 LLaMA 的中文遵循指令模型。</p>
<p>GitHub：<a href="https://github.com/Facico/Chinese-Vicuna">https://github.com/Facico/Chinese-Vicuna</a></p>
<p><strong>GPT4-x-Alpaca</strong></p>
<p>GPT4-x-Alpaca 是一个经过 GPT4 对话与 GPTeacher 精细调整的 LLaMA 13B 模型。关于其训练和性能方面的资料相对较少。</p>
<p>Hugging Face：<a href="https://huggingface.co/chavinlo/gpt4-x-alpaca">https://huggingface.co/chavinlo/gpt4-x-alpaca</a></p>
<p><strong>gpt4-x-vicuna-13b</strong></p>
<p>作为基础模型，采用了 <a href="https://huggingface.co/eachadea/vicuna-13b-1.1">https://huggingface.co/eachadea/vicuna-13b-1.1</a>。对 Teknium 的 GPTeacher 数据集、未发布的 Roleplay v2 数据集、GPT-4-LLM 数据集以及 Nous Research Instruct 数据集进行了微调。大约包含 180，000 条来自 GPT-4 的指令，已清除所有 OpenAI 审查 /“作为 AI 语言模型” 等相关内容。</p>
<p>Hugging Face：<a href="https://huggingface.co/NousResearch/gpt4-x-vicuna-13b">https://huggingface.co/NousResearch/gpt4-x-vicuna-13b</a></p>
<p><strong>GPT4All</strong></p>
<p>这是一个训练助手式大语言模型的演示，基于 LLaMa，使用约 800k 个 GPT-3.5 Turbo 生成数据。</p>
<p>GitHub：<a href="https://github.com/nomic-ai/gpt4all">https://github.com/nomic-ai/gpt4all</a></p>
<p>GitHub：<a href="https://github.com/nomic-ai/pyllamacpp">https://github.com/nomic-ai/pyllamacpp</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=GhRNIuTA2Z0">https://www.youtube.com/watch?v=GhRNIuTA2Z0</a></p>
<p><strong>GPTQ-for-LLaMA</strong></p>
<p>使用 GPTQ 对 LLaMA 进行 4 位量化。GPTQ 是 SOTA 的单次权重量化方法。</p>
<p>GitHub：<a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">https://github.com/qwopqwop200/GPTQ-for-LLaMa</a></p>
<p><strong>Koala</strong></p>
<p>Koala 是基于 LLaMa 微调的语言模型。请查看下面 Blog，这篇文章介绍了下载、恢复 Koala 模型权重以及在本地运行 Koala 聊天机器人的过程。</p>
<p>Blog：<a href="https://bair.berkeley.edu/blog/2023/04/03/koala/">https://bair.berkeley.edu/blog/2023/04/03/koala/</a></p>
<p>GitHub：<a href="https://github.com/young-geng/EasyLM/blob/main/docs/koala.md">https://github.com/young-geng/EasyLM/blob/main/docs/koala.md</a></p>
<p>Demo：<a href="https://chat.lmsys.org/?model=koala-13b">https://chat.lmsys.org/?model=koala-13b</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=A4rcKUZieEU">https://www.youtube.com/watch?v=A4rcKUZieEU</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=kSLcedGSez8">https://www.youtube.com/watch?v=kSLcedGSez8</a></p>
<p><strong>llama.cpp</strong></p>
<p>使用纯 C/C++ 实现 LLaMa 模型的推理过程。</p>
<p>GitHub：<a href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></p>
<p>支持三种模型：LLaMA、Alpaca 和 GPT4All</p>
<p><strong>LLaMA-Adapter V2</strong></p>
<p>LLaMA-Adapter：<a href="https://arxiv.org/pdf/2303.16199.pdf">https://arxiv.org/pdf/2303.16199.pdf</a> 和 LLaMA-Adapter V2：<a href="https://arxiv.org/pdf/2304.15010.pdf">https://arxiv.org/pdf/2304.15010.pdf</a> 已经发布。</p>
<p>GitHub：<a href="https://github.com/ZrrSkywalker/LLaMA-Adapter">https://github.com/ZrrSkywalker/LLaMA-Adapter</a></p>
<p><strong>Lit-LLaMA ️</strong></p>
<p>LLaMA 独立实现，完全开源且遵循 Apache 2.0 许可证。这个实现是在 nanoGPT 的基础上构建的。</p>
<p>GitHub：<a href="https://github.com/Lightning-AI/lit-llama">https://github.com/Lightning-AI/lit-llama</a></p>
<p><strong>OpenAlpaca</strong></p>
<p>这是 OpenAlpaca 项目的代码仓库，旨在基于 OpenLLaMA 构建并分享一个指令跟随模型。与 OpenLLaMA 一样，OpenAlpaca 采用 Apache 2.0 许可证进行授权。该仓库包含以下内容：</p>
<ul>
<li>用于微调模型的数据。</li>
<li>微调模型的代码。</li>
<li>微调模型的权重。</li>
<li>OpenAlpaca 的使用示例。</li>
</ul>
<p>GitHub：<a href="https://github.com/yxuansu/OpenAlpaca">https://github.com/yxuansu/OpenAlpaca</a></p>
<p><strong>OpenBuddy：面向所有人的开放式多语言聊天机器人</strong></p>
<p>OpenBuddy 是一个功能强大的开源多语言聊天机器人模型，旨在为全球用户提供无缝的英语、中文和其他语言的会话 AI 和多语言支持。该模型基于 Facebook 的 LLAMA 模型构建，通过微调扩展了词汇表、增加了常用字符和改进了令牌嵌入。</p>
<p>OpenBuddy 利用这些改进和多轮对话数据集提供了一个强大的模型，可以回答各种语言的问题并执行翻译任务。</p>
<p>GitHub：<a href="https://github.com/OpenBuddy/OpenBuddy">https://github.com/OpenBuddy/OpenBuddy</a></p>
<p><strong>Pygmalion-7b</strong></p>
<p>Pygmalion 7B 是一个对话模型，基于 Meta 的 LLaMA-7B 模型构建。这是版本 1。使用 Pygmalion-6B-v8-pt4 数据集的一个子集对模型进行了微调，对于熟悉该项目的人而言，这一点很重要。</p>
<p>Hugging Face：<a href="https://huggingface.co/PygmalionAI/pygmalion-7b">https://huggingface.co/PygmalionAI/pygmalion-7b</a></p>
<p><strong>StableVicuna</strong></p>
<p>我们自豪地介绍 StableVicuna，这是第一个通过强化学习从人类反馈中训练的大规模开源聊天机器人（RHLF）。StableVicuna 是 Vicuna v0 13b 的进一步指令微调和 RLHF 训练版本，而 Vicuna v0 13b 则是指令微调的 LLaMA 13b 模型。有兴趣的读者，可以阅读：<a href="https://vicuna.lmsys.org/">https://vicuna.lmsys.org/</a></p>
<p>网站：<a href="https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot">https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot</a></p>
<p>Hugging Face：<a href="https://huggingface.co/spaces/CarperAI/StableVicuna">https://huggingface.co/spaces/CarperAI/StableVicuna</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=m">https://www.youtube.com/watch?v=m</a>_xD0algP4k</p>
<p><strong>StackLLaMA</strong></p>
<p>这是一个在 Stack Exchange 上使用 RLHF 训练的 LLaMa 模型，使用了三种方法的组合：监督微调（SFT）、奖励 / 偏好建模（RM）和人类反馈的强化学习（RLHF），训练数据包括问题和答案。</p>
<p>网站：<a href="https://huggingface.co/blog/stackllama">https://huggingface.co/blog/stackllama</a></p>
<p><strong>The Bloke alpaca-lora-65B-GGML</strong></p>
<p>对 changusung Alpaca-lora-65B 进行了 4 位和 2 位量化的 GGML 模型，以便在 CPU 上进行推理，同时使用 llama.cpp 实现。</p>
<p>Hugging Face：<a href="https://huggingface.co/TheBloke/alpaca-lora-65B-GGML">https://huggingface.co/TheBloke/alpaca-lora-65B-GGML</a></p>
<p><strong>The Bloke’s StableVicuna-13B-GPTQ</strong></p>
<p>这个代码仓库包含 CarterAI StableVicuna 13B 的 4 位 GPTQ 格式量化模型。这个模型的生成过程首先将上述代码仓库中的增量与原始的 Llama 13B 权重合并，然后使用 GPTQ-for-LLaMa 进行 4 位量化。</p>
<p>Hugging Face：<a href="https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ">https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ</a></p>
<p><strong>The Bloke’s WizardLM-7B-uncensored-GPTQ</strong></p>
<p>这些文件是 Eric Hartford “未经审查” 的 WizardLM 模型的 GPTQ 4 位模型文件，是使用 GPTQ-for-LLaMa 进行 4 位量化的结果。Eric 使用 WizardLM 方法对经过编辑的数据集进行了新的 7B 训练，该数据集删除了所有 “我很抱歉……” 类型的 ChatGPT 响应。</p>
<p>Hugging Face：<a href="https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ">https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ</a></p>
<p><strong>Vicuna（FastChat）</strong></p>
<p>一款开源聊天机器人，能达到 ChatGPT 90% 的能力。</p>
<p>GitHub：<a href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=4VByC2NpV30">https://www.youtube.com/watch?v=4VByC2NpV30</a></p>
<p><strong>Vigogne</strong></p>
<p>这个代码仓库包含使用 Hugging Face 的 PEFT 库提供的低秩适应（LoRA）方法，复现了斯坦福大学 Alpaca 的法语版本的代码。除了 LoRA 技术之外，我们还使用 bitsandbytes 提供的 LLM.int8() 来将预训练语言模型（PLMs）量化为 int8。将这两种技术结合起来，使我们能够在单个消费级 GPU（如 RTX 4090）上微调 PLMs。</p>
<p>GitHub：<a href="https://github.com/bofenghuang/vigogne">https://github.com/bofenghuang/vigogne</a></p>
<p><strong>WizardLM</strong></p>
<p>这是一个使用 Evol-Instruct 技术的指令跟随 LLM 模型，使得大型预训练语言模型能够遵循复杂的指令。</p>
<p>GitHub：<a href="https://github.com/nlpxucan/WizardLM">https://github.com/nlpxucan/WizardLM</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=5IAxCL4dHWk">https://www.youtube.com/watch?v=5IAxCL4dHWk</a></p>
<p><strong><strong><strong>#02</strong></strong></strong></p>
<p><strong>BigCode StartCoder</strong></p>
<p>**BigCode 是一个开放的科学合作项目，旨在负责任地训练大语言模型，以应用于编码领域。**你可以在主要网站上找到更多信息，也可以在 Twitter 上关注 BigCode。</p>
<p><strong>在这个组织中，你可以找到这个合作项目的工件，包括 StarCoder，一个用于编码的最先进的语言模型，The Stack，可用的最大的预训练数据集，包含宽容的代码，以及 SantaCoder，一个参数达到 1.1B 的编码模型。</strong></p>
<p>网站：<a href="https://huggingface.co/bigcode">https://huggingface.co/bigcode</a></p>
<p>Hugging Face：<a href="https://huggingface.co/spaces/bigcode/bigcode-playground">https://huggingface.co/spaces/bigcode/bigcode-playground</a></p>
<p><strong><strong><strong>#03</strong></strong></strong></p>
<p><strong>BLOOM（BigScience）</strong></p>
<p><strong>BigScience大型开放科学开放获取多语言模型。</strong></p>
<p>Hugging Face：<a href="https://huggingface.co/bigscience/bloom">https://huggingface.co/bigscience/bloom</a></p>
<p>Hugging Face Demo：<a href="https://huggingface.co/spaces/huggingface/bloom">https://huggingface.co/spaces/huggingface/bloom</a>_demo</p>
<p>以下是 BLOOM 项目的复现或衍生项目：</p>
<p><strong>BLOOM-LoRA</strong></p>
<p>针对各种 Instruct-Tuning 数据集的低秩适应方法。</p>
<p>GitHub：<a href="https://github.com/linhduongtuan/BLOOM-LORA">https://github.com/linhduongtuan/BLOOM-LORA</a></p>
<p><strong>Petals</strong></p>
<p>使用分布式的 176B 参数的 BLOOM 或 BLOOMZ 生成文本，并对其进行微调以适应自己的任务。</p>
<p>GitHub：<a href="https://github.com/bigscience-workshop/petals">https://github.com/bigscience-workshop/petals</a></p>
<p><strong><strong><strong>#04</strong></strong></strong></p>
<p><strong>Cerebras-GPT（Cerebras）</strong></p>
<p>**这是一系列开放、计算高效的大语言模型。**Cerebras 开源了七个 GPT-3 模型，参数从 1.11 亿到 130 亿不等。<strong>这些模型使用了 Chinchilla 公式进行训练，创造了精度和计算效率的新标准。</strong></p>
<p>网站：<a href="https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/">https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/</a></p>
<p>Hugging Face：<a href="https://huggingface.co/cerebras">https://huggingface.co/cerebras</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=9P3">https://www.youtube.com/watch?v=9P3</a>_Zw_1xpw</p>
<p><strong><strong><strong>#05</strong></strong></strong></p>
<p><strong>Flamingo（Google/Deepmind）</strong></p>
<p><strong>使用单一视觉语言模型处理多项任务。</strong></p>
<p>网站：<a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model">https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model</a></p>
<p>以下是基于 Flamingo 项目的复现或衍生项目：</p>
<p><strong>Flamingo — Pytorch</strong></p>
<p>这是 Flamingo 项目的 Pytorch 实现，它是一种最先进的少样本视觉问答注意力网络。该实现包括 Perceiver Resampler（包括学习查询，以供键/值被关注，以及媒体嵌入），专门的掩码交叉注意力块，以及交叉注意力末端的 tanh 门控和相应的前馈块。</p>
<p>GitHub：<a href="https://github.com/lucidrains/flamingo-pytorch">https://github.com/lucidrains/flamingo-pytorch</a></p>
<p><strong>OpenFlamingo</strong></p>
<p>欢迎使用我们的 DeepMind Flamingo 模型的开源版本！在这个仓库中，我们提供了一个 PyTorch 实现，用于训练和评估 OpenFlamingo 模型。我们还提供了一个经过训练的初始 OpenFlamingo 9B 模型，该模型是在一个新的 Multimodal C4 数据集上训练的（即将推出）。有关详细信息，请参阅我们的博客文章。</p>
<p>GitHub：<a href="https://github.com/mlfoundations/open">https://github.com/mlfoundations/open</a>_flamingo</p>
<p><strong><strong><strong>#06</strong></strong></strong></p>
<p><strong>FLAN（Google）</strong></p>
<p>**这个代码库包含用于生成指令调整数据集集合的代码。**第一个数据集是原始的 Flan 2021，它记录在《Finetuned Language Models are Zero-Shot Learners》中，第二个数据集是扩展版本，称为 Flan Collection，它在《The Flan Collection: Designing Data and Methods for Effective Instruction Tuning》中描述，并用于生成 Flan-T5 和 Flan-PaLM。</p>
<p>GitHub：<a href="https://github.com/google-research/FLAN">https://github.com/google-research/FLAN</a></p>
<p>以下是基于 FLAN 项目的复现或衍生项目：</p>
<p><strong>FastChat-T5</strong></p>
<p>我们很高兴地推出 FastChat-T5：这是一个紧凑而商业友好的聊天机器人！它是从 Flan-T5 微调而来，可用于商业应用，并且使用的参数比 Dolly-V2 少 4 倍，性能更好。</p>
<p>GitHub：<a href="https://github.com/lm-sys/FastChat#FastChat-T5">https://github.com/lm-sys/FastChat#FastChat-T5</a></p>
<p>Hugging Face：<a href="https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface">https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface</a>_api.py</p>
<p><strong>Flan-Alpaca</strong></p>
<p>这个仓库包含代码，用于将 Stanford Alpaca 的合成指令微调方法扩展到已有指令微调模型（如 Flan-T5）。预训练模型和演示都可以在 HuggingFace 上获取。</p>
<p>GitHub：<a href="https://github.com/declare-lab/flan-alpaca">https://github.com/declare-lab/flan-alpaca</a></p>
<p><strong>Flan-UL2</strong></p>
<p>这是一个基于 T5 架构的编码器 - 解码器模型，名为 Flan-UL2。它使用了去年早些时候发布的 UL2 模型相同的配置，并使用了 “Flan” 提示微调和数据集合集进行微调。</p>
<p>Hugging Face：<a href="https://huggingface.co/google/flan-ul2">https://huggingface.co/google/flan-ul2</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=cMT3RzjawEc">https://www.youtube.com/watch?v=cMT3RzjawEc</a></p>
<p><strong><strong><strong>#07</strong></strong></strong></p>
<p><strong>GALACTICA（Meta）</strong></p>
<p><strong>根据 Mitchell 等人（2018）的研究，本模型卡提供有关 GALACTICA 模型的信息，包括其训练方式和预期使用情况。<strong>有关模型的训练和评估的详细信息可以在发布的论文中找到：</strong></strong></p>
<p>GitHub：<a href="https://github.com/paperswithcode/galai/blob/main/docs/model">https://github.com/paperswithcode/galai/blob/main/docs/model</a>_card.md</p>
<p>基于 GALACTICA 项目的复现或衍生项目：</p>
<p><strong>Galpaca</strong></p>
<p>这是在 Alpaca 数据集上微调的 30B GALACTICA 模型。</p>
<p>Hugging Face：<a href="https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b">https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b</a></p>
<p>Hugging Face：<a href="https://huggingface.co/TheBloke/galpaca-30B-GPTQ-4bit-128g">https://huggingface.co/TheBloke/galpaca-30B-GPTQ-4bit-128g</a></p>
<p><strong><strong><strong>#08</strong></strong></strong></p>
<p><strong>GLM（General Language Model）</strong></p>
<p><strong>GLM 是一个通用的语言模型，使用自回归填空目标进行预训练，可以在各种自然语言理解和生成任务上进行微调。</strong></p>
<p>基于 GLM 项目的复现或衍生项目：</p>
<p><strong>ChatGLM-6B</strong></p>
<p>ChatGLM-6B 是基于通用语言模型（GLM）框架的开源双语言模型，具有 62 亿个参数。通过量化技术，用户可以在消费级图形卡上进行本地部署（在 INT4 量化级别下仅需要 6GB 的 GPU 内存）。</p>
<p>GitHub：<a href="https://github.com/THUDM/ChatGLM-6B">https://github.com/THUDM/ChatGLM-6B</a></p>
<p><strong><strong><strong>#09</strong></strong></strong></p>
<p><strong>GPT-J</strong></p>
<p>**GPT-J 是由 EleutherAI 开发的开源人工智能语言模型。**GPT-J 在各种零样本下游任务上的表现与 OpenAI 的 GPT-3 非常相似，并且甚至可以在代码生成任务上胜过它。**最新版本 GPT-J-6B 是一种基于数据集 The Pile 的语言模型。**The Pile 是一个开源的 825 gibibyte 语言建模数据集，分为 22 个较小的数据集。<strong>GPT-J 的功能类似于 ChatGPT，尽管它不作为聊天机器人，只作为文本预测模型。</strong></p>
<p>GitHub：<a href="https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b">https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b</a></p>
<p>Demo：<a href="https://6b.eleuther.ai/">https://6b.eleuther.ai/</a></p>
<p>以下是基于 GLM 项目的复现或衍生项目：</p>
<p><strong>Dolly（Databricks）</strong></p>
<p>Databricks 的 Dolly 是一个在 Databricks 机器学习平台上训练的大语言模型，它展示了一个两年前的开源模型（GPT-J）经过仅 30 分钟的针对 50k 个记录的专注语料库的微调后，可以展现出不同于基于其构建的基础模型的惊人高质量的指令跟随行为。我们认为这一发现非常重要，因为它表明了创建强大的人工智能技术的能力比以前意识到的要容易得多。</p>
<p>GitHub：<a href="https://github.com/databrickslabs/dolly">https://github.com/databrickslabs/dolly</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=AWAo4iyNWGc">https://www.youtube.com/watch?v=AWAo4iyNWGc</a></p>
<p><strong>GPT-J-6B instruction-tuned on Alpaca-GPT4</strong></p>
<p>这个模型是在 Alpaca 提示的 GPT-4 生成上使用 LoRA 进行微调的，共进行了 30，000 步（批量大小为 128），在四个 V100S 上花费了超过 7 小时的时间。</p>
<p>Hugging Face：<a href="https://huggingface.co/vicgalle/gpt-j-6B-alpaca-gpt4?text=My+name+is+Teven+and+I+am">https://huggingface.co/vicgalle/gpt-j-6B-alpaca-gpt4?text=My+name+is+Teven+and+I+am</a></p>
<p><strong>GPT4All-J</strong></p>
<p>此仓库包含了基于 GPT-J 构建的开源助手式大语言模型的演示、数据和训练代码。</p>
<p>GitHub：<a href="https://github.com/nomic-ai/gpt4all">https://github.com/nomic-ai/gpt4all</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=5icWiTvDQS0">https://www.youtube.com/watch?v=5icWiTvDQS0</a></p>
<p><strong><strong><strong>#10</strong></strong></strong></p>
<p><strong>GPT-NeoX</strong></p>
<p>**该代码库记录了 EleutherAI 在 GPU 上训练大规模语言模型的库。**我们目前的框架基于 NVIDIA 的 Megatron 语言模型，并已经添加了 DeepSpeed 的技术以及一些新的优化技巧。<strong>我们的目标是将这个仓库作为一个集中且易于访问的地方，汇集大规模自回归语言模型训练技术，并加速大规模训练的研究。</strong></p>
<p>GitHub：<a href="https://github.com/EleutherAI/gpt-neox">https://github.com/EleutherAI/gpt-neox</a></p>
<p><strong><strong><strong>#11</strong></strong></strong></p>
<p><strong>h2oGPT</strong></p>
<p><strong>我们的目标是创建全球最好的开源 GPT！</strong></p>
<p>GitHub：<a href="https://github.com/h2oai/h2ogpt">https://github.com/h2oai/h2ogpt</a></p>
<p>Hugging Face：<a href="https://huggingface.co/spaces/h2oai/h2ogpt-oasst1-256-6.9b-hosted">https://huggingface.co/spaces/h2oai/h2ogpt-oasst1-256-6.9b-hosted</a></p>
<p><strong><strong><strong>#12</strong></strong></strong></p>
<p><strong>HuggingGPT</strong></p>
<p><strong>HuggingGPT 是一个协作系统，由 LLM 作为控制器和众多来自 HuggingFace Hub 的专家模型作为协作执行者组成。</strong></p>
<p>GitHub：<a href="https://github.com/microsoft/JARVIS">https://github.com/microsoft/JARVIS</a></p>
<p><strong><strong><strong>#13</strong></strong></strong></p>
<p><strong>Mosaic ML’s MPT-7B</strong></p>
<p>**MPT-7B 是一款 GPT 风格的模型，是 MosaicML 基础系列中的第一款模型。**它是由 MosaicML 策划的数据集中的 1T 标记训练而成的，是开源的、商用可用的，并且在评估指标上等同于 LLaMa 7B。**MPT 架构包含了所有最新的 LLM 建模技术 - 快闪式注意力（Flash Attention）实现高效率、Alibi 用于上下文长度的外推、以及稳定性改进来减轻损失的波动。**基础模型和几个变体，包括一个 64K 上下文长度的微调模型都是可用的。</p>
<p>网站：<a href="https://www.mosaicml.com/blog/mpt-7b">https://www.mosaicml.com/blog/mpt-7b</a></p>
<p>GitHub：<a href="https://github.com/mosaicml/llm-foundry#mpt">https://github.com/mosaicml/llm-foundry#mpt</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=NY0bLFqkBL0">https://www.youtube.com/watch?v=NY0bLFqkBL0</a></p>
<p><strong><strong><strong>#14</strong></strong></strong></p>
<p><strong>Nvidia NeMo（GPT-2B-001）</strong></p>
<p>**GPT-2B-001 是一种基于 transformer 的语言模型。**GPT 是指一类类似于 GPT-2 和 3 的 transformer 解码模型，而 2B 则指可训练参数总数（20 亿）。<strong>该模型是使用 NeMo 在 1.1T 个标记上进行训练的。</strong></p>
<p>Hugging Face：<a href="https://huggingface.co/nvidia/GPT-2B-001">https://huggingface.co/nvidia/GPT-2B-001</a></p>
<p><strong><strong><strong>#15</strong></strong></strong></p>
<p><strong>OpenAssistant Models</strong></p>
<p><strong>每个人都能使用的对话型人工智能。</strong></p>
<p>网站：<a href="https://open-assistant.io/">https://open-assistant.io/</a></p>
<p>GitHub：<a href="https://github.com/LAION-AI/Open-Assistant">https://github.com/LAION-AI/Open-Assistant</a></p>
<p>Hugging Face：<a href="https://huggingface.co/OpenAssistant">https://huggingface.co/OpenAssistant</a></p>
<p><strong><strong><strong>#16</strong></strong></strong></p>
<p><strong>OpenLLaMA</strong></p>
<p>**在这个代码库中，我们发布了 Meta AI 的 LLaMA 大语言模型的开源复现版本，采用宽松许可证。**在此版本中，我们发布了经过训练的 2000 亿标记的 7B OpenLLaMA 模型的公共预览版。**我们提供了预训练的 OpenLLaMA 模型的 PyTorch 和 Jax 权重，以及评估结果和与原始 LLaMA 模型的比较。**请继续关注我们的更新。</p>
<p>GitHub：<a href="https://github.com/openlm-research/open">https://github.com/openlm-research/open</a>_llama</p>
<p><strong><strong><strong>#17</strong></strong></strong></p>
<p><strong>Palmyra Base 5B（Writer）</strong></p>
<p>**Palmyra Base 主要使用英文文本进行预训练。**请注意，仍然有一小部分非英语数据存在于通过 CommonCrawl 访问的训练语料库中。**在模型的预训练过程中，采用了因果语言建模（CLM）目标。**与 GPT-3 类似，Palmyra Base 是仅包含解码器的模型系列的成员。**因此，它是通过自监督的因果语言建模目标进行预训练的。**Palmyra Base 使用 GPT-3 的提示和一般实验设置，以便根据 GPT-3 进行评估。</p>
<p>Hugging Face：<a href="https://huggingface.co/Writer/palmyra-base">https://huggingface.co/Writer/palmyra-base</a></p>
<p>基于 Palmyra 项目的复现或衍生项目：</p>
<p><strong>Camel 5B</strong></p>
<p>介绍一下 Camel-5b，它是一个最先进的指令跟随大语言模型，旨在提供卓越的性能和多功能性。Camel-5b 基于 Palmyra-Base 的基础架构进行了优化，专门针对不断增长的先进自然语言处理和理解需求进行了设计。</p>
<p>Hugging Face：<a href="https://huggingface.co/Writer/camel-5b-hf">https://huggingface.co/Writer/camel-5b-hf</a></p>
<p><strong><strong><strong>#18</strong></strong></strong></p>
<p><strong>Polyglot</strong></p>
<p><strong>这是一篇有关多语言平衡能力的大语言模型的介绍。<strong>已经发布了各种多语言模型，如 mBERT，BLOOM 和 XGLM。<strong>因此，有人可能会问：</strong>“为什么我们需要再次制作多语言模型？</strong>” 在回答这个问题之前，我们想问：</strong>“为什么世界各地的人们会用自己的语言制作单语言模型，即使已经有很多多语言模型了？**” 我们想指出当前多语言模型的非英语语言性能不佳是最重要的原因之一。**因此，我们希望制作具有更高非英语语言性能的多语言模型。<strong>这就是我们需要再次制作多语言模型并将它们命名为 “Polyglot” 的原因。</strong></p>
<p>GitHub：<a href="https://github.com/EleutherAI/polyglot">https://github.com/EleutherAI/polyglot</a></p>
<p><strong><strong><strong>#19</strong></strong></strong></p>
<p><strong>Pythia</strong></p>
<p><strong>跨时间和尺度解释自回归 Transformer。</strong></p>
<p>GitHub：<a href="https://github.com/EleutherAI/pythia">https://github.com/EleutherAI/pythia</a></p>
<p>基于 Pythia 项目的复现或衍生项目：</p>
<p><strong>Dolly 2.0</strong></p>
<p>Dolly 2.0 是一个使用 EleutherAI Pythia 模型家族作为基础、仅在新的高质量人类生成的指令追踪数据集上进行微调的 12B 参数语言模型，该数据集由 Databricks 员工进行了众包。</p>
<p>网站：<a href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm">https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm</a></p>
<p>Hugging Face：<a href="https://huggingface.co/databricks">https://huggingface.co/databricks</a></p>
<p>GitHub：<a href="https://github.com/databrickslabs/dolly/tree/master/data">https://github.com/databrickslabs/dolly/tree/master/data</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=grEp5jipOtg">https://www.youtube.com/watch?v=grEp5jipOtg</a></p>
<p><strong><strong><strong>#20</strong></strong></strong></p>
<p><strong>Replit-Code</strong></p>
<p>**replit-code-v1-3b 是一个专注于代码补全的 27 亿因果语言模型。**该模型是在 Stack Dedup v1.2 数据集的子集上训练的，训练混合包括以下 20 种语言，按标记数量降序排列：</p>
<p>Markdown、Java、JavaScript、Python、TypeScript、PHP、SQL、JSX、reStructuredText、Rust、C、CSS、Go、C++、HTML、Vue、Ruby、Jupyter Notebook、R、Shell</p>
<p>该模型的训练数据集包含总计 1750 亿个标记，重复使用 3 个时代，因此 replit-code-v1-3b 已经在 5250 亿个标记上进行了训练（每个参数大约 195 个标记）。</p>
<p>Hugging Face：<a href="https://huggingface.co/replit/replit-code-v1-3b">https://huggingface.co/replit/replit-code-v1-3b</a></p>
<p><strong><strong><strong>#21</strong></strong></strong></p>
<p><strong>The RWKV Language Model</strong></p>
<p>**RWKV 是一个可以并行运行的循环神经网络，其性能相当于 Transformer-level 的大语言模型（LLM），名称来自其四个主要参数：**R、W、K 和 V，发音为 “RwaKuv”。</p>
<p>GitHub：<a href="https://github.com/BlinkDL">https://github.com/BlinkDL</a></p>
<p>ChatRWKV：<a href="https://github.com/BlinkDL/ChatRWKV">https://github.com/BlinkDL/ChatRWKV</a></p>
<p>Hugging Face Demo：<a href="https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio">https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio</a></p>
<p>RWKV pip package：<a href="https://pypi.org/project/rwkv/">https://pypi.org/project/rwkv/</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=B3Qa2rRsaXo">https://www.youtube.com/watch?v=B3Qa2rRsaXo</a></p>
<p><strong><strong><strong>#22</strong></strong></strong></p>
<p><strong>Segment Anything</strong></p>
<p>**“Segment Anything Model（SAM）”能够根据输入的提示，例如点或框，产生高质量的对象掩模，并可用于生成图像中所有对象的掩模。**它已经在一个包含 1100 万张图片和 11 亿个掩模的数据集上进行了训练，并在各种分割任务的零样本情况下表现出强大的性能。</p>
<p>网站：<a href="https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/">https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/</a></p>
<p>GitHub：<a href="https://github.com/facebookresearch/segment-anything">https://github.com/facebookresearch/segment-anything</a></p>
<p><strong><strong><strong>#23</strong></strong></strong></p>
<p><strong>StableLM</strong></p>
<p>**StableLM 是一种新的开源语言模型，其 Alpha 版本提供了 30 亿和 70 亿参数版本，之后还会推出 150 亿到 650 亿参数的模型。**开发者可以自由地检查、使用和调整我们的 StableLM 基础模型，用于商业或研究目的，但需要遵守 CC BY-SA-4.0 许可协议的条款。</p>
<p>​**StableLM 是在建立在 The Pile 上的新实验数据集上训练的，数据集大小为原来的三倍，包含了 1.5 万亿个内容单元。**这个数据集的丰富性使得 StableLM 在对话和编程任务方面表现出了出乎意料的高性能，尽管它的参数规模只有 3 到 7 亿（相比之下，GPT-3 有 1750 亿个参数）。</p>
<p>网站：<a href="https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models">https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models</a></p>
<p>GitHub：<a href="https://github.com/stability-AI/stableLM/">https://github.com/stability-AI/stableLM/</a></p>
<p>Hugging Face：<a href="https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat">https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=0uI7SoMn0Es">https://www.youtube.com/watch?v=0uI7SoMn0Es</a></p>
<p><strong><strong><strong>#24</strong></strong></strong></p>
<p><strong>Together’s RedPajama-INCITE 3B and 7B</strong></p>
<p>**我们发布了基于 RedPajama 数据集训练的第一批模型，包括 3B 和 7B 参数的基础模型，旨在尽可能精准地复制 LLaMA 模型的架构。**此外，我们还发布了完全开源的指令调优和对话模型。</p>
<p>网站：<a href="https://www.together.xyz/blog/redpajama-models-v1">https://www.together.xyz/blog/redpajama-models-v1</a></p>
<p>Hugging Face：<a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1">https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1</a></p>
<p><strong><strong><strong>#25</strong></strong></strong></p>
<p><strong>XGLM</strong></p>
<p><strong>XGLM 模型是在 “Few-shot Learning with Multilingual Language Models” 中提出的。</strong></p>
<p>GitHub：<a href="https://github.com/facebookresearch/fairseq/tree/main/examples/xglm">https://github.com/facebookresearch/fairseq/tree/main/examples/xglm</a></p>
<p>Hugging Face：<a href="https://huggingface.co/docs/transformers/model">https://huggingface.co/docs/transformers/model</a>_doc/xglm</p>
<p><strong><strong><strong>#26</strong></strong></strong></p>
<p><strong>Other Repositories</strong></p>
<p><strong>couchpotato888</strong></p>
<p>Hugging Face：<a href="https://huggingface.co/couchpotato888">https://huggingface.co/couchpotato888</a></p>
<p><strong>crumb</strong></p>
<p>Hugging Face：<a href="https://huggingface.co/crumb">https://huggingface.co/crumb</a></p>
<p><strong>Knut Jägersberg</strong></p>
<p>Hugging Face：<a href="https://huggingface.co/KnutJaegersberg">https://huggingface.co/KnutJaegersberg</a></p>
<p><strong>LaMini-LM：来自大规模指令的多样化压缩模型群</strong></p>
<p>LaMini-LM 是一系列小型、高效的语言模型，它们是从 ChatGPT 中提取并经过训练的，训练数据集包含 258 万条指令。我们尝试了不同的模型架构、大小和检查点，并在各种 NLP 基准测试和人类评估中广泛评估了它们的性能。</p>
<p>Paper：<a href="https://arxiv.org/abs/2304.14402">https://arxiv.org/abs/2304.14402</a></p>
<p>GitHub：<a href="https://github.com/mbzuai-nlp/LaMini-LM">https://github.com/mbzuai-nlp/LaMini-LM</a></p>
<p>Review：<a href="https://www.youtube.com/watch?v=TeJrG3juAL4&amp;t=42s">https://www.youtube.com/watch?v=TeJrG3juAL4&amp;t=42s</a></p>
<p><strong>Teknium</strong></p>
<p>Hugging Face: <a href="https://huggingface.co/teknium">https://huggingface.co/teknium</a></p>
<p>​</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[万字长文聊聊Web3的组成架构]]></title>
        <id>https://jobslee0.github.io/post/wan-zi-chang-wen-liao-liao-web3-de-zu-cheng-jia-gou/</id>
        <link href="https://jobslee0.github.io/post/wan-zi-chang-wen-liao-liao-web3-de-zu-cheng-jia-gou/">
        </link>
        <updated>2023-03-16T10:09:49.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>原文链接：https://mp.weixin.qq.com/s/R3YBcN2t2VLeHZTpsGzw5Q</p>
</blockquote>
<p>Web3 发展至今，生态已然初具雏形，如果将当前阶段的 Web3 生态组成架构抽象出一个鸟瞰图，由下而上可划分为四个层级：<strong>区块链网络层、中间件层、应用层、访问层</strong>。下面我们来具体看看每一层级都有什么。另外，此章节会涉及到很多项目的名称，因为篇幅原因不会一一进行介绍，有兴趣的可以另外去查阅相关资料进行深入了解。</p>
<figure data-type="image" tabindex="1"><img src="https://jobslee0.github.io/post-images/1678961499732.png" alt="" loading="lazy"></figure>
<h3 id="区块链网络层">区块链网络层</h3>
<p>最底层是「区块链网络层」，也是 Web3 的基石层，主要由各区块链网络所组成。</p>
<p>组成该层级的区块链网络还不少，Bitcoin、Ethereum、BNB Chain(BSC)、Polygon、Arbitrum、Polkadot、Cosmos、Celestia、Avalanche、Aptos、Sui 等等，还有很多。根据 Blockchain-Comparison 的统计，截止撰文之日的区块链至少有 150 条。这里我们主要说的是公链，联盟链不包括在内。因为区块链实在太多，会有些眼花缭乱，所以有必要进行分门别类。</p>
<p>首先，不同区块链之间存在着分层结构，有 <strong>Layer0、Layer1、Layer2</strong> 之分。其次，Web3 的繁荣发展，依赖于智能合约技术，而智能合约的运行环境为虚拟机。智能合约和虚拟机的关系，就和 Java 程序和 JVM 的关系类似。从不同的虚拟机维度上划分区块链，就可以分为两大类：<strong>EVM 链</strong>和 <strong>Non-EVM 链</strong>。EVM 是 Ethereum Virtual Machine，即为以太坊虚拟机的简称。EVM 链即为兼容 EVM 的区块链，而 Non-EVM 顾名思义就是不兼容 EVM 的区块链。最后，还可以根据存储的数据大小进行分类，可以分为计算型区块链和存储型区块链。</p>
<p>先从分层结构说起。最好理解的是 Layer1，我们所熟知的比特币、以太坊、EOS、BSC 都属于 Layer1，也称为主链。在分布式系统中，存在 CAP 定理，即一个分布式系统不可能同时满足三个特性：一致性、可用性、分区容错性。一个分布式系统只能满足三项中的两项。Layer1 的区块链本质上也是分布式系统，也同样存在不可能三角问题，只是三个特性与 CAP 不同，分别为：可扩展性、安全性、去中心化，每个区块链也只能满足三项中的两项。比特币和以太坊偏向于安全性和去中心化，所以可扩展性比较弱，TPS 比较低。EOS 和 BSC 则只依赖于少数节点来维护共识，相比于比特币和以太坊，减低了去中心化特性，但提高了可扩展性，从而能达到很高的 TPS。</p>
<p>为了解决比特币和以太坊的可扩展性问题，就慢慢衍生出了 Layer2。Layer2 是作为依附于主链的子链而存在，主要用于承载 Layer1 的交易量，承担执行层的角色，而 Layer1 则可变成结算层，可大大减少交易压力。目前主流的 Layer2 都是扩展以太坊的子链，包括 Arbitrum、Optimism、zkSync、StarkNet、Polygon 等。比特币也有 Layer2，主要包括闪电网络、Stacks、RSK 和 Liquid，但目前都比较小众。</p>
<p>Layer0 则比较抽象了，一般被定义为区块链基础设施服务层，主要由模块化区块链所构成，包括 Celestia、Polkadot、Cosmos 等。模块化区块链这个概念主要是由 Celestia 提出的，其核心设计思路就是把区块链的共识、执行、数据可用性这几个核心模块拆分开来，每个模块由一条单独的链来完成，再将几个模块组合到一起完成全部工作。这和软件架构设计中所提倡的模块化设计思想是一样的，可实现高内聚低耦合。</p>
<figure data-type="image" tabindex="2"><img src="https://jobslee0.github.io/post-images/1678961510676.png" alt="" loading="lazy"></figure>
<p>实现跨链通信的跨链桥或跨链协议也可以划入 Layer0。跨链桥的数量也是非常多，撰写此文时，debridges.com 上统计的跨链桥多达 113 条，其中 TVL 排名最高的三个分别为 Polygon、Arbitrum、Optimism 的官方跨链桥，这几个桥分别实现了各自的 Layer2 和以太坊之间的资产跨链。TVL 排名第四位的则是 Multichain，其前身为 Anyswap，是连接了最多条区块链的第三方跨链桥，截至今年 1 月份时，其连接的区块链多达 81 条。</p>
<p>聊完分层结构的划分，我们再从 EVM 的维度来梳理下不同的区块链。前面说过，从 EVM 维度上可划分为 EVM 链和 Non-EVM 链两大类。</p>
<p>EVM 链是目前最主流的方向，基于 EVM 链的 DApp 和用户群体是目前整个 Web3 生态里规模最大的。有些原生就是兼容 EVM 的，比如 BSC、Heco、Arbitrum、Optimism 等；有些则是后期才扩展兼容 EVM 的，比如 zkSync 1.0 并不兼容 EVM，而 zkSync 2.0 则是兼容 EVM 的。很多区块链就算早期并不兼容 EVM，但也逐渐在拥抱 EVM。比如，Polkadot 推出了 Moonbeam 平行链来兼容 EVM，Cosmos 则有 Evmos。</p>
<p>目前来看，排名靠前的区块链中，大部分都已经兼容 EVM，不过依然还有少部分 Non-EVM 链存在，比如 Solana、Terra、NEAR、Aptos、Sui。另外，EVM 链的智能合约主要使用 Solidity 作为开发语言，而 Non-EVM 链则主要使用 Rust 或 Move 语言开发智能合约。</p>
<p>以上提到的这些区块链，主要还是偏向于解决去中心化计算的区块链，这些区块链普遍不支持大数据的存储，比如文件存储。而存储型的区块链则聚焦于解决大数据存储的问题，这类区块链目前不太多，主要有 Filecoin、Arweave、Storj、Siacoin 和 EthStorage。</p>
<p>目前组成「区块链网络层」的区块链成员们主要就包括这些了，未来还会不断有新成员加入，但也有不少旧成员逐渐没落而被遗落在角落里。</p>
<h3 id="中间件层">中间件层</h3>
<p>在区块链网络层之上的这一层，我称之为「中间件层」，主要为上层应用提供各种通用服务和功能。所提供的通用服务和功能包括但不限于：<strong>安全审计、预言机、索引查询服务、API 服务、数据分析、数据存储、基本的金融服务、数字身份、DAO 治理</strong>等。提供通用服务和功能的组件则可称为「中间件」，这些中间件也是存在多种形式，可以是链上协议，也可以是链下平台，或链下组织，包括中心化的企业或去中心化自治组织 DAO。下面就来聊聊这一层具体都有哪些中间件。</p>
<p>先来聊聊安全审计，这是非常核心的中间件，因为 Web3 里的区块链和应用大多都是开源的，且很多都是跟金融强相关，因此，安全性就成为了重中之重，安全审计自然也变成了刚需。安全审计的服务大多由一些安全审计公司所提供，比较知名的审计公司包括：CertiK、OpenZeppelin、ConsenSys、Hacken、Quantstamp，以及国内主要有慢雾、链安、派盾等。另外，还有不少知名度不高的小审计公司。</p>
<p>除了审计公司，还有一些提供 Bug Bounty 的平台，一般就是在这些平台上发布任务，让白帽黑客们来找 Bug，找到的 Bug 安全漏洞等级越高则可获得的赏金越高。目前，全球最大的 Bug Bounty 平台是 Immunefi。</p>
<p>接着，再来聊聊预言机（Oracle Machine，简称 Oracle），在 Web3 生态里也是扮演着非常重要的角色，是区块链系统与外部数据源之间沟通的桥梁，主要实现智能合约与链下真实世界的数据互通。因为区块链网络本身对状态一致性的限制，需要保证每个节点在给定相同输入的情况下必须获得相同的结果，所以区块链被设计成一个封闭系统，只能获取到链内的数据，而无法主动获取外部系统的数据。但很多应用场景中是需要用到外部数据的，这些外部数据就由预言机来提供，这也是目前区块链与外部数据实现互通的唯一途径。</p>
<p>根据预言机所提供的具体功能，目前对预言机的分类大致有：DeFi 预言机、NFT 预言机、SocialFi 预言机、跨链预言机、隐私预言机、信用预言机、去中心化预言机网络。具体的预言机项目有 CreDA、Privy、UMA、Banksea、DOS、NEST、Chainlink 等，其中，Chainlink 为预言机的龙头，其定位为去中心化预言机网络，推出了 Data Feeds、VRF、Keepers、Proof of Reserve、CCIP 等一系列产品和服务。</p>
<p>然后，索引查询服务也是很关键的中间件，解决了链上数据的复杂查询问题。比如要查询 Uniswap 上某一天的总交易量，如果直接在链上查询是很麻烦的。所以就有了对索引查询服务的需求，这块的主要代表为 The Graph 和 Covalent。The Graph 的实现方案主要是可定制化监听链上数据并映射成自定义的数据进行存储，从而方便查询。而 Covalent 则是将很多通用、广泛使用的数据封装成统一的 API 服务，供用户查询。</p>
<p>提到 API 服务，除了 Covalent，还存在解决其他不同需求的 API 提供商，比如：NFTScan，是聚焦于提供 NFT API 数据服务的；Infura 和 Alchemy，则主要提供区块链网络节点服务；API3，旨在打造去中心化 API 服务。</p>
<p>不管是索引查询服务还是 API 服务，都是链上数据相关的服务，数据分析也是数据相关的服务，这一版块的成员主要有 Dune Analytics、Flipside Crypto、DeBank、Chainalysis 等。</p>
<p>数据存储中间件则和底层几个专门做存储的区块链容易混淆，也有人将底层的 Filecoin、Arweave、Storj 等划分到这一层，但我觉得这些本质上还是底层区块链，所以我将其划入到区块链网络层。而中间件层的数据存储，目前主要就是 IPFS。IPFS 全称为 InterPlanetary File System，中文名为星际文件系统，是一个基于内容寻址、分布式、点对点的新型超媒体传输协议，其旨在取代 HTTP 协议。IPFS 与区块链网络很相似，但其实并不属于区块链网络，基于 IPFS 的 Filecoin 才是区块链网络。</p>
<p>接下来，看看有哪些中间件是提供基本金融服务的。这块的代表性组件主要包括 Uniswap、Curve、Compound、Aave 等，Uniswap 和 Curve 提供了链上交易功能，而 Compound 和 Aave 则是链上借贷平台。这几个本质上都是应用层的链上协议，但因为这些协议都逐渐被越来越多其他应用所依赖，类似于成为了乐高积木，可以用来组合搭建出不同的应用，于是就变成了通用性的应用协议，即下沉为了中间件的角色。</p>
<p>其实，任何具有可组合性的组件，不管是链上应用协议，还是链下提供不同服务的中心化实体，或者是 DAO，只要其提供的服务和功能是大部分应用都需要的，就可以划入「中间件层」。不同的中间件就和不同的乐高积木一样，通过组装不同的积木就可以创建出不同的应用。包括数字身份、DAO 治理的工具等，其实也都是同样道理。</p>
<h3 id="应用层">应用层</h3>
<p>应用层是 Web3 生态里最繁荣的一层，这一层里，充斥着各种不同的 DApps，可谓是百花齐放、百家争鸣。下面我们主要介绍几个发展得相对比较繁荣的板块。</p>
<h4 id="nft">NFT</h4>
<p>NFT 全称为 Non-Fungible Token，表示「非同质化代币」，国内也称为数字藏品，用于代表艺术品等独一无二的数字资产。</p>
<p>第一个真正意义上的 NFT 项目叫 CryptoPunks，于 2017 年 6 月发布，由 10,000 个 24x24 像素的头像所组成。每个头像都是由算法生成的，独一无二且所有头像都上传到了以太坊上，也是目前为止唯一一个将所有头像数据全部上链的 NFT 项目。下图为 CryptoPunks 官网展示的部分头像：</p>
<figure data-type="image" tabindex="3"><img src="https://jobslee0.github.io/post-images/1678961525389.png" alt="" loading="lazy"></figure>
<p>截止撰文之日，CryptoPunks 的地板价（即最低价）为 66.88 ETH，按 ETH 的价格换算成美元，大概为 $84,397.21 美元。最贵的一个 CryptoPunk，成交价达到了 8000 ETH，成交于 2022 年 2 月 12 日。一个 NFT 头像为何会这么贵，这对于很多人都是很难理解的。其中，最主要的一个原因，就是它是第一个 NFT 项目，就和比特币是第一个区块链一样，其开创性的地位所带来的价值潜力非常大。</p>
<p>受 CryptoPunks 的启发，一家名为 Axiom Zen（Dapper Labs 的前身）的公司于 2017 年 11 月底发行了 CryptoKitties，国内也称为加密猫、以太猫、谜恋猫。CryptoKitties 上线后便病毒式地传播开来，还造成了以太坊的拥堵，暴露出以太坊的性能问题。CryptoKitties 发行之前，Axiom Zen 的技术总监 Dieter Shirley 以 CryptoKitties 为案例，还提出了 ERC721 Token 协议作为 NFT 的通用技术标准，而随着 CryptoKitties 爆火后，以 ERC721 为主要技术标准的 NFT 被进一步采用，如今 ERC721 已经成为了所有 NFT 的基础标准之一。</p>
<p>继 CryptoPunks 和 CryptoKitties 之后，NFT 开始逐渐遍地开花，NFT 生态逐渐蓬勃发展。NFT 发展至今，已经涉足到了多个领域，如果对 NFT 生态的所有组成部分做详细分类的话，可以多达几十种。如果只聚焦于 NFT 本身，即 NFT 的不同用例，那大致可以做出以下分类：<strong>收藏品、艺术品、音乐、影视、游戏、体育运动、虚拟土地、金融、品牌、DID</strong>。下面主要介绍每个分类的一些代表性的 NFT 项目。</p>
<p>收藏品其实很难单独定义为一个类别，宽泛地讲，几乎任何东西都可以归为收藏品，包括艺术品、游戏道具、虚拟土地等。能被定义为收藏品的 NFT 主要需具备一个特性：稀缺性。比如，10000 个 CryptoPunks 中，外星人的数量最少，所以有很高的稀缺性，而男性最多，稀缺性就很低了。最知名的收藏品 NFT，除了 CryptoPunks，还有 BAYC，全称为 Bored Ape Yacht Club，也称为无聊猿。无聊猿不只是一套单独的 NFT，其实只是「无聊猿宇宙」的开端，基于无聊猿之后，背后的团队 Yuga Labs 又相继发行了无聊猿犬舍俱乐部（Bored Ape Kennel Club，BAKC)、变异猿游艇俱乐部（Mutant Ape Yacht Club，MAYC），也发行了 ApeCoin（APE）代币，还推出了 Otherside，专为元宇宙打造的虚拟土地。这些，都已经形成了「无聊猿宇宙」系列 IP，而且无聊猿不只是在加密圈内流行，在圈外的周边产品也在不断增加，比如有无聊猿的帽子、衣服、雕像、餐厅等。无聊猿的成功已超越了 CryptoPunks，Yuga Labs 之后还直接收购了 CryptoPunks。</p>
<p>NFT 的特性能有效保护版权的所有权，所以在艺术品领域流行开来也是理所当然。艺术品 NFT 有几个代表性的作品值得介绍一番，第一个是艺术家 Beeple 的作品，名为“每一天：第一个 5000 天（EVERYDAYS: THE FIRST 5000 DAYS）”，是将他过去 5000 天内每天创作一幅的所有作品（共 5000 幅）合成一个 NFT 图像，在 2021 年 3 月以 69,346,250 美元售出。第二个值得介绍的是生成艺术，也称为衍生艺术。生成艺术中的艺术品不是由人创作出来的，而是由编程算法自动生成的，最知名的 NFT 生成艺术平台叫 Art Blocks，是一个基于以太坊的随机生成艺术平台。艺术家们可以把自己设计的独特算法上传到 Art Blocks 平台，并设定特定数量 NFT 进行发行，NFT 会根据算法自动生成。最后再介绍目前最贵的 NFT 艺术品，叫 ”The Merge“，2021 年 12 月以 9180 万美元天价成交。与其他 NFT 不同，”The Merge“ 其实不是一个单独的作品，而是由多个「mass」代币动态组合而成的。销售的其实也是 mass 代币，当初共售出 312,686 个 mass 代币，共有 28,983 个买家，即是说，”The Merge“ 是由这 28,983 个买家共同拥有其所有权，每个买家所拥有的 mass 代币数量就代表了占有多少份额的所有权。”The Merge“ 也可以理解为是一个碎片化 NFT 作品。</p>
<p>音乐 NFT 的兴起和艺术品类似，主要也是因为版权。下面介绍几个具有代表性的音乐 NFT 相关人物，第一个要介绍的是 Justin David Blau，是美国 DJ 和电子舞曲制作人，以艺名 3LAU 而闻名。他是最早采用音乐 NFT 的人之一，在 2020 年秋天卖出了他的第一张 NFT。而在 2021 年 2 月底，凭借 Ultraviolet NFT 专辑为他带来了 1168 万美元的收入。2021 年 5 月又成立了 NFT 音乐平台 Royal，8 月份完成了种子轮融资 1600 万美元，有 a16z、Coinbase 等顶级机构参与。第二个要介绍的是 Don Diablo，荷兰 DJ、数字艺术家、唱片制作人、音乐家和电子舞曲创作者，他在 2021 年卖出第一部完整的音乐会电影 NFT，名为 “Destination Hexagonia”，成交价 600 ETH（当时为 126 万美元）。最后再介绍一个叫 Kingship 的摇滚乐队，这是一支由无聊猿组成的虚拟乐队，由环球音乐集团所组建。</p>
<p>NFT 也席卷到了影视圈，有几个知名的影视剧都陆续发行了 NFT，国外有《权力的游戏》《蝙蝠侠》《指环王》《黑客帝国》《行尸走肉》等，国内有《大话西游》《流浪地球》《我不是药神》《封神三部曲》等。</p>
<p>NFT 用在游戏里主要就是作为游戏资产的载体，相比于传统游戏内的资产，NFT 的形式对游戏玩家来说可以真正拥有游戏资产的所有权，且 NFT 可以在游戏外流通交易。第一个游戏 NFT 项目就是 CryptoKitties，每一只猫都是一个独立的 NFT。后面讲到 GameFi 小节再继续深入聊聊游戏这块。</p>
<p>体育运动领域也同样涉足了 NFT，目前最知名的两大体育 NFT 平台是 NBA Top Shot 和 Sorare。NBA Top Shot 顾名思义主要以 NBA 为主，而 Sorare 则服务于足球领域。除了 NBA 和足球，橄榄球、棒球、拳击、摔跤也都纷纷推出了各自的 NFT 纪念品。</p>
<p>虚拟土地类 NFT 主要由一些主打「元宇宙」概念的项目所推行，比较知名的有 Decentraland、The Sandbox、Roblox、Axie Infinity Land、Otherdeed 等。</p>
<p>金融和 NFT 的结合，主要就是将 NFT 应用到 DeFi 中，比如 UniswapV3 中的流动性仓位就是 NFT。另外，还有一个思路则是先将 NFT 碎片化，接着将这些碎片后的 NFT 再赋予 DeFi 功能，比如可以赋予交易、借贷、质押挖矿等功能。</p>
<p>品牌和 NFT 的结合，主要是作为一种新的营销方式。这两三年陆续有各种品牌加入这个阵营，比如，奢侈品品牌有 GUCCI、LV、爱马仕等，餐饮品牌有 Taco Bell、星巴克、必胜客、可口可乐等，汽车品牌有迈凯伦、雪佛兰等，运动品牌有阿迪达斯、李宁、耐克等，还有很多其他品牌。</p>
<p>最后，聊聊 DID，全称为 Decentralized Identity，即去中心化身份。所有人都知道 DID 非常重要，但其发展还比较缓慢，目前除了细分领域 ENS 域名之后，还没有成熟的 DID 体系形成网络效应。目前，应用最广泛的只有域名，基于以太坊的 ENS 是龙头，ENS 之于 Web3，就相当于 DNS 之于 Web2。不同的是，ENS 解析的域名，映射的不是网站 IP，而是用户的以太坊地址。比如，以太坊创始人 V 神的 ENS 为 “vitalik.eth”，映射的地址为 0xd8da6bf26964af9d7eed9e03e53415d37aa96045。</p>
<p>NFT 的可应用场景实在太多了，上面所列出的分类还没能覆盖到全部。因为 NFT 的特性，任何具有所有权的东西都可以指代，所以坊间有“万物皆可 NFT”的说法。</p>
<h4 id="defi">DeFi</h4>
<p>DeFi 即去中心化金融，崛起于 2020 年夏天，因此那段时间也被称为 DeFi Summer。根据 TradingView 的统计数据，2020 年夏天刚崛起时，DeFi 总市值仅 50 亿美元，随后一路飙升，在 2021 年底达到了最高峰，将近 1800 亿美元。</p>
<figure data-type="image" tabindex="4"><img src="https://jobslee0.github.io/post-images/1678961534210.png" alt="" loading="lazy"></figure>
<p>DeFi 有很多细分板块，主要包括：<strong>稳定币、交易所、衍生品、借贷、聚合器、保险、预测市场、指数</strong>等。</p>
<p>稳定币主要可分为三类：<strong>中心化稳定币、超额抵押稳定币、算法稳定币</strong>。其中，超额抵押稳定币和算法稳定币为<strong>去中心化稳定币</strong>。</p>
<p>中心化稳定币直接与法定货币挂钩，由中心化机构所发行，要求每单位稳定币需要有 1:1 的法币储备。目前交易量最大的两个稳定币 USDT 和 USDC，都是法币抵押稳定币，与美元 1:1 挂钩，分别由 Tether 和 Circle 两家中心化机构所发行。另外，币安，全球第一大中心化数字货币交易所，联合 Paxos 发行了自己的法币抵押稳定币 BUSD，目前也是全球交易量排名第三的稳定币，仅次于 USDT 和 USDC。</p>
<p>超额抵押稳定币通过超额抵押其他加密货币而锻造，抵押品会被锁定在智能合约里，智能合约会根据抵押品的价值锻造出对应数量的稳定币，智能合约依靠价格预言机来维持与法币的锚定。此类型的稳定币主要以 DAI 为代表，由 MakerDAO 推出，和美元保持 1:1 锚定，目前交易量排名第四。</p>
<p>算法稳定币则比较新颖，顾名思义，主要是通过算法来控制稳定币的供应。此赛道的选手也不少，包括 UST、FEI、AMPL、ESD、BAC、FRAX、CUSD、USDD、USDN 等，但目前还没有一个真正实现稳定的算法稳定币出现。</p>
<p>接着，来聊聊交易所，DeFi 里的交易所是指<strong>去中心化交易所</strong>，简称 <strong>DEX</strong>。DEX 是 DeFi 所有板块里市值占比最高的板块，也是 DeFi 的基石板块。如果对 DEX 再进一步细分，还可以分为现货 DEX 和衍生品 DEX，衍生品 DEX 主要交易永续合约或期权。如果从交易模式上划分，那 DEX 主要可分为两种：<strong>Orderbook 模式</strong>和 <strong>AMM 模式</strong>。Orderbook 模式的 DEX，主要包括 dYdX、apeX、0x、Loopring 等。AMM 模式的 DEX 则比较多了，主要包括 Uniswap、SushiSwap、PancakeSwap、Curve、Balancer、Bancor、GMX、Perpetual 等。</p>
<p>Orderbook 模式是最早出现的交易类型，交易方式和股票盘口的买卖方式一样，交易用户可选择成为挂单者（maker）或吃单者（taker），交易会根据价格优先和时间优先的规则撮合成交。采用 Orderbook 的 DEX，根据其发展历程主要还可以再分为三种模式：<strong>纯链上撮合+结算模式、链下撮合+链上结算模式、Layer2 模式</strong>。</p>
<p>纯链上撮合结算模式，用户提交的挂单和吃单都是直接在链上，吃单会直接和链上的挂单成交。该模式的代表为 EtherDelta，其优点是完全链上，去中心化程度高，但缺点是交易性能很低且交易成本很贵，用户挂单、撤单都需要支付燃料费。</p>
<p>链下撮合+链上结算模式的代表则是 0x 协议，相比于第一种模式，主要多了链下的「中继器」角色，用户通过链下签名的方式生成委托单并提交给中继器，由中继器来维护 Orderbook，撮合成功的委托单再由中继器提交到链上进行结算。因为将撮合移到了链下处理，大大提高了交易性能，但结算是一笔笔单独结算的，所以结算的性能成为了瓶颈。</p>
<p>Layer2 模式的代表为 dYdX，背后所使用的技术主要由 StarkWare 所提供的产品 StarkEx 所支持。其基本原理就是部署一个单独的、专用的 Layer2，用户的撮合交易和结算都发生在这个 Layer2 上，然后定时将所有交易记录（包括结算记录）全部打包生成证明并发送到 Layer1 上进行验证。与 Layer2 公链不同，Layer2 公链提供的是通用交易，而 dYdX 背后所使用的这个 Layer2 只能用于专用的交易场景，这其实算是个私有链，也可称为应用链，这也是一种新的应用模式。这种模式的交易体验和中心化交易所已经相差无几了，但中心化程度比较高。</p>
<p>完全去中心化且交易体验也较好的交易模式，目前主流的就是 AMM 模式了，AMM 为 Automated Market Maker 的简称，也称为自动做市商模式。引爆 AMM 模式的是 Uniswap，于 2018 年 11 月上线，之后的 SushiSwap、PancakeSwap、Curve 等都是基于 Uniswap 的模式进行改造。该模式需要流动性池作支撑，流动性提供者（简称 LP）往交易池里注入资产作为流动性，其实就是资金池，然后用户直接和流动性池进行交易，而 LP 则从中赚取用户的交易手续费。</p>
<p>关于交易所暂时就先聊这么多，接着来看看<strong>衍生品</strong>。DeFi 衍生品板块主要包括几个方向：<strong>永续合约、期权、合成资产、利率衍生品</strong>。</p>
<p>永续合约也是期货合约，加了杠杆的交易产品，前面提到的 dYdX、apeX、GMX、Perpetual 就是知名的几个永续合约 DEX。期权比期货复杂，DeFi 期权领域的玩家主要包括 Hegic、Charm、Opium、Primitive、Opyn 等，但目前期权市场还很小，被关注的不多。合成资产是由一种或多种资产/衍生品组合并进行代币化的加密资产，早期主要合成 DAI、WBTC 等数字资产，后面基于现实世界中的股票、货币、贵金属等的合成资产也越来越多，目前该赛道的龙头项目是 Synthetix，另外还有 Mirror、UMA、Linear、Duet、Coinversation 等项目。DeFi 的利率衍生品主要是基于加密资产利率开发不同类型的衍生产品，以满足 DeFi 用户对确定性收益的不同需求，主要玩家有 BarnBridge、Swivel Finance、Element Finance 等。</p>
<p>接着来看看<strong>借贷</strong>，这也是 TVL 很高的一个版块，和 DEX 一样也是 DeFi 的基石。这块的借贷协议主要有 Compound、Aave、Maker、Cream、Liquity、Venus、Euler、Fuse 等。目前，大部分借贷协议都是采用<strong>超额抵押的借贷模型</strong>，所谓超额抵押，举个例子，比如，要借出 80 美元的资产，那至少需要存入价值 100 美元的抵押资产，即抵押资产价值要高于借贷资产价值。</p>
<p>虽然超额抵押模型是主流，但也存在几个创新方向：<strong>无息贷款、资产隔离池、跨链借贷、信用贷</strong>。无息贷款的代表为 Liquity，用户在 Liquity 借出其稳定币 LUSD 的时候，用户一次性支付借款和赎回费用，借出后无需支付利息。资产隔离池就是将不同的借贷资产分开为不同的池子，每个借贷池都是独立的，避免一个不良资产或者一个池子受损导致整个平台都被连累。目前，资产隔离池差不多已经成为了标配，很多借贷协议都引入了这种模式，除了一开始就使用这种模式的 Fuse，包括 Compound、Aave、Euler 等协议也都加入了阵营。跨链借贷也是一个新趋势，Flux、Compound、Aave 等都在这个方向上进行拓展。信用贷在传统金融非常普遍，但在 DeFi 领域还比较少，主要是还缺乏有效的链上信用体系，目前的代表项目是 Wing Finance。</p>
<p>下一个是<strong>聚合器</strong>，DeFi 聚合器也分为好几种类型：<strong>DEX 聚合器、收益聚合器、资产管理聚合器、信息聚合器</strong>。DEX 聚合器，主要就是将多个 DEX 聚合到一起，通过算法从中寻找出最优的交易路径，主流的 DEX 聚合器包括 1inch、Matcha、ParaSwap，以及 MetaMask 钱包内置的 MetaMask Swap 等。收益聚合器主要有 Yearn Finance、Alpha Finance、Harvest Finance、Convex Finance 等，主要就是聚合各种流动性挖矿，让参与多平台的 Yield Farming（收益耕作）实现自动化。资产管理聚合器主要就是监控、跟踪和管理 DeFi 用户的资产和负债，主要以 Zapper 和 Zerion 为代表。最后是信息聚合器，主要包括 CoinMarketCap、DeFiPulse、DeBank、DeFiPrime 等平台。另外，这些其实都是中心化数据平台，但其在 DeFi 生态里依然扮演了重要角色，DeFi 生态里并非全都是去中心化的应用。</p>
<p>然后，再简单聊聊<strong>保险</strong>。我们知道，保险在传统金融中是非常大的一块市场，但 DeFi 里的保险发展至今，却是非常缓慢。整个 Web3 行业里，各种风险很多，协议漏洞风险、项目跑路风险、监管风险等，所以实际上对 DeFi 保险的需求市场本身很大，但因为开发设计门槛高，且流动性比较低，所以才导致整个保险赛道发展缓慢，目前依然处于非常早期的阶段，Nexus Mutual、Cover、Unslashed、Opium 等项目是该领域主要的玩家。</p>
<p>然后，再看看<strong>预测市场</strong>。预测市场是依托数据的市场，可用于押注和预测未来的所有事件，也是以太坊生态最早出现的应用场景之一，并在 2020 年美国大选中迎来爆发式增长，主要项目有 PolyMarket、Augur、Omen 等。</p>
<p>最后就是<strong>指数</strong>板块，提供一揽子资产敞口的指数基金在 DeFi 领域逐渐兴起。但广为人知的指数其实并不多，主要有：DPI、sDEFI、PIPT、DEFI++。DPI 全称为 DeFi Pulse Index，是由 DeFi Pulse 和 Set Protocol 合作创建的，是一种市值加权指数，包含了一些主流 DeFi 协议代币作为基础资产，包括 Uniswap、Aave、Maker、Synthetix、Loopring、Compound、Sushi 等。DPI 可以赎回为一揽子基础资产。sDEFI 则是由 Synthetix 所推出的指数代币，是该领域历史最悠久的指数。sDEFI 是一种合成资产，它不持有任何基础代币，而是使用预言机喂价来跟踪代币价值。PIPT 全称为 Power Index Pool Token，是由 PowerPool 所发行，由 8 种代币资产所组成。PowerPool 发行的指数除了 PIPT，另外还有 Yearn Lazy Ape Index、Yearn Ecosystem Token Index 和 ASSY Index 三个指数。DEFI++ 则是由 PieDAO 所发行，其组成有 14 种资产。PieDAO 还发行了 BCP 和 PLAY，BCP 由 WBTC、WETH、DEFI++ 三种代币组成，PLAY 则由一些元宇宙项目的代币所组成。</p>
<h4 id="gamefi">GameFi</h4>
<p>GameFi 从字面上理解就是 Game Finance，是游戏和金融的融合体，也是目前 Web3 游戏的代名词。GameFi 这个词语诞生之前，Web3 游戏则通常被称为区块链游戏，或简称链游。</p>
<p>CryptoKitties 是第一款广为人知的区块链游戏，这是一款虚拟养猫的养成类游戏，每一只猫咪都是一个独立的 NFT。初代猫咪总共有 50000 只，每只猫咪都有不同的属性。玩家购买猫咪 NFT 后，就可以开始玩繁殖小猫的游戏。生出来的小猫咪，有部分基因属性会遗传自上一代，而有些基因则随机生成。生出来的猫咪本质上就是新的 NFT，可以卖出变现。如果生成的新猫咪产生了稀有的基因属性，还可以卖到不错的价格。截止撰文之日（2023 年 1 月底），已经产生了 2,021,774 只猫咪，持有的钱包地址有 136,283。</p>
<p>继 CryptoKitties 之后，越来越多养成类游戏陆续出现，如加密狗、加密兔、加密青蛙等等。打破这种局面的是一款叫 Fomo3D 的游戏，这是一款公开、透明、去中心化的博彩资金盘游戏。游戏规则也简单，用户通过支付 ETH 购买 Key 参与游戏，用户支付的 ETH 会分配到奖池、分红池、空投池、官方池等。拥有 Key 则可以得到持续的分红，拥有的 Key 越多，则得到的分红会越多。且每轮游戏存在一个倒计时（24 小时），倒计时结束时，最后一个购买 Key 的玩家可以获得奖池里大部分的 ETH。但每次有用户购买 Key，则倒计时剩余时间会增加 30 秒。第一轮游戏持续了很久时间，最后被人用技术手段赢走了奖池。Fomo3D 爆火之后，也是各种优化升级版的同类游戏不断出现，但事实证明，这类游戏还是无法持久。</p>
<p>而之后，再次引爆市场的游戏则是 Axie Infinity，国内则被称为“阿蟹”（与 Axie 谐音）。这是一款结合了宝可梦和加密猫玩法的游戏，游戏里的 Axies 可以升级、繁殖、对战、交易等。与加密猫等游戏不同的是，Axie Infinity 的经济系统里还引入了 SLP 和 AXS 代币，玩家可通过战斗赢取 SLP 代币，而通过消耗 SLP 和 AXS 可以繁殖新的 Axies，赢取的 SLP 代币和繁殖出来的 Axies 都可以在市场上出售来赚取收入。</p>
<p>不过，Axie Infinity 其实在 2018 年就已经问世，但直到 2021 年才开始爆红，让其爆红的主要原因在于它的 Play-To-Earn 模式被推广开来了，即边玩边赚的特性呈病毒式传播了。其赚钱路径主要是先投入成本购买 Axies，然后通过玩游戏赚 SLP 代币和繁殖新的 Axies，再把 SLP 代币和 Axies 出售换成 ETH 或稳定币，最终将 ETH 或稳定币换成法币。这种赚钱模式一开始是从菲律宾逐渐流行起来的，当时，新冠疫情爆发，菲律宾当地许多人陷入了无收入的困境，而 Axie Infinity 的边玩边赚特性让这些人看到了希望。而且，这种赚钱模式也吸引了众多打金工作室，且逐渐从菲律宾扩展到了印度、印度尼西亚、巴西、中国等。截止撰文之日，日活用户已达 280 万。</p>
<p>而现在，边玩边赚模式几乎成为了 Web3 游戏的标配。</p>
<p>其他比较知名的游戏还有 Decentraland、The Sandbox、Illuvium、Star Atlas、Alien Worlds 等。这些就不展开说了，感兴趣的可以自行去搜索了解。</p>
<h4 id="socialfi">SocialFi</h4>
<p>SocialFi 顾名思义就是 Social Finance，是社交和金融在 Web3 领域的有机结合，其实就是去中心化社交，是近两年才开始流行的概念。目前，在这个赛道的知名项目还比较少，目前的龙头是 Lens Protocol。</p>
<p>Lens Protocol 是由 Aave 团队所开发的，在 2022 年 5 月上线。它不是一个独立的社交应用，也不是一个带有前端的完整社交产品，而是提供了一系列模块化组件的社交图谱平台，而具体的应用产品可以采用这些组件去构建。所以，Lens 的定义其实是 Web3 社交应用的基础设施。上线之初就已经拥有了 50 多个生态项目，比较热门的有 Lenster、Lenstube、ORB、Phaver、re:meme、Lensport、Lensta 等。</p>
<p>Lenster 是去中心化社交媒体应用，可以通过连接 Web3 钱包并使用 Lens 来登录。登录用户就可以在 Lenster 发布内容，和在微博或推特发布内容类似，不同的是，在 Lenster 上发布内容时可选择收费。也可以评论其他用户的内容，不过目前还不支持层级式的评论。</p>
<p>Lenstube 则是去中心化视频平台，可以理解为就是去中心化的 Youtube。</p>
<p>ORB 是去中心化职业社交媒体应用，具有端到端链上信誉系统。具体来说，ORB 可以通过将各种 NFT 和 POAP 与用户经验、教育、技能和项目联系起来，从而创建个人去中心化专业档案并建立链上可信度，以及探索工作机会和申请链上身份，还可以用在链上分享自己的想法，与 Web3 人士建立联系并构建社区。此外，ORB 还允许用户利用碎片化时间通过学习 Web3 知识来获取 NFT，即 Learn-to-Earn。</p>
<p>Phaver 是一款适用于 iOS 和 Android 的 Share-to-Earn 社交应用，用户可以发布帖子，内容可以是图片、链接、产品应用等。用户还可以浏览 Lens 内的所有内容。Lens Profile 用户连接钱包后，还可以通过 Phaver 直接发布帖子到 Lens 中。</p>
<p>re:meme 是一个链上 meme 生成器，允许用户上传 meme 模版，也能选择是否收费，然后其他人可以用图像编辑器添加文本、绘图和补充图像等。:meme 还可以扩展到音乐、视频和学术论文等媒体格式。</p>
<p>Lensport 是一个只聚焦于 Lens 协议的社交 NFT 市场，用户可以发现、发布和出售帖子，也可以投资支持创作者。</p>
<p>Lensta 是一个聚焦于 Lens 协议的图片流应用，可以浏览 Lens 中带有图片的最新、最热门以及 Lenster、Lensport 等上收集费用最多的帖子。</p>
<h3 id="访问层">访问层</h3>
<p>访问层是 Web3 组成架构里的最上层，也是直接面向终端用户的入口层。这一层里主要包括钱包、浏览器、聚合器等，另外，有一些 Web2 的社交媒体平台也成为了 Web3 的入口。</p>
<p>先来看看钱包，这也是最主要的入口。目前的钱包有多种分类，有浏览器钱包、手机钱包、硬件钱包、多签钱包、MPC 钱包、智能合约钱包等。</p>
<p>浏览器钱包就是通过网络浏览器使用的加密钱包，是大部分用户使用最广泛的钱包，最常用的就是 MetaMask、Coinbase Wallet、WalletConnect 等。MetaMask 是最被广泛支持的钱包之一，支持所有的 EVM 链，也已经成为了所有 DApps 的标准，目前支持的浏览器包括 Chrome、Brave、Firefox、Edge，以浏览器插件的方式存在。Coinbase Wallet 顾名思义是由交易所 Coinbase 所发行的钱包，于 2021 年 11 月推出后迅速发展，成为了与 MetaMask 旗鼓相当的对手，但浏览器还只支持 Chrome。WalletConnect 则比较特殊，它并不是一款具体的钱包应用，而是连接 DApps 和钱包的开源协议。最常用的就是用于连接手机钱包，在浏览器上的 DApp 选择连接 WalletConnect，会展示一个二维码，用你的手机钱包扫这个二维码就可以授权你的手机钱包连接上浏览器上的 DApp。而且，WalletConnect 支持所有区块链，不只是 EVM 链，也支持接入所有钱包。另外，不像 MetaMask 和 Coinbase Wallet 需要安装其浏览器插件，WalletConnect 不需要安装浏览器插件，所以可以支持所有浏览器，比如也支持 Safari，而 MetaMask 和 Coinbase Wallet 是不支持 Safari 的。因此，WalletConnect 成为了最受欢迎的钱包，也成为了所有 DApp 接入钱包的标配。</p>
<p>手机钱包，即移动端数字资产钱包，很多钱包都支持。MetaMask 和 Coinbase Wallet 也有手机端的钱包 App。另外，比较知名的手机钱包还有 TokenPocket、BitKeep、Rainbow、imToken、Crypto.com 等。大部分流行的手机钱包都支持多链，包括 EVM 链，也包括 Non-EVM 链，比如 TokenPocket 目前支持了 Bitcoin、Ethereum、BSC、TRON、Polygon、Arbitrum、Avalanche、Solana、Cosmos、Polkadot、Aptos 等。</p>
<p>硬件钱包则是把数字资产私钥存储在安全的硬件设备中，与互联网隔离，可通过 USB 即插即用。现在使用最广泛的硬件钱包是 Ledger 和 Trezor。Ledger 目前有三款不同型号的硬件钱包：Ledger Stax、Ledger Nano X、Ledger Nano S Plus。Ledger Stax 是在 2023 年才推出的新型号，支持触摸屏，而另外两款则不支持。Trezor 则有两款型号：Trezor Model T 和 Trezor Model One。Model T 支持触摸屏。除了 Ledger 和 Trezor，市面上的硬件钱包还有 SafePal、OneKey、imKey、KeepKey、ColdLar 等。</p>
<p>多签钱包，顾名思义，是指需要多人签名才能执行操作的钱包。最知名的多签钱包就是 Gnosis Safe，其本质上是一套链上智能合约，最常用的就是 2/3 签名，即总共有三个用户共同管理钱包，每次执行操作时，需要这三人中至少两个人的签名才能触发链上执行。</p>
<p>MPC 全称为 Multi-Party Computation，MPC 钱包也称为多方计算钱包，是新一代钱包类型，通过对私钥进行多方计算在链下实现多签和跨链等复杂的验证方式。简单来说，就是将私钥拆分成多个分片，然后由多方各自存储管理每个分片，签名的时候，再联合多方将分片重新拼接成完整的私钥。MPC 钱包与多签签名很类似，也可以实现 2/3 签名，不同的是，多签钱包是在智能合约层面实现签名校验，而 MPC 钱包则是通过链下计算实现的。目前已提供 MPC 钱包服务的还不多，主要有 ZenGo、Safeheron、Fordefi、OpenBlock、web3auth 等。</p>
<p>智能合约钱包就是使用智能合约账户作为地址的钱包，多签钱包 Gnosis Safe 也属于智能合约钱包。而近一两年对智能合约钱包最新的尝试则是结合「账户抽象（Account Abstraction）」的新一代钱包。账户抽象主要是要将签名者和账户分离开来，钱包地址不再与唯一的私钥强绑定，可以实现更换签名者，也可以实现多签，还可以实现更换签名算法。目前在这个赛道的选手除了 Gnosis Safe 还有 UniPass、Argent、Blocto 等。</p>
<p>钱包暂时就聊这么多，接着来说说浏览器。很多 DApp 都还是只提供了网页版本的前端，所以浏览器就成为了重要的访问入口。但因为不是所有浏览器都支持钱包扩展插件，所以也不是所有浏览器都能成为很好的 Web3 入口。最常用的浏览器是 Chrome，所有浏览器钱包都会开发 Chrome 的钱包插件。而 Safari 则很少用做 Web3 DApp 入口，因为除了 WalletConnect，没有其他浏览器钱包能够支持。还有一个值得介绍的浏览器是 Brave，这是一款内置了钱包的浏览器，其内置钱包叫 Brave Wallet。</p>
<p>有一些聚合器也是 Web3 的访问入口，比如 DappRadar 收集了各种 DApps，用户可以通过它浏览并连接到这些 DApps。还有 Zapper、DeBank、Zerion 之类的聚合器，可以帮助用户追踪他们在各种 Web3 应用的所有资产和操作记录。</p>
<p>最后，像 Twitter 和 Reddit 这类 Web3 的社交媒体平台，因为聚集了很多 Web3 社群，也逐渐变成了 Web3 的访问入口。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[B树与B+树]]></title>
        <id>https://jobslee0.github.io/post/b-shu-yu-bshu/</id>
        <link href="https://jobslee0.github.io/post/b-shu-yu-bshu/">
        </link>
        <updated>2023-03-16T09:35:23.000Z</updated>
        <content type="html"><![CDATA[<p>B树和B+树都是常见的数据结构，用于在磁盘等外存储介质上存储数据并进行高效的查询、插入和删除操作。它们的主要区别在于：</p>
<ol>
<li>结构差异</li>
</ol>
<p>B树的每个节点包含一个键值和一个指向子节点的指针数组。而B+树的每个非叶子节点只包含键值，而不包含指向数据的指针。所有的数据都保存在叶子节点中，并且叶子节点通过一个指针链表连接在一起。</p>
<ol start="2">
<li>叶子节点区别</li>
</ol>
<p>B树的叶子节点包含指向数据的指针和数据本身，而B+树的叶子节点只包含数据本身。这使得B+树的叶子节点能够更好地利用磁盘块，因为每个节点只存储数据而不需要指针信息。</p>
<ol start="3">
<li>遍历方式</li>
</ol>
<p>在B树中，由于每个节点都包含指向子节点的指针，因此在进行遍历时需要进行递归操作。而在B+树中，由于数据只存储在叶子节点中，因此在进行遍历时只需要遍历叶子节点即可。</p>
<ol start="4">
<li>查询性能</li>
</ol>
<p>由于B+树的叶子节点只包含数据本身，因此在进行范围查询时只需要遍历叶子节点，而不需要进行中间节点的遍历。这使得B+树在范围查询时具有更高的查询性能。</p>
<p>总的来说，B+树比B树更适合于磁盘等外存储介质上的数据存储和查询，因为它具有更好的磁盘块利用率和查询性能。而B树则更适用于内存中的数据结构。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[几种常见的IO模型]]></title>
        <id>https://jobslee0.github.io/post/ji-chong-chang-jian-de-io-mo-xing/</id>
        <link href="https://jobslee0.github.io/post/ji-chong-chang-jian-de-io-mo-xing/">
        </link>
        <updated>2023-03-16T09:34:08.000Z</updated>
        <content type="html"><![CDATA[<h3 id="传统io">传统IO</h3>
<figure data-type="image" tabindex="1"><img src="https://jobslee0.github.io/post-images/1678959301605.png" alt="" loading="lazy"></figure>
<h3 id="reactor事件驱动">Reactor事件驱动</h3>
<figure data-type="image" tabindex="2"><img src="https://jobslee0.github.io/post-images/1678959307216.png" alt="" loading="lazy"></figure>
<h3 id="reactor-业务io分离">Reactor-业务IO分离</h3>
<figure data-type="image" tabindex="3"><img src="https://jobslee0.github.io/post-images/1678959312699.png" alt="" loading="lazy"></figure>
<h3 id="reactor-并发读写">Reactor-并发读写</h3>
<figure data-type="image" tabindex="4"><img src="https://jobslee0.github.io/post-images/1678959318911.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSQ队列核心原理分析]]></title>
        <id>https://jobslee0.github.io/post/psq/</id>
        <link href="https://jobslee0.github.io/post/psq/">
        </link>
        <updated>2023-03-16T09:30:08.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p><a href="https://github.com/wjw465150/PSQueueServer">PSQ-Github地址</a></p>
</blockquote>
<p>PSQ是友缘在线早些年，由大师独自编写的一个轻量级队列，该队列开箱即用、效率高，可以达到每秒并发40000-50000个请求，并开创性提供了队列回溯消费的功能。</p>
<h3 id="技术架构">技术架构</h3>
<figure data-type="image" tabindex="1"><img src="https://jobslee0.github.io/post-images/1678959201761.png" alt="" loading="lazy"></figure>
<h3 id="核心原理">核心原理</h3>
<ol>
<li>底层数据结构：最底层使用了leansoft.bigqueue，此队列基于内存映射文件，实现了LRU及页映射特性</li>
<li>数据结构扩展：基于原始队列，构建环形队列，并支持回溯、持久化（使用NIO）特性</li>
<li>网络接口：网络接口基于Netty实现，根据参数定制对应消息对象，由于NIO的模型特性，网络IO效率非常的高</li>
<li>服务包装：服务使用tanukisoftware.wrapper进行了包装，启动比较轻巧，可以很好的跟操作系统融合</li>
<li>扩展特性：系统使用JMX扩展进行了Bean的监控和管理，有效提高了系统的可操作性</li>
</ol>
<h3 id="磁盘与内存">磁盘与内存</h3>
<p>在 LeanSoft BigQueue 中，Memory mapped page 是一种利用内存映射文件实现的存储方式。它允许应用程序映射一个磁盘文件到内存，从而避免从磁盘读取数据的慢速过程。通过使用 Memory mapped page，LeanSoft BigQueue 可以在内存和磁盘之间平衡读写性能，从而提高吞吐量。</p>
<p>Java 中可以使用 <code>FileChannel</code> 类刷新直接内存到磁盘。你可以使用该类打开一个文件，并使用 <code>map</code> 方法将该文件映射到直接内存。然后，你可以使用该内存中的数据进行操作，并通过调用 <code>force</code> 方法将其写入磁盘。</p>
<p>以下是使用直接内存和 FileChannel 类的示例代码：</p>
<pre><code class="language-java">import java.io.File;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;

public class DirectMemoryExample {

    public static void main(String[] args) throws IOException {
        File file = new File(&quot;data.dat&quot;);

        // Open the file and map it to memory
        RandomAccessFile raf = new RandomAccessFile(file, &quot;rw&quot;);
        FileChannel channel = raf.getChannel();
        MappedByteBuffer buffer = channel.map(FileChannel.MapMode.READ_WRITE, 0, 1024);

        // Write data to the memory
        buffer.putInt(100);
        buffer.putInt(200);

        // Flush the changes to disk
        buffer.force();

        // Close the file
        channel.close();
        raf.close();
    }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[记一次JVM泄漏问题及解决方案]]></title>
        <id>https://jobslee0.github.io/post/ji-yi-ci-jvm-xie-lou-wen-ti-ji-jie-jue-fang-an/</id>
        <link href="https://jobslee0.github.io/post/ji-yi-ci-jvm-xie-lou-wen-ti-ji-jie-jue-fang-an/">
        </link>
        <updated>2023-01-10T09:35:56.000Z</updated>
        <content type="html"><![CDATA[<h4 id="问题现象">问题现象</h4>
<p>前段时间，公司线上的服务器开始出现pod反复重启的现象，通过普罗米修斯监控大盘，发现是JVM内存突破了pod限制。</p>
<h4 id="问题分析">问题分析</h4>
<h5 id="现象分析">现象分析</h5>
<ol>
<li>硬件配置</li>
</ol>
<p>我们的应用服务是使用k8s来部署的，应用的实现是jdk8，出现问题的服务pod分配的总数为3台，每个pod分配3G内存和1个高性能CPU，pod的内存required和limit分别为2G和3G。</p>
<ol start="2">
<li>JVM参数</li>
</ol>
<p>默认参数，未配置。</p>
<ol start="3">
<li>JVM内存分析</li>
</ol>
<p>年轻代正常；老年代占用较多，可能有大对象存在；老年代内存增长同pod增长趋势相同，且有按小时增长迹象，并会触发FullGC。</p>
<h5 id="问题猜测">问题猜测</h5>
<ol>
<li>随着业务增长，pod内存可能确实不足</li>
<li>JVM默认参数无法有效限制JVM的内存使用</li>
<li>某个小时任务存在问题，且使用了大对象</li>
<li>大对象触发FullGC后并没有有效释放内存</li>
<li>查询k8s官方Issues，是否存在pod与Java版本兼容bug</li>
</ol>
<h5 id="尝试解决">尝试解决</h5>
<ol>
<li>增加pod内存，使得<code>required=4G limit=5G</code></li>
<li>增加JVM参数限制<code>-Xms=4G -Xmx=4G</code>，堆外内存也要限制，（特别注意jdk8变为了元空间）<code>-XX:MetaspaceSize=300m -XX:MaxMetaspaceSize=300m</code></li>
<li>翻查代码，确实发现某个定时任务存在着大批量的List内存存储，且长时间不释放，但是短时间内没有更好的修改替代方案</li>
<li>调整年轻代大小<code>-Xmn</code>，使大对象在年轻代就被回收，而不进入老年代</li>
<li>升级jdk8的小版本，提升jvm对容器限制的感知（这块见参考文档3）</li>
</ol>
<h5 id="尝试结果">尝试结果</h5>
<ol>
<li>JVM内存增长导致的重启次数变得减少一点，部分FullGC之后pod容器依然健在，但是时间久了依然会被kill重启</li>
<li>大对象没有被提前回收，依然进入了老年代，且FullGC明显增多</li>
</ol>
<h5 id="初步结论">初步结论</h5>
<ol>
<li>业务增长pod内存确实需要适当增加</li>
<li>JVM参数需要进行手动限制，但是年轻代大小可以不需要调整，反而可以降低FullGC次数</li>
<li>代码端想更好的优化方法，尽量让数据对象生命周期缩短，但是改动依然比较难</li>
<li>jdk8小版本有对容器限制感知的修复，有一定用处（这块见参考文档3）</li>
</ol>
<h4 id="问题解决">问题解决</h4>
<p>保持上面的解决参数和思路，我们的服务勉强保持了一段时间，但是告警和重启依然存在，实在令人头疼，很明显还是有一部分JVM的内存泄漏。</p>
<p>后来通过不断的排查监控大盘，我发现堆内内存经过FullGC后，都可以做到到达临界线后不再增长，这说明堆内参数大小和限制都起到了很好的作用；但是要知道，JVM还有一部分叫做堆外内存的东西，参数中限制了我们常知道的MetaSpace，在大盘中我却发现了non-heap memory部分总有比MetaSpace多出的300M，其增长趋势同pod内存一样一致，只是不是那么明显。</p>
<p>这引发了我进一步的思考，堆外内存是不是除了元数据区，还有一块未曾想到的区域？</p>
<p>经过翻查资料：</p>
<figure data-type="image" tabindex="1"><img src="https://jobslee0.github.io/post-images/1673343775722.png" alt="" loading="lazy"></figure>
<p>发现有一块叫做Direct Memory的区域，这块区域就是我们常提到的NIO为了减少内存拷贝而直接申请的部分，这部分在常见数据库读写客户端中大量存在。</p>
<p>通过排查代码，果不其然，代码除了保留List大对象之外，还大量使用了RedisTemplate对象，这个客户端底层连接基于netty实现，正是NIO那部分。</p>
<p>于是，加上<code>-MaxDirectMemorySize</code>参数后，再次测试，JVM内存稳固被限制，果然没有问题了！</p>
<h4 id="问题总结">问题总结</h4>
<p>总的来看，这次的问题<code>DirectMemory</code>是一个主要问题，代码的<code>大对象处理</code>是一个次要问题，其次才应该考虑对JVM的<code>FullGC</code>次数进行优化。</p>
<p><strong>被忽略的<code>DirectMemory</code>因为默认为堆大小的1/4，加上我们大量基于<code>RedisTemplate</code>中NIO机制的Redis读写和本身POD的大小有限，最终这部分未被考虑的内存不断增长，导致最后POD内存溢出被kill掉，而这个过程因为本身还未达到JVM本身内存大小的限制，所以也是没有看到JVM的OOM日志的原因。</strong></p>
<p>这里再对<code>DirectMemory</code>进行一个简单的总结：</p>
<ol>
<li>Direct Memory 不是由Java虚拟机直接管理的，而是由操作系统管理。因此，Direct Memory 不会受到Java虚拟机的垃圾回收机制的影响，它的回收是由操作系统自行处理的。</li>
<li>虽然Direct Memory不是由Java虚拟机管理的，但是Java虚拟机会在启动时将一块内存注册到操作系统中，用于存放 Direct Memory，这块内存也可以被限制大小；同样，Direct Memory在必要时需要手动进行释放，否则会出现内存泄漏的情况。</li>
<li>JVM中的Direct Memory是通过Native Memory来实现的，即直接在操作系统中申请的内存空间，而不是通过Java Heap来分配的。通常，Direct Memory可以通过调用java.nio.Buffer类的clean()方法进行释放。此外，也可以通过反射来调用sun.misc.Cleaner类的clean()方法来释放Direct Memory。在JDK9中，引入了一个新的API，即jdk.internal.misc.Unsafe类的invokeCleaner()方法，该方法可以用于显式地释放Direct Memory。调用该方法会触发Cleaner机制，异步清理Direct Memory。当该内存块没有被引用时，Cleaner线程将清理该内存块。</li>
<li><code>MaxDirectMemorySize</code> 是控制 Direct Memory 最大分配空间的参数，它的默认值与 Java 堆最大空间有关，通常为 Java 堆最大空间的1/4。当应用程序申请的 Direct Memory 超过了这个值时，将会抛出 <code>OutOfMemoryError</code> 异常。</li>
</ol>
<p>JVM是一个复杂的大工程，它帮我们很好的完成了对内存的管理；出现了内存泄漏不可怕，要从现象和问题本身出发，要从JVM本身出发，万万不能被所谓的“八股文”和“权威”给束缚住，上来就考虑GC优化。</p>
<p>那些被刻板经验给忽略的，往往就是真相，仅此而已。</p>
<h5 id="参考文档">参考文档</h5>
<ol>
<li>https://blog.csdn.net/tterminator/article/details/54342666</li>
<li>https://zhuanlan.zhihu.com/p/370241822</li>
<li>https://blog.51cto.com/lookingdream/4046529</li>
<li>https://juejin.cn/post/6844903894863052814</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[算法学习笔记]]></title>
        <id>https://jobslee0.github.io/post/suan-fa-xue-xi-bi-ji/</id>
        <link href="https://jobslee0.github.io/post/suan-fa-xue-xi-bi-ji/">
        </link>
        <updated>2023-01-10T07:46:31.000Z</updated>
        <content type="html"><![CDATA[<p>常见时间复杂度</p>
<ul>
<li>一个顺序结构的代码，时间复杂度是 O(1)</li>
<li>二分查找，或者更通用地说是采用分而治之的二分策略，时间复杂度都是 O(logn)</li>
<li>一个简单的 for 循环，时间复杂度是 O(n)</li>
<li>两个顺序执行的 for 循环，时间复杂度是 O(n)+O(n)=O(2n)，其实也是 O(n)</li>
<li>两个嵌套的 for 循环，时间复杂度是 O(n²)</li>
</ul>
<p>时间复杂度演进</p>
<ul>
<li>第一步，暴力解法。在没有任何时间、空间约束下，完成代码任务的开发。</li>
<li>第二步，无效操作处理。将代码中的无效计算、无效存储剔除，降低时间或空间复杂度。</li>
<li>第三步，时空转换。设计合理数据结构，完成时间复杂度向空间复杂度的转移。</li>
</ul>
<p>3 个基本操作，增、删、查；3个基本思考，设计合理数据结构的方法论</p>
<ul>
<li>首先，这段代码对数据进行了哪些操作？</li>
<li>其次，这些操作中，哪个操作最影响效率，对时间复杂度的损耗最大？</li>
<li>最后，哪种数据结构最能帮助你提高数据操作的使用效率？</li>
</ul>
<p>解决代码问题的方法论；宏观上，它可以分为以下 4 个步骤：</p>
<ol>
<li><strong>复杂度分析</strong>（估算问题中复杂度的上限和下限）</li>
<li><strong>定位问题</strong>（根据问题类型，确定采用何种算法思维）</li>
<li><strong>数据操作分析</strong>（根据增、删、查和数据顺序关系去选择合适的数据结构，利用空间换取时间）</li>
<li><strong>编码实现</strong></li>
</ol>
<hr>
<p>线性表</p>
<ul>
<li>
<p>种类</p>
<ul>
<li>单向链表</li>
<li>循环链表</li>
<li>双向链表</li>
<li>双向循环链表</li>
</ul>
</li>
<li>
<p>基础操作</p>
</li>
</ul>
<pre><code class="language-C">// 增
s.next = p.next;
p.next = s;
// 删
p.next = p.next.next;
// 查O(n)
</code></pre>
<ul>
<li>常见方法</li>
</ul>
<pre><code class="language-C">// 翻转
while(curr){
    next = curr.next;
    curr.next = prev；
    prev = curr;
    curr = next;
}
// 中间节点/环（快慢指针）
while(fast &amp;&amp; fast.next &amp;&amp; fast.next.next){
    fast = fast.next.next;
    slow = slow.next;
}
</code></pre>
<ul>
<li>
<p>使用场景</p>
<ul>
<li>线性表对数据的存储方式是按照顺序的存储</li>
<li>当数据的元素个数不确定，且需要经常进行数据的新增和删除时，那么链表会比较合适</li>
</ul>
</li>
</ul>
<p>栈</p>
<ul>
<li>
<p>特性</p>
<ul>
<li>后进先出</li>
<li>栈顶（top）</li>
<li>栈底（bottom）</li>
<li>继承了线性表的优点与不足，是个限制版的线性表，只允许数据从栈顶进出</li>
</ul>
</li>
<li>
<p>种类</p>
<ul>
<li>顺序栈（借助数组来实现）</li>
<li>链栈（用链表的方式对栈的表示）</li>
</ul>
</li>
<li>
<p>基础操作</p>
<ul>
<li>push 或压栈</li>
<li>pop 或出栈</li>
</ul>
</li>
<li>
<p>使用场景</p>
<ul>
<li>需要高频使用新增、删除操作，且新增和删除操作的数据执行顺序具备后来居上的相反关系时，栈就是个不错的选择</li>
<li>浏览器的前进和后退，括号匹配等问题</li>
</ul>
</li>
</ul>
<p>队列</p>
<ul>
<li>
<p>特性</p>
<ul>
<li>先进先出</li>
<li>队头（front）</li>
<li>队尾（rear）</li>
<li>当队列为空时，front 和 rear 都指向头结点</li>
<li>有了头结点后，哪怕队列为空，头结点依然存在，能让 front 指针和 rear 指针依然有意义</li>
<li>继承了线性表的优点与不足，是加了限制的线性表，队列的增和删的操作只能在这个线性表的头和尾进行</li>
</ul>
</li>
<li>
<p>种类</p>
<ul>
<li>顺序队列（依赖数组来实现）</li>
<li>链式队列（依赖链表来实现）</li>
<li>循环队列（约瑟夫环问题）</li>
</ul>
</li>
<li>
<p>使用场景</p>
<ul>
<li>在可以确定队列长度最大值时，建议使用循环队列</li>
<li>无法确定队列长度时，应考虑使用链式队列</li>
<li>很像现实中人们排队买票的场景，在面对数据处理顺序非常敏感的问题时，队列一定是个不错的技术选型</li>
</ul>
</li>
</ul>
<p>数组</p>
<ul>
<li>
<p>特性</p>
<ul>
<li>定义简单，访问方便</li>
<li>所有元素类型必须相同</li>
<li>最大长度必须在定义时给出</li>
<li>使用的内存空间必须连续</li>
</ul>
</li>
<li>
<p>基础操作</p>
<ul>
<li>增加：若插入数据在最后，则时间复杂度为 O(1)；如果中间某处插入数据，则时间复杂度为 O(n)。</li>
<li>删除：对应位置的删除，扫描全数组，时间复杂度为 O(n)。</li>
<li>查找：如果只需根据索引值进行一次查找，时间复杂度是 O(1)；但是要在数组中查找一个数值满足指定条件的数据，则时间复杂度是 O(n)</li>
</ul>
</li>
<li>
<p>使用场景</p>
<ul>
<li>在数据数量确定，即较少甚至不需要使用新增数据、删除数据操作的场景下使用，这样就有效地规避了数组天然的劣势</li>
<li>在数据对位置敏感的场景下，比如需要高频根据索引位置查找数据时，数组就是个很好的选择了</li>
</ul>
</li>
<li>
<p>链表存在的价值</p>
<ul>
<li>首先，链表的长度是可变的，数组的长度是固定的，在申请数组的长度时就已经在内存中开辟了若干个空间；如果没有引用 ArrayList 时，数组申请的空间永远是我们在估计了数据的大小后才执行，所以在后期维护中也相当麻烦</li>
<li>其次，链表不会根据有序位置存储，进行插入数据元素时，可以用指针来充分利用内存空间；数组是有序存储的，如果想充分利用内存的空间就只能选择顺序存储，而且需要在不取数据、不删除数据的情况下才能实现</li>
</ul>
</li>
</ul>
<p>字符串</p>
<ul>
<li>
<p>类型</p>
<ul>
<li>顺序存储（用一组地址连续的存储单元来存储串中的字符序列，一般是用定长数组来实现，有些语言会在串值后面加一个不计入串长度的结束标记符，比如 \0 来表示串值的终结）</li>
<li>链式存储（一个结点可以考虑存放多个字符，如果最后一个结点未被占满时，可以使用 &quot;#&quot; 或其他非串值字符补全）</li>
</ul>
</li>
<li>
<p>基础操作</p>
<ul>
<li>
<p>在线性表的基本操作中，大多以“单个元素”作为操作对象</p>
</li>
<li>
<p>在字符串的基本操作中，通常以“串的整体”作为操作对象</p>
</li>
<li>
<p>增O(n)</p>
</li>
<li>
<p>删O(n)</p>
</li>
<li>
<p>查</p>
<ul>
<li>子串查找（字符串匹配）（这种匹配算法需要从主串中找到跟模式串的第 1 个字符相等的位置，然后再去匹配后续字符是否与模式串相等O(nm)）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>树</p>
<ul>
<li>
<p>特性</p>
<ul>
<li>结点和边</li>
<li>不存在环</li>
<li>深度</li>
<li>层</li>
<li>一对多</li>
</ul>
</li>
<li>
<p>类型</p>
<ul>
<li>
<p>链式存储法</p>
</li>
<li>
<p>顺序存储法（结点 X 的下标为 i，那么 X 的左子结点总是存放在 2 * i 的位置，X 的右子结点总是存放在 2 * i + 1 的位置）</p>
</li>
<li>
<p>二叉树（每个结点最多有两个分支，即每个结点最多有两个子结点，分别称作左子结点和右子结点）O(n)</p>
<ul>
<li>
<p>满二叉树，定义为除了叶子结点外，所有结点都有 2 个子结点</p>
</li>
<li>
<p>完全二叉树，定义为除了最后一层以外，其他层的结点个数都达到最大，并且最后一层的叶子结点都靠左排列（对于一棵完全二叉树而言，仅仅浪费了下标为 0 的存储位置。而如果是一棵非完全二叉树，则会浪费大量的存储空间）</p>
<ul>
<li>
<p>二叉查找树（也称作二叉搜索树）O(logn)</p>
<ul>
<li>左小右大</li>
<li>规避相等</li>
<li>中序有序</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>基础操作</p>
<ul>
<li>
<p>前序遍历（根左右）</p>
</li>
<li>
<p>中序遍历（左根右）</p>
</li>
<li>
<p>后序遍历（左右根）</p>
</li>
<li>
<p>二叉查找树（删除）</p>
<ul>
<li>
<p>情况一，如果要删除的结点是某个叶子结点，则直接删除，将其父结点指针指向 null 即可</p>
</li>
<li>
<p>情况二，如果要删除的结点只有一个子结点，只需要将其父结点指向的子结点的指针换成其子结点的指针即可</p>
</li>
<li>
<p>情况三，如果要删除的结点有两个子结点，则有两种可行的操作方式</p>
<ul>
<li>第一种，找到这个结点的左子树中最大的结点，替换要删除的结点</li>
<li>第二种，找到这个结点的右子树中最小的结点，替换要删除的结点</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>哈希表</p>
<ul>
<li>
<p>特性</p>
<ul>
<li>
<p>哈希表的核心思想：如果有一种方法，可以实现“地址 = f (关键字)”的映射关系，那么就可以快速完成基于数据的数值的查找了O(1）</p>
</li>
<li>
<p>hash方法</p>
<ul>
<li>直接定制法（H (key) = a*key + b。 这里，a 和 b 是设置好的常数）</li>
<li>数字分析法（每个关键字 key 都是由 s 位数字组成（k1,k2,…,Ks），并从中提取分布均匀的若干位组成哈希地址）</li>
<li>平方取中法（先求关键字的平方值，通过平方扩大差异，然后取中间几位作为最终存储地址）</li>
<li>折叠法（取它们的叠加和的值（舍去进位）作为哈希地址）</li>
<li>除留余数法（key mod p）</li>
</ul>
</li>
<li>
<p>解决冲突</p>
<ul>
<li>开放定址法</li>
<li>链地址法</li>
</ul>
</li>
</ul>
</li>
<li>
<p>基础操作</p>
<ul>
<li>对于给定的 key，通过哈希函数计算哈希地址 H (key)</li>
</ul>
</li>
<li>
<p>使用场景</p>
<ul>
<li>如果不需要有序遍历数据，并且可以提前预测数据量的大小，那么哈希表在速度和易用性方面是无与伦比的</li>
</ul>
</li>
</ul>
<hr>
<p>递归（Recursion）</p>
<ul>
<li>
<p>递归的基本思想：把规模大的问题转化为<strong>规模小的相同的子问题</strong>来解决</p>
</li>
<li>
<p>这个解决问题的函数必须有明确的<strong>结束条件</strong>，否则就会导致无限递归的情况</p>
</li>
<li>
<p>递归的实现包含了两个部分，一个是<strong>递归主体</strong>，另一个是<strong>终止条件</strong></p>
</li>
<li>
<p>数学归纳法</p>
<ul>
<li>
<p>当一个问题同时满足以下 2 个条件时，就可以使用递归的方法求解</p>
<ul>
<li>可以拆解为除了数据规模以外，求解思路完全相同的子问题</li>
<li>存在终止条件</li>
</ul>
</li>
</ul>
</li>
<li>
<p>写出递归代码的关键在于</p>
<ul>
<li>写出递推公式</li>
<li>找出终止条件</li>
</ul>
</li>
<li>
<p>递归的应用非常广泛，很多数据结构和算法的编码实现都要用到递归，例如分治策略、快速排序等</p>
</li>
</ul>
<p>分治</p>
<ul>
<li>
<p>分而治之</p>
</li>
<li>
<p>互相独立</p>
</li>
<li>
<p>形式相同</p>
</li>
<li>
<p>原问题都需要具备以下几个特征：</p>
<ul>
<li>难度在降低</li>
<li>问题可分</li>
<li>解可合并</li>
<li>相互独立（各个子问题之间相互独立，某个子问题的求解不会影响到另一个子问题）</li>
</ul>
</li>
<li>
<p>分治法在每轮递归上，都包含了<strong>分解问题</strong>、<strong>解决问题</strong>和<strong>合并结果</strong>这 3 个步骤</p>
</li>
<li>
<p>二分查找O(logn)</p>
<ul>
<li>
<p><strong>输入的数列是有序的</strong></p>
<ul>
<li>选择一个标志 i 将集合 L 分为二个子集合，一般可以使用中位数</li>
<li>判断标志 L(i) 是否能与要查找的值 des 相等，相等则直接返回结果</li>
<li>如果不相等，需要判断 L(i) 与 des 的大小</li>
<li>基于判断的结果决定下步是向左查找还是向右查找；如果向某个方向查找的空间为 0，则返回结果未查到</li>
<li>回到初始步骤</li>
</ul>
</li>
<li>
<p>二分查找的时间复杂度是 O(logn)（当你面对某个代码题，而且约束了时间复杂度是 O(logn) 或者是 O(nlogn) 时，可以想一下分治法是否可行）</p>
</li>
<li>
<p>二分查找的循环次数并不确定，一般是达到某个条件就跳出循环（编码的时候，多数会采用 while 循环加 break 跳出的代码结构）</p>
</li>
<li>
<p>二分查找处理的原问题必须是有序的（当你在一个有序数据环境中处理问题时，可以考虑分治法；相反，如果原问题中的数据并不是有序的，则使用分治法的可能性就会很低了）</p>
</li>
</ul>
</li>
<li>
<p>在面对陌生问题时，需要注意原问题的<strong>数据是否有序</strong>，预期的时间复杂度是否带有 <strong>logn 项</strong>，是否可以通<strong>过小问题的答案合并出原问题的答案</strong>。如果这些先决条件都满足，你就应该第一时间想到分治法</p>
</li>
</ul>
<p>排序</p>
<ul>
<li>冒泡排序</li>
<li>插入排序</li>
<li>归并排序</li>
<li>快速排序</li>
</ul>
<p>动态规划</p>
<ul>
<li>
<p>问题的解决难度与数据规模有关</p>
<ul>
<li>原问题可被分解</li>
<li>子问题的解可以合并为原问题的解</li>
<li><sub>所有的子问题相互独立</sub></li>
</ul>
</li>
<li>
<p>运筹学方法，多轮决策过程中的最优方法</p>
</li>
<li>
<p>最短路径问题</p>
</li>
<li>
<p><strong>状态</strong></p>
</li>
<li>
<p>通用的方法论（k 表示多轮决策的第 k 轮）</p>
<ul>
<li>分阶段，将原问题划分成几个子问题（一个子问题就是多轮决策的一个阶段，它们可以是不满足独立性的）</li>
<li>找状态，选择合适的状态变量 Sk（它需要具备描述多轮决策过程的演变，更像是决策可能的结果）</li>
<li>做决策，确定决策变量 uk（每一轮的决策就是每一轮可能的决策动作，例如 D2 的可能的决策动作是 D2 -&gt; E2 和 D2 -&gt; E3）</li>
<li>状态转移方程（这个步骤是动态规划最重要的核心，即 sk+1= uk(sk) ）</li>
<li>定目标（写出代表多轮决策目标的指标函数 Vk,n）</li>
<li>寻找终止条件</li>
</ul>
</li>
<li>
<p>补充概念</p>
<ul>
<li>策略（每轮的动作是决策，多轮决策合在一起常常被称为策略）</li>
<li>策略集合（由于每轮的决策动作都是一个变量，这就导致合在一起的策略也是一个变量；我们通常会称所有可能的策略为策略集合；动态规划的目标，也可以说是从策略集合中，找到最优的那个策略）</li>
</ul>
</li>
<li>
<p>如下几个特征的问题，可以采用动态规划求解</p>
<ul>
<li><strong>最优子结构</strong>（它的含义是，原问题的最优解所包括的子问题的解也是最优的；例如，某个策略使得 A 到 G 是最优的。假设它途径了 Fi，那么它从 A 到 Fi 也一定是最优的）</li>
<li><strong>无后效性</strong>（某阶段的决策，无法影响先前的状态；可以理解为今天的动作改变不了历史）</li>
<li><strong>有重叠子问题</strong>（也就是，子问题之间不独立；**这个性质是动态规划区别于分治法的条件；**如果原问题不满足这个特征，也是可以用动态规划求解的，无非就是杀鸡用了宰牛刀）</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trouble Shoot]]></title>
        <id>https://jobslee0.github.io/post/trouble-shoot/</id>
        <link href="https://jobslee0.github.io/post/trouble-shoot/">
        </link>
        <updated>2023-01-10T07:46:05.000Z</updated>
        <content type="html"><![CDATA[<p>Swagger2 and Spring-Healthy-Check Integration Issues</p>
<p>KeyWords:</p>
<pre><code class="language-Bash">org.springframework.context.ApplicationContextException: Failed to start bean 'documentationPluginsBootstrapper'; nested exception is java.lang.NullPointerException
</code></pre>
<p>Version:</p>
<pre><code class="language-XML">&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;/artifactId&gt;
  &lt;version&gt;2.6.3&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.springfox&lt;/groupId&gt;
    &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;
    &lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Describe:</p>
<p>After the two components are integrated,  we open the spring-healthy-check and will see the &quot;Exception&quot; due to the healthy-check bug.</p>
<p>Solution:</p>
<pre><code class="language-Java">/**
 * 解决swagger配合健康检查时，无法启动的问题
 * @param webEndpointsSupplier
 * @param servletEndpointsSupplier
 * @param controllerEndpointsSupplier
 * @param endpointMediaTypes
 * @param corsProperties
 * @param webEndpointProperties
 * @param environment
 * @return
 */
@Bean
public WebMvcEndpointHandlerMapping webEndpointServletHandlerMapping(WebEndpointsSupplier webEndpointsSupplier, ServletEndpointsSupplier servletEndpointsSupplier, ControllerEndpointsSupplier controllerEndpointsSupplier, EndpointMediaTypes endpointMediaTypes, CorsEndpointProperties corsProperties, WebEndpointProperties webEndpointProperties, Environment environment) {
    List&lt;ExposableEndpoint&lt;?&gt;&gt; allEndpoints = new ArrayList&lt;&gt;();
    Collection&lt;ExposableWebEndpoint&gt; webEndpoints = webEndpointsSupplier.getEndpoints();
    allEndpoints.addAll(webEndpoints);
    allEndpoints.addAll(servletEndpointsSupplier.getEndpoints());
    allEndpoints.addAll(controllerEndpointsSupplier.getEndpoints());
    String basePath = webEndpointProperties.getBasePath();
    EndpointMapping endpointMapping = new EndpointMapping(basePath);
    boolean shouldRegisterLinksMapping = webEndpointProperties.getDiscovery().isEnabled() &amp;&amp;
            (org.springframework.util.StringUtils.hasText(basePath) || ManagementPortType.get(environment).equals(ManagementPortType.DIFFERENT));
    return new WebMvcEndpointHandlerMapping(endpointMapping, webEndpoints, endpointMediaTypes, corsProperties.toCorsConfiguration(), new EndpointLinksResolver(allEndpoints, basePath), shouldRegisterLinksMapping, null);
}
</code></pre>
<p>Reference: https://github.com/springfox/springfox/issues/3462</p>
<hr>
<p>Sharding-JDBC and Spring-Healthy-Check Integration Issues</p>
<p>KeyWords:</p>
<pre><code class="language-Bash">WARN  o.s.boot.actuate.jdbc.DataSourceHealthIndicator:94 -  DataSource health check failed
org.springframework.dao.InvalidDataAccessApiUsageException: ConnectionCallback; isValid; nested exception is java.sql.SQLFeatureNotSupportedException: isValid
</code></pre>
<p>Version:</p>
<pre><code class="language-XML">&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;/artifactId&gt;
  &lt;version&gt;2.6.3&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt;
  &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt;
  &lt;version&gt;4.1.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Describe:</p>
<p>After the two components are integrated, we open the spring-healthy-check and will see the &quot;Exception&quot; due to the sharding-jdbc was not implementing the healthy-check api.</p>
<p>Solution:</p>
<pre><code class="language-Java">/**
 * 数据库检查配置
 */
@Configuration
public class DbCheckConfig {
    /**
     * 对数据库检测做默认处理（健康健康检查用到sharding-jdbc时，该组件没有完全实现MySQL驱动会导致问题）
     */
    @Bean
    public DataSourcePoolMetadataProvider dataSourcePoolMetadataProvider() {
        return dataSource -&gt; new DefaultDataSourcePoolMetadata();
    }

    /**
     * 默认的数据源池元数据
     */
    private static class DefaultDataSourcePoolMetadata implements DataSourcePoolMetadata {
        @Override
        public Float getUsage() {
            return null;
        }

        @Override
        public Integer getActive() {
            return null;
        }
        @Override
        public Integer getMax() {
            return null;
        }

        @Override
        public Integer getMin() {
            return null;
        }

        @Override
        public String getValidationQuery() {
            // 用于检查的简单查询语句
            return &quot;select 1 from dual&quot;;
        }

        @Override
        public Boolean getDefaultAutoCommit() {
            return null;
        }
    }
}
// or
@SpringBootApplication(exclude = {DataSourceHealthContributorAutoConfiguration.class})
</code></pre>
<p>Reference: https://www.cnblogs.com/laeni/p/16089788.html</p>
<hr>
<p>Swagger2 and Spring-Healthy-Check Integration Issues</p>
<p>KeyWords:</p>
<pre><code class="language-bash">org.springframework.context.ApplicationContextException: Failed to start bean 'documentationPluginsBootstrapper'; nested exception is java.lang.NullPointerException
</code></pre>
<p>Version:</p>
<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;/artifactId&gt;
  &lt;version&gt;2.6.3&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.springfox&lt;/groupId&gt;
    &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;
    &lt;version&gt;3.0.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Describe:</p>
<p>After the two components are integrated, we open the spring-healthy-check and will see the &quot;Exception&quot; due to the healthy-check bug.</p>
<p>Solution:</p>
<pre><code class="language-java">/**
 * 解决swagger配合健康检查时，无法启动的问题
 * @param webEndpointsSupplier
 * @param servletEndpointsSupplier
 * @param controllerEndpointsSupplier
 * @param endpointMediaTypes
 * @param corsProperties
 * @param webEndpointProperties
 * @param environment
 * @return
 */
@Bean
public WebMvcEndpointHandlerMapping webEndpointServletHandlerMapping(WebEndpointsSupplier webEndpointsSupplier, ServletEndpointsSupplier servletEndpointsSupplier, ControllerEndpointsSupplier controllerEndpointsSupplier, EndpointMediaTypes endpointMediaTypes, CorsEndpointProperties corsProperties, WebEndpointProperties webEndpointProperties, Environment environment) {
    List&lt;ExposableEndpoint&lt;?&gt;&gt; allEndpoints = new ArrayList&lt;&gt;();
    Collection&lt;ExposableWebEndpoint&gt; webEndpoints = webEndpointsSupplier.getEndpoints();
    allEndpoints.addAll(webEndpoints);
    allEndpoints.addAll(servletEndpointsSupplier.getEndpoints());
    allEndpoints.addAll(controllerEndpointsSupplier.getEndpoints());
    String basePath = webEndpointProperties.getBasePath();
    EndpointMapping endpointMapping = new EndpointMapping(basePath);
    boolean shouldRegisterLinksMapping = webEndpointProperties.getDiscovery().isEnabled() &amp;&amp;
            (org.springframework.util.StringUtils.hasText(basePath) || ManagementPortType.get(environment).equals(ManagementPortType.DIFFERENT));
    return new WebMvcEndpointHandlerMapping(endpointMapping, webEndpoints, endpointMediaTypes, corsProperties.toCorsConfiguration(), new EndpointLinksResolver(allEndpoints, basePath), shouldRegisterLinksMapping, null);
}
</code></pre>
<p>Reference: https://github.com/springfox/springfox/issues/3462</p>
<p>Sharding-JDBC and Spring-Healthy-Check Integration Issues</p>
<p>KeyWords:</p>
<pre><code class="language-bash">WARN  o.s.boot.actuate.jdbc.DataSourceHealthIndicator:94 -  DataSource health check failed
org.springframework.dao.InvalidDataAccessApiUsageException: ConnectionCallback; isValid; nested exception is java.sql.SQLFeatureNotSupportedException: isValid
</code></pre>
<p>Version:</p>
<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;/artifactId&gt;
  &lt;version&gt;2.6.3&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt;
  &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt;
  &lt;version&gt;4.1.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Describe:</p>
<p>After the two components are integrated, we open the spring-healthy-check and will see the &quot;Exception&quot; due to the sharding-jdbc was not implementing the healthy-check api.</p>
<p>Solution:</p>
<pre><code class="language-java">/**
 * 数据库检查配置
 */
@Configuration
public class DbCheckConfig {
    /**
     * 对数据库检测做默认处理（健康健康检查用到sharding-jdbc时，该组件没有完全实现MySQL驱动会导致问题）
     */
    @Bean
    public DataSourcePoolMetadataProvider dataSourcePoolMetadataProvider() {
        return dataSource -&gt; new DefaultDataSourcePoolMetadata();
    }

    /**
     * 默认的数据源池元数据
     */
    private static class DefaultDataSourcePoolMetadata implements DataSourcePoolMetadata {
        @Override
        public Float getUsage() {
            return null;
        }

        @Override
        public Integer getActive() {
            return null;
        }
        @Override
        public Integer getMax() {
            return null;
        }

        @Override
        public Integer getMin() {
            return null;
        }

        @Override
        public String getValidationQuery() {
            // 用于检查的简单查询语句
            return &quot;select 1 from dual&quot;;
        }

        @Override
        public Boolean getDefaultAutoCommit() {
            return null;
        }
    }
}
// or
@SpringBootApplication(exclude = {DataSourceHealthContributorAutoConfiguration.class})
</code></pre>
<p>Reference: https://www.cnblogs.com/laeni/p/16089788.html</p>
<p>Sharding-JDBC Load Meta Data Slowly</p>
<p>KeyWords:</p>
<pre><code class="language-bash">ShardingSphere-metadata                  : Meta data load finished, cost 471600 milliseconds.
</code></pre>
<p>Version:</p>
<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt;
  &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt;
  &lt;version&gt;4.1.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Describe:</p>
<p>When spring application starts, the application always waits for a long time and then shows the log above. Actually, the load time could be shortened by config.</p>
<p>Solution:</p>
<pre><code class="language-yaml">spring.shardingsphere.props.max.connections.size.per.query = 50
</code></pre>
<p>Reference: https://github.com/apache/shardingsphere/issues/6212</p>
<p>The Apollo-Config is Unable to Update the Configuration of Application</p>
<p>KeyWords:</p>
<pre><code class="language-bash">com.ctrip.framework.apollo
jasypt-spring-boot-starter
Unable to Update the Configuration
</code></pre>
<p>Version:</p>
<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;com.ctrip.framework.apollo&lt;/groupId&gt;
  &lt;artifactId&gt;apollo-client&lt;/artifactId&gt;
  &lt;version&gt;1.9.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
   &lt;groupId&gt;com.github.ulisesbocchio&lt;/groupId&gt;
   &lt;artifactId&gt;jasypt-spring-boot-starter&lt;/artifactId&gt;
   &lt;version&gt;3.0.4&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Describe:</p>
<p>After using <code>jasypt-spring-boot-starter</code> component, I found that the modifies of Apollo-Config could not been updated in Spring-Application. It's because the high version of <code>jasypt-spring-boot-starter</code> conflicts with <code>com.ctrip.framework.apollo</code>, the detail in the Reference below.</p>
<p>Solution:</p>
<pre><code class="language-plain">Use the version less than 2.2.0 for jasypt-spring-boot-starter(Please pay attention to the difference between the version 2.0.0 and the version 3.3.0 for the encryption algorithm).
Implement the ConfigChangeListener in Apollo by yourself.
</code></pre>
<p>Reference:</p>
<p>https://github.com/apolloconfig/apollo/issues/2162</p>
<p>https://blog.csdn.net/qingquanyingyue/article/details/109197386</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MongoDB实战问答]]></title>
        <id>https://jobslee0.github.io/post/mongodb-shi-zhan-wen-da/</id>
        <link href="https://jobslee0.github.io/post/mongodb-shi-zhan-wen-da/">
        </link>
        <updated>2023-01-10T07:04:07.000Z</updated>
        <content type="html"><![CDATA[<h4 id="mongodb底层实现是b树还是b树">MongoDB底层实现是B树还是B+树？</h4>
<p><strong>B+树</strong></p>
<p>这个问题国内论坛真的是难以找到靠谱的答案，大部分给出的都是不准确的B树，当然B+树确实是B树的一种，但是准确的说当前版本MongoDB底层实现应该是B+树才是准确的</p>
<p>下面来给出具体的说明：</p>
<p>我们知道MongoDB当下支持两种引擎，一个是默认的WiredTiger，另一个是In-Memory，In-Memory是保存在内存中，我们平时讨论和使用的是默认的WiredTiger（<u>本讨论从MongoDB3.2将WiredTiger作为默认引擎的前提下展开，至于老版本的数据结构这里忽略掉</u>）</p>
<p>WiredTiger文档中有这么一句话：</p>
<figure data-type="image" tabindex="1"><img src="https://jobslee0.github.io/post-images/1673344193920.png" alt="" loading="lazy"></figure>
<p>大概意思就是<code>使用称为B-Tree（具体为B+Tree）的数据结构在内存中维护表的数据</code>，这直接说明了具体的数据结构应该为B+树，只不过<strong>B+树是B树的一种变形</strong>而已</p>
<h4 id="mongodb大数据量分页怎么做">MongoDB大数据量分页怎么做？</h4>
<ol>
<li><code>count</code>语法会导致全局扫表，不建议使用<code>count</code>做页码总数的统计</li>
<li>对于不带条件的全局数据统计，建议使用<code>estimatedDocumentCount</code></li>
<li>使用自定义带索引的标识或者<code>_id</code>中包含的时间戳，作为范围查询的条件，并往后limit有限条数据</li>
</ol>
<h4 id="mongodb是否需要自行实现_id来提升性能">MongoDB是否需要自行实现_id来提升性能？</h4>
<p><strong>不需要</strong>，<code>_id</code>基于MongoDB自定义的规则实现，插入时直接生成，无需检查重复，效率已经足够</p>
<p>具体可参照前两篇MongoDB文章</p>
<h4 id="mongodb读写分离有延迟怎么办">MongoDB读写分离有延迟怎么办？</h4>
<ol>
<li>检查服务器间网络是否正常</li>
<li>检查服务器磁盘是否正常</li>
<li>通过节点opLog日志排查是否同步正常</li>
<li>升级4.4版本使用<code>流复制</code>提升效率</li>
<li>读写分离这种主从部署的方式，在新版本中已经不建议使用，建议使用<code>副本集</code>进行读写</li>
</ol>
<h4 id="mongodb大数据量写索引慢有什么办法解决">MongoDB大数据量写索引慢有什么办法解决？</h4>
<ol>
<li>MongoDB在写入索引的时候存在阻塞，所以最开始设计就要考虑合理的索引设计</li>
<li>对于不太关注历史数据的数据库，可以使用部分索引，减少索引构建的数据量</li>
<li>定期清理数据，减少数据量</li>
</ol>
<h4 id="mongodb新版本有哪些需要关注的新特性">MongoDB新版本有哪些需要关注的新特性？</h4>
<ol>
<li><a href="https://www.mongodb.com/docs/manual/core/capped-collections/">6.0版本-上限集合</a></li>
<li><a href="https://www.mongodb.com/docs/manual/core/timeseries-collections/">5.0版本-时间序列</a></li>
<li><a href="https://www.mongodb.com/docs/manual/core/replica-set-sync/">4.4版本-流复制</a></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MongoDB与MySQL的插入、查询性能测试]]></title>
        <id>https://jobslee0.github.io/post/mongodb-yu-mysql-de-cha-ru-cha-xun-xing-neng-ce-shi/</id>
        <link href="https://jobslee0.github.io/post/mongodb-yu-mysql-de-cha-ru-cha-xun-xing-neng-ce-shi/">
        </link>
        <updated>2023-01-10T06:53:46.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>原文链接：https://www.cnblogs.com/wodeboke-y/p/10828985.html</p>
</blockquote>
<h2 id="71-平均每条数据的插入时间">7.1  平均每条数据的插入时间</h2>
<p>先上张图，来点直观感受：</p>
<figure data-type="image" tabindex="1"><img src="https://jobslee0.github.io/post-images/1673345059945.PNG" alt="" loading="lazy"></figure>
<p>图上数据横坐标是平均每插入1000条数据所需要的时间，单位是秒。记住，是每1000条数据，不是每条数据哦。</p>
<p>总结：</p>
<ol>
<li>数据库的平均插入速率：<strong>MongoDB不指定_id插入 &gt; MySQL不指定主键插入 &gt; MySQL指定主键插入 &gt; MongoDB指定_id插入</strong>。</li>
<li>MongoDB在指定_id与不指定_id插入时速度相差很大，而MySQL的差别却小很多。</li>
</ol>
<p>分析：</p>
<ol>
<li>在指定_id或主键时，两种数据库在插入时要对索引值进行处理，并查找数据库中是否存在相同的键值，这会减慢插入的速率。</li>
<li>在MongoDB中，指定索引插入比不指定慢很多，这是因为，MongoDB里每一条数据的_id值都是唯一的。当在不指定_id插入数据的时候，其_id是系统自动计算生成的。MongoDB通过计算机特征值、时间、进程ID与随机数来确保生成的_id是唯一的。而在指定_id插入时，MongoDB每插一条数据，都需要检查此_id可不可用，当数据库中数据条数太多的时候，这一步的查询开销会拖慢整个数据库的插入速度。</li>
<li>MongoDB会充分使用系统内存作为缓存，这是一种非常优秀的特性。我们的测试机的内存有64G，在插入时，MongoDB会尽可能地在内存快写不进去数据之后，再将数据持久化保存到硬盘上。这也是在不指定_id插入的时候，MongoDB的效率遥遥领先的原因。但在指定_id插入时，当数据量一大内存装不下时，MongoDB就需要将磁盘中的信息读取到内存中来查重，这样一来其插入效率反而慢了。</li>
<li>MySQL不愧是一种非常稳定的数据库，无论在指定主键还是在不指定主键插入的情况下，其效率都差不了太多。</li>
</ol>
<h2 id="72-插入稳定性分析">7.2  插入稳定性分析</h2>
<p>插入稳定性是指，随着数据量的增大，每插入一定量数据时的插入速率情况。</p>
<p>在本次测试中，我们把这个指标的规模定在10w，即显示的数据是在每插入10w条数据时，在这段时间内每秒钟能插入多少条数据。</p>
<p>先呈现四张图上来：</p>
<ol>
<li>MongoDB指定_id插入：</li>
</ol>
<figure data-type="image" tabindex="2"><img src="https://jobslee0.github.io/post-images/1673345102543.PNG" alt="" loading="lazy"></figure>
<ol start="2">
<li>MongoDB不指定_id插入：</li>
</ol>
<figure data-type="image" tabindex="3"><img src="https://jobslee0.github.io/post-images/1673345108127.PNG" alt="" loading="lazy"></figure>
<ol start="3">
<li>MySQL指定PRIMARY KEY插入：</li>
</ol>
<figure data-type="image" tabindex="4"><img src="https://jobslee0.github.io/post-images/1673345114526.PNG" alt="" loading="lazy"></figure>
<ol start="4">
<li>MySQL不指定PRIMARY KEY插入：</li>
</ol>
<figure data-type="image" tabindex="5"><img src="https://jobslee0.github.io/post-images/1673345119911.png" alt="" loading="lazy"></figure>
<p>总结：</p>
<ol>
<li>整体上的插入速度还是和上一回的统计数据类似：<strong>MongoDB不指定_id插入 &gt; MySQL不指定主键插入 &gt; MySQL指定主键插入 &gt; MongoDB指定_id插入</strong>。</li>
<li>从图中可以看出，在指定主键插入数据的时候，MySQL与MongoDB在不同数据数量级时，每秒插入的数据每隔一段时间就会有一个波动，在图表中显示成为规律的毛刺现象。而在不指定插入数据时，在大多数情况下插入速率都比较平均，但随着数据库中数据的增多，插入的效率在某一时段有瞬间下降，随即又会变稳定。</li>
<li>整体上来看，MongoDB的速率波动比MySQL的严重，方差变化较大。</li>
<li>MongoDB在指定_id插入时，当插入的数据变多之后，插入效率有明显地下降。在其他三种的插入测试中，从开始到结束，其插入的速率在大多数的时候都固定在一个标准上。</li>
</ol>
<p>分析：</p>
<ol>
<li>毛刺现象是因为，当插入的数据太多的时候，MongoDB需要将内存中的数据写进硬盘，MySQL需要重新分表。这些操作每当数据库中的数据达到一定量级后就会自动进行，因此每隔一段时间就会有一个明显的毛刺。</li>
<li>MongoDB毕竟还是新生事物，其稳定性没有已应用多年的MySQL优秀。</li>
<li>MongoDB在指定_id插入的时候，其性能的下降还是很厉害的。</li>
</ol>
<h2 id="73-mysql与mongodb读取性能的简单测试">7.3  MySQL与MongoDB读取性能的简单测试</h2>
<p>这是一个附加的测试，也并没有测试得非常完整，但还是很能说明一些问题的。</p>
<p>测试方法：</p>
<p>先在1 – 100, 000, 000这一亿个数中，分别随机取1w, 5w, 10w, 20w, 50w个互不相同的数字，再计算其md5值，并保存。</p>
<p>至于为什么最高只选到50w这个规模，这是因为我在随机生成100w个互不相同的数字的时候，写的脚本跑了一晚上都没有跑出来，估计是我生成的<a href="http://lib.csdn.net/base/datastructure" title="算法与数据结构知识库">算法</a>写得太烂了。我不想重新再弄了，暂就以50w为上限吧。</p>
<p>在上述带主键插入的两个数据库里，分别以上一步生成的md5源为输入进行查询操作。同样，每查询1000条数据在日志文件中将当前系统时间写入。</p>
<p>测试结果：</p>
<p>以下三张图的横坐标是每查询1000条数据所需要的时间，单位为s；纵坐标是查询的规模，分为1w, 5w,10w, 20w, 50w五个等级。</p>
<figure data-type="image" tabindex="6"><img src="https://jobslee0.github.io/post-images/1673345127429.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="7"><img src="https://jobslee0.github.io/post-images/1673345132663.png" alt="" loading="lazy"></figure>
<p>这张图是详细对比，可以看出MySQL与MongoDB之间的差异了吗……</p>
<figure data-type="image" tabindex="8"><img src="https://jobslee0.github.io/post-images/1673345138528.png" alt="" loading="lazy"></figure>
<p>总结：</p>
<ol>
<li>在读取的数据规模不大时，MongoDB的查询速度真是一骑绝尘，甩开MySQL好远好远。</li>
<li>在查询的数据量逐渐增多的时候，MySQL的查询速度是稳步下降的，而MongoDB的查询速度却有些起伏。</li>
</ol>
<p>分析：</p>
<ol>
<li>如果MySQL没有经过查询优化的话，其查询速度就不要跟MongoDB比了。MongoDB可以充分利用系统的内存资源，我们的测试机器内存是64GB的，内存越大MongoDB的查询速度就越快，毕竟磁盘与内存的I/O效率不是一个量级的。</li>
<li>本次实验的查询的数据也是随机生成的，因此所有待查询的数据都存在MongoDB的内存缓存中的概率是很小的。在查询时，MongoDB需要多次将内存中的数据与磁盘进行交互以便查找，因此其查询速率取决于其交互的次数。这样就存在这样一种可能性，尽管待查询的数据数目较多，但这段随机生成的数据被MongoDB以较少的次数从磁盘中取出。因此，其查询的平均速度反而更快一些。这样看来，MongoDB的查询速度波动也处在一个合理的范围内。</li>
<li>MySQL的稳定性还是毋庸置疑的。</li>
</ol>
<h2 id="8-测试总结">8. 测试总结</h2>
<h2 id="81-测试结论">8.1  测试结论</h2>
<ol>
<li>相比较MySQL，<strong>MongoDB数据库更适合那些读作业较重的任务模型</strong>。MongoDB能充分利用机器的内存资源。如果机器的内存资源丰富的话，MongoDB的查询效率会快很多。</li>
<li>在带”_id”插入数据的时候，MongoDB的插入效率其实并不高。如果想充分利用MongoDB性能的话，推荐采取不带”_id”的插入方式，然后对相关字段作索引来查询。</li>
</ol>
<h2 id="82-测试需要进一步注意的问题">8.2  测试需要进一步注意的问题</h2>
<p>对MongoDB的读取测试考虑不周，虽然这只是一个额外的测试。在这个测试中，随机生成大量待测试的数据很有必要，但生成大量互不相同的数据就没有必要了。正是这一点，把我的读取测试规模限定在了50w条，没能进一步进行分析。</p>
<h2 id="83-mongodb的优势">8.3  MongoDB的优势</h2>
<ol>
<li><strong>MongoDB适合那些对数据库具体数据格式不明确或者数据库数据格式经常变化的需求模型</strong>，而且对开发者十分友好。</li>
<li>MongoDB官方就自带一个分布式文件系统，可以很方便地部署到服务器机群上。MongoDB里有一个Shard的概念，就是方便为了服务器分片使用的。每增加一台Shard，MongoDB的插入性能也会以接近倍数的方式增长，磁盘容量也很可以很方便地扩充。</li>
<li>MongoDB还自带了对map-reduce运算框架的支持，这也很方便进行数据的统计。</li>
</ol>
<p>其他方面的优势还在发掘中，本人也是刚刚接触这个不久。</p>
<h2 id="84-mongodb的缺陷">8.4  MongoDB的缺陷</h2>
<ol>
<li>事务关系支持薄弱。这也是所有NoSQL数据库共同的缺陷，不过NoSQL并不是为了事务关系而设计的，具体应用还是很需求。</li>
<li>稳定性有些欠缺，这点从上面的测试便可以看出。</li>
<li>MongoDB一方面在方便开发者的同时，另一方面对运维人员却提出了相当多的要求。业界并没有成熟的MongoDB运维经验，MongoDB中数据的存放格式也很随意，等等问题都对运维人员的考验。</li>
</ol>
]]></content>
    </entry>
</feed>