{"posts":[{"title":"开源大语言模型汇总","content":"开源大语言模型汇总 原文链接：https://mp.weixin.qq.com/s/BQOJNwfkApiZnFveMDBQ-w #01 Alpaca/LLaMA（Meta/Stanford） **斯坦福 Alpaca：**一个遵循指令的 LLaMA 模型。 LLaMA 网站：https://ai.facebook.com/blog/large-language-model-llama-meta-ai/ Alpaca 网站：https://crfm.stanford.edu/2023/03/13/alpaca.html Alpaca GitHub：https://github.com/tatsu-lab/stanford_alpaca 能否用于商业用途：不能 以下是基于 Meta 的 LLaMA 项目或斯坦福大学的 Alpaca 项目的复制品或相关项目： Alpaca.cpp 在你的设备上本地快速运行一个类似于 ChatGPT 的模型。下面的录屏并未加速，而是实际运行在一台配有 4GB 权重的 M2 MacBook Air 上。 GitHub：https://github.com/antimatter15/alpaca.cpp Alpaca-LoRA 这个代码库包含了用低秩适应（LoRA）方法复现斯坦福 Alpaca 结果的代码。我们为树莓派（用于研究）提供了一个与 text-davinci-003 相似质量的 Instruct 模型，并且代码可以轻松地应用于 13b、30b 和 65b 模型。 GitHub：https://github.com/tloen/alpaca-lora Demo：https://huggingface.co/spaces/tloen/alpaca-lora AlpacaGPT4-LoRA-7B-OpenLLaMA Hugging Face：https://huggingface.co/LLMs LLMs Models：https://huggingface.co/LLMs Baize Baize 是一个使用低秩适应（LoRA）进行微调的开源聊天模型。它利用了由 ChatGPT 自我对话产生的 100,000 个对话数据。同时，我们还使用了 Alpaca 的数据来提高其性能表现。目前已经发布了 7B、13B 和 30B 的模型。 GitHub：https://github.com/project-baize/baize Paper：https://arxiv.org/pdf/2304.01196.pdf Cabrita 一款葡萄牙语微调的指令型 LLaMA 模型。 GitHub：https://github.com/22-hours/cabrita Chinese-LLaMA-Alpaca 为了推动中文 NLP 社区大模型的开放研究，该项目开源了中文 LLaMA 模型和经过指令微调的 Alpaca 大型模型。这些模型在原始 LLaMA 的基础上，扩展了中文词汇表并使用中文数据进行二次预训练，从而进一步提高了对中文基本语义理解的能力。同时，中文 Alpaca 模型还进一步利用中文指令数据进行微调，明显提高了模型对指令理解和执行的能力。具体详情请参阅技术报告（崔、杨、姚，2023）。 GitHub：https://github.com/ymcui/Chinese-LLaMA-Alpaca Chinese-Vicuna 一款基于 LLaMA 的中文遵循指令模型。 GitHub：https://github.com/Facico/Chinese-Vicuna GPT4-x-Alpaca GPT4-x-Alpaca 是一个经过 GPT4 对话与 GPTeacher 精细调整的 LLaMA 13B 模型。关于其训练和性能方面的资料相对较少。 Hugging Face：https://huggingface.co/chavinlo/gpt4-x-alpaca gpt4-x-vicuna-13b 作为基础模型，采用了 https://huggingface.co/eachadea/vicuna-13b-1.1。对 Teknium 的 GPTeacher 数据集、未发布的 Roleplay v2 数据集、GPT-4-LLM 数据集以及 Nous Research Instruct 数据集进行了微调。大约包含 180，000 条来自 GPT-4 的指令，已清除所有 OpenAI 审查 /“作为 AI 语言模型” 等相关内容。 Hugging Face：https://huggingface.co/NousResearch/gpt4-x-vicuna-13b GPT4All 这是一个训练助手式大语言模型的演示，基于 LLaMa，使用约 800k 个 GPT-3.5 Turbo 生成数据。 GitHub：https://github.com/nomic-ai/gpt4all GitHub：https://github.com/nomic-ai/pyllamacpp Review：https://www.youtube.com/watch?v=GhRNIuTA2Z0 GPTQ-for-LLaMA 使用 GPTQ 对 LLaMA 进行 4 位量化。GPTQ 是 SOTA 的单次权重量化方法。 GitHub：https://github.com/qwopqwop200/GPTQ-for-LLaMa Koala Koala 是基于 LLaMa 微调的语言模型。请查看下面 Blog，这篇文章介绍了下载、恢复 Koala 模型权重以及在本地运行 Koala 聊天机器人的过程。 Blog：https://bair.berkeley.edu/blog/2023/04/03/koala/ GitHub：https://github.com/young-geng/EasyLM/blob/main/docs/koala.md Demo：https://chat.lmsys.org/?model=koala-13b Review：https://www.youtube.com/watch?v=A4rcKUZieEU Review：https://www.youtube.com/watch?v=kSLcedGSez8 llama.cpp 使用纯 C/C++ 实现 LLaMa 模型的推理过程。 GitHub：https://github.com/ggerganov/llama.cpp 支持三种模型：LLaMA、Alpaca 和 GPT4All LLaMA-Adapter V2 LLaMA-Adapter：https://arxiv.org/pdf/2303.16199.pdf 和 LLaMA-Adapter V2：https://arxiv.org/pdf/2304.15010.pdf 已经发布。 GitHub：https://github.com/ZrrSkywalker/LLaMA-Adapter Lit-LLaMA ️ LLaMA 独立实现，完全开源且遵循 Apache 2.0 许可证。这个实现是在 nanoGPT 的基础上构建的。 GitHub：https://github.com/Lightning-AI/lit-llama OpenAlpaca 这是 OpenAlpaca 项目的代码仓库，旨在基于 OpenLLaMA 构建并分享一个指令跟随模型。与 OpenLLaMA 一样，OpenAlpaca 采用 Apache 2.0 许可证进行授权。该仓库包含以下内容： 用于微调模型的数据。 微调模型的代码。 微调模型的权重。 OpenAlpaca 的使用示例。 GitHub：https://github.com/yxuansu/OpenAlpaca OpenBuddy：面向所有人的开放式多语言聊天机器人 OpenBuddy 是一个功能强大的开源多语言聊天机器人模型，旨在为全球用户提供无缝的英语、中文和其他语言的会话 AI 和多语言支持。该模型基于 Facebook 的 LLAMA 模型构建，通过微调扩展了词汇表、增加了常用字符和改进了令牌嵌入。 OpenBuddy 利用这些改进和多轮对话数据集提供了一个强大的模型，可以回答各种语言的问题并执行翻译任务。 GitHub：https://github.com/OpenBuddy/OpenBuddy Pygmalion-7b Pygmalion 7B 是一个对话模型，基于 Meta 的 LLaMA-7B 模型构建。这是版本 1。使用 Pygmalion-6B-v8-pt4 数据集的一个子集对模型进行了微调，对于熟悉该项目的人而言，这一点很重要。 Hugging Face：https://huggingface.co/PygmalionAI/pygmalion-7b StableVicuna 我们自豪地介绍 StableVicuna，这是第一个通过强化学习从人类反馈中训练的大规模开源聊天机器人（RHLF）。StableVicuna 是 Vicuna v0 13b 的进一步指令微调和 RLHF 训练版本，而 Vicuna v0 13b 则是指令微调的 LLaMA 13b 模型。有兴趣的读者，可以阅读：https://vicuna.lmsys.org/ 网站：https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot Hugging Face：https://huggingface.co/spaces/CarperAI/StableVicuna Review：https://www.youtube.com/watch?v=m_xD0algP4k StackLLaMA 这是一个在 Stack Exchange 上使用 RLHF 训练的 LLaMa 模型，使用了三种方法的组合：监督微调（SFT）、奖励 / 偏好建模（RM）和人类反馈的强化学习（RLHF），训练数据包括问题和答案。 网站：https://huggingface.co/blog/stackllama The Bloke alpaca-lora-65B-GGML 对 changusung Alpaca-lora-65B 进行了 4 位和 2 位量化的 GGML 模型，以便在 CPU 上进行推理，同时使用 llama.cpp 实现。 Hugging Face：https://huggingface.co/TheBloke/alpaca-lora-65B-GGML The Bloke’s StableVicuna-13B-GPTQ 这个代码仓库包含 CarterAI StableVicuna 13B 的 4 位 GPTQ 格式量化模型。这个模型的生成过程首先将上述代码仓库中的增量与原始的 Llama 13B 权重合并，然后使用 GPTQ-for-LLaMa 进行 4 位量化。 Hugging Face：https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ The Bloke’s WizardLM-7B-uncensored-GPTQ 这些文件是 Eric Hartford “未经审查” 的 WizardLM 模型的 GPTQ 4 位模型文件，是使用 GPTQ-for-LLaMa 进行 4 位量化的结果。Eric 使用 WizardLM 方法对经过编辑的数据集进行了新的 7B 训练，该数据集删除了所有 “我很抱歉……” 类型的 ChatGPT 响应。 Hugging Face：https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ Vicuna（FastChat） 一款开源聊天机器人，能达到 ChatGPT 90% 的能力。 GitHub：https://github.com/lm-sys/FastChat Review：https://www.youtube.com/watch?v=4VByC2NpV30 Vigogne 这个代码仓库包含使用 Hugging Face 的 PEFT 库提供的低秩适应（LoRA）方法，复现了斯坦福大学 Alpaca 的法语版本的代码。除了 LoRA 技术之外，我们还使用 bitsandbytes 提供的 LLM.int8() 来将预训练语言模型（PLMs）量化为 int8。将这两种技术结合起来，使我们能够在单个消费级 GPU（如 RTX 4090）上微调 PLMs。 GitHub：https://github.com/bofenghuang/vigogne WizardLM 这是一个使用 Evol-Instruct 技术的指令跟随 LLM 模型，使得大型预训练语言模型能够遵循复杂的指令。 GitHub：https://github.com/nlpxucan/WizardLM Review：https://www.youtube.com/watch?v=5IAxCL4dHWk #02 BigCode StartCoder **BigCode 是一个开放的科学合作项目，旨在负责任地训练大语言模型，以应用于编码领域。**你可以在主要网站上找到更多信息，也可以在 Twitter 上关注 BigCode。 在这个组织中，你可以找到这个合作项目的工件，包括 StarCoder，一个用于编码的最先进的语言模型，The Stack，可用的最大的预训练数据集，包含宽容的代码，以及 SantaCoder，一个参数达到 1.1B 的编码模型。 网站：https://huggingface.co/bigcode Hugging Face：https://huggingface.co/spaces/bigcode/bigcode-playground #03 BLOOM（BigScience） BigScience大型开放科学开放获取多语言模型。 Hugging Face：https://huggingface.co/bigscience/bloom Hugging Face Demo：https://huggingface.co/spaces/huggingface/bloom_demo 以下是 BLOOM 项目的复现或衍生项目： BLOOM-LoRA 针对各种 Instruct-Tuning 数据集的低秩适应方法。 GitHub：https://github.com/linhduongtuan/BLOOM-LORA Petals 使用分布式的 176B 参数的 BLOOM 或 BLOOMZ 生成文本，并对其进行微调以适应自己的任务。 GitHub：https://github.com/bigscience-workshop/petals #04 Cerebras-GPT（Cerebras） **这是一系列开放、计算高效的大语言模型。**Cerebras 开源了七个 GPT-3 模型，参数从 1.11 亿到 130 亿不等。这些模型使用了 Chinchilla 公式进行训练，创造了精度和计算效率的新标准。 网站：https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/ Hugging Face：https://huggingface.co/cerebras Review：https://www.youtube.com/watch?v=9P3_Zw_1xpw #05 Flamingo（Google/Deepmind） 使用单一视觉语言模型处理多项任务。 网站：https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model 以下是基于 Flamingo 项目的复现或衍生项目： Flamingo — Pytorch 这是 Flamingo 项目的 Pytorch 实现，它是一种最先进的少样本视觉问答注意力网络。该实现包括 Perceiver Resampler（包括学习查询，以供键/值被关注，以及媒体嵌入），专门的掩码交叉注意力块，以及交叉注意力末端的 tanh 门控和相应的前馈块。 GitHub：https://github.com/lucidrains/flamingo-pytorch OpenFlamingo 欢迎使用我们的 DeepMind Flamingo 模型的开源版本！在这个仓库中，我们提供了一个 PyTorch 实现，用于训练和评估 OpenFlamingo 模型。我们还提供了一个经过训练的初始 OpenFlamingo 9B 模型，该模型是在一个新的 Multimodal C4 数据集上训练的（即将推出）。有关详细信息，请参阅我们的博客文章。 GitHub：https://github.com/mlfoundations/open_flamingo #06 FLAN（Google） **这个代码库包含用于生成指令调整数据集集合的代码。**第一个数据集是原始的 Flan 2021，它记录在《Finetuned Language Models are Zero-Shot Learners》中，第二个数据集是扩展版本，称为 Flan Collection，它在《The Flan Collection: Designing Data and Methods for Effective Instruction Tuning》中描述，并用于生成 Flan-T5 和 Flan-PaLM。 GitHub：https://github.com/google-research/FLAN 以下是基于 FLAN 项目的复现或衍生项目： FastChat-T5 我们很高兴地推出 FastChat-T5：这是一个紧凑而商业友好的聊天机器人！它是从 Flan-T5 微调而来，可用于商业应用，并且使用的参数比 Dolly-V2 少 4 倍，性能更好。 GitHub：https://github.com/lm-sys/FastChat#FastChat-T5 Hugging Face：https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py Flan-Alpaca 这个仓库包含代码，用于将 Stanford Alpaca 的合成指令微调方法扩展到已有指令微调模型（如 Flan-T5）。预训练模型和演示都可以在 HuggingFace 上获取。 GitHub：https://github.com/declare-lab/flan-alpaca Flan-UL2 这是一个基于 T5 架构的编码器 - 解码器模型，名为 Flan-UL2。它使用了去年早些时候发布的 UL2 模型相同的配置，并使用了 “Flan” 提示微调和数据集合集进行微调。 Hugging Face：https://huggingface.co/google/flan-ul2 Review：https://www.youtube.com/watch?v=cMT3RzjawEc #07 GALACTICA（Meta） 根据 Mitchell 等人（2018）的研究，本模型卡提供有关 GALACTICA 模型的信息，包括其训练方式和预期使用情况。有关模型的训练和评估的详细信息可以在发布的论文中找到： GitHub：https://github.com/paperswithcode/galai/blob/main/docs/model_card.md 基于 GALACTICA 项目的复现或衍生项目： Galpaca 这是在 Alpaca 数据集上微调的 30B GALACTICA 模型。 Hugging Face：https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b Hugging Face：https://huggingface.co/TheBloke/galpaca-30B-GPTQ-4bit-128g #08 GLM（General Language Model） GLM 是一个通用的语言模型，使用自回归填空目标进行预训练，可以在各种自然语言理解和生成任务上进行微调。 基于 GLM 项目的复现或衍生项目： ChatGLM-6B ChatGLM-6B 是基于通用语言模型（GLM）框架的开源双语言模型，具有 62 亿个参数。通过量化技术，用户可以在消费级图形卡上进行本地部署（在 INT4 量化级别下仅需要 6GB 的 GPU 内存）。 GitHub：https://github.com/THUDM/ChatGLM-6B #09 GPT-J **GPT-J 是由 EleutherAI 开发的开源人工智能语言模型。**GPT-J 在各种零样本下游任务上的表现与 OpenAI 的 GPT-3 非常相似，并且甚至可以在代码生成任务上胜过它。**最新版本 GPT-J-6B 是一种基于数据集 The Pile 的语言模型。**The Pile 是一个开源的 825 gibibyte 语言建模数据集，分为 22 个较小的数据集。GPT-J 的功能类似于 ChatGPT，尽管它不作为聊天机器人，只作为文本预测模型。 GitHub：https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b Demo：https://6b.eleuther.ai/ 以下是基于 GLM 项目的复现或衍生项目： Dolly（Databricks） Databricks 的 Dolly 是一个在 Databricks 机器学习平台上训练的大语言模型，它展示了一个两年前的开源模型（GPT-J）经过仅 30 分钟的针对 50k 个记录的专注语料库的微调后，可以展现出不同于基于其构建的基础模型的惊人高质量的指令跟随行为。我们认为这一发现非常重要，因为它表明了创建强大的人工智能技术的能力比以前意识到的要容易得多。 GitHub：https://github.com/databrickslabs/dolly Review：https://www.youtube.com/watch?v=AWAo4iyNWGc GPT-J-6B instruction-tuned on Alpaca-GPT4 这个模型是在 Alpaca 提示的 GPT-4 生成上使用 LoRA 进行微调的，共进行了 30，000 步（批量大小为 128），在四个 V100S 上花费了超过 7 小时的时间。 Hugging Face：https://huggingface.co/vicgalle/gpt-j-6B-alpaca-gpt4?text=My+name+is+Teven+and+I+am GPT4All-J 此仓库包含了基于 GPT-J 构建的开源助手式大语言模型的演示、数据和训练代码。 GitHub：https://github.com/nomic-ai/gpt4all Review：https://www.youtube.com/watch?v=5icWiTvDQS0 #10 GPT-NeoX **该代码库记录了 EleutherAI 在 GPU 上训练大规模语言模型的库。**我们目前的框架基于 NVIDIA 的 Megatron 语言模型，并已经添加了 DeepSpeed 的技术以及一些新的优化技巧。我们的目标是将这个仓库作为一个集中且易于访问的地方，汇集大规模自回归语言模型训练技术，并加速大规模训练的研究。 GitHub：https://github.com/EleutherAI/gpt-neox #11 h2oGPT 我们的目标是创建全球最好的开源 GPT！ GitHub：https://github.com/h2oai/h2ogpt Hugging Face：https://huggingface.co/spaces/h2oai/h2ogpt-oasst1-256-6.9b-hosted #12 HuggingGPT HuggingGPT 是一个协作系统，由 LLM 作为控制器和众多来自 HuggingFace Hub 的专家模型作为协作执行者组成。 GitHub：https://github.com/microsoft/JARVIS #13 Mosaic ML’s MPT-7B **MPT-7B 是一款 GPT 风格的模型，是 MosaicML 基础系列中的第一款模型。**它是由 MosaicML 策划的数据集中的 1T 标记训练而成的，是开源的、商用可用的，并且在评估指标上等同于 LLaMa 7B。**MPT 架构包含了所有最新的 LLM 建模技术 - 快闪式注意力（Flash Attention）实现高效率、Alibi 用于上下文长度的外推、以及稳定性改进来减轻损失的波动。**基础模型和几个变体，包括一个 64K 上下文长度的微调模型都是可用的。 网站：https://www.mosaicml.com/blog/mpt-7b GitHub：https://github.com/mosaicml/llm-foundry#mpt Review：https://www.youtube.com/watch?v=NY0bLFqkBL0 #14 Nvidia NeMo（GPT-2B-001） **GPT-2B-001 是一种基于 transformer 的语言模型。**GPT 是指一类类似于 GPT-2 和 3 的 transformer 解码模型，而 2B 则指可训练参数总数（20 亿）。该模型是使用 NeMo 在 1.1T 个标记上进行训练的。 Hugging Face：https://huggingface.co/nvidia/GPT-2B-001 #15 OpenAssistant Models 每个人都能使用的对话型人工智能。 网站：https://open-assistant.io/ GitHub：https://github.com/LAION-AI/Open-Assistant Hugging Face：https://huggingface.co/OpenAssistant #16 OpenLLaMA **在这个代码库中，我们发布了 Meta AI 的 LLaMA 大语言模型的开源复现版本，采用宽松许可证。**在此版本中，我们发布了经过训练的 2000 亿标记的 7B OpenLLaMA 模型的公共预览版。**我们提供了预训练的 OpenLLaMA 模型的 PyTorch 和 Jax 权重，以及评估结果和与原始 LLaMA 模型的比较。**请继续关注我们的更新。 GitHub：https://github.com/openlm-research/open_llama #17 Palmyra Base 5B（Writer） **Palmyra Base 主要使用英文文本进行预训练。**请注意，仍然有一小部分非英语数据存在于通过 CommonCrawl 访问的训练语料库中。**在模型的预训练过程中，采用了因果语言建模（CLM）目标。**与 GPT-3 类似，Palmyra Base 是仅包含解码器的模型系列的成员。**因此，它是通过自监督的因果语言建模目标进行预训练的。**Palmyra Base 使用 GPT-3 的提示和一般实验设置，以便根据 GPT-3 进行评估。 Hugging Face：https://huggingface.co/Writer/palmyra-base 基于 Palmyra 项目的复现或衍生项目： Camel 5B 介绍一下 Camel-5b，它是一个最先进的指令跟随大语言模型，旨在提供卓越的性能和多功能性。Camel-5b 基于 Palmyra-Base 的基础架构进行了优化，专门针对不断增长的先进自然语言处理和理解需求进行了设计。 Hugging Face：https://huggingface.co/Writer/camel-5b-hf #18 Polyglot 这是一篇有关多语言平衡能力的大语言模型的介绍。已经发布了各种多语言模型，如 mBERT，BLOOM 和 XGLM。因此，有人可能会问：“为什么我们需要再次制作多语言模型？” 在回答这个问题之前，我们想问：“为什么世界各地的人们会用自己的语言制作单语言模型，即使已经有很多多语言模型了？**” 我们想指出当前多语言模型的非英语语言性能不佳是最重要的原因之一。**因此，我们希望制作具有更高非英语语言性能的多语言模型。这就是我们需要再次制作多语言模型并将它们命名为 “Polyglot” 的原因。 GitHub：https://github.com/EleutherAI/polyglot #19 Pythia 跨时间和尺度解释自回归 Transformer。 GitHub：https://github.com/EleutherAI/pythia 基于 Pythia 项目的复现或衍生项目： Dolly 2.0 Dolly 2.0 是一个使用 EleutherAI Pythia 模型家族作为基础、仅在新的高质量人类生成的指令追踪数据集上进行微调的 12B 参数语言模型，该数据集由 Databricks 员工进行了众包。 网站：https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm Hugging Face：https://huggingface.co/databricks GitHub：https://github.com/databrickslabs/dolly/tree/master/data Review：https://www.youtube.com/watch?v=grEp5jipOtg #20 Replit-Code **replit-code-v1-3b 是一个专注于代码补全的 27 亿因果语言模型。**该模型是在 Stack Dedup v1.2 数据集的子集上训练的，训练混合包括以下 20 种语言，按标记数量降序排列： Markdown、Java、JavaScript、Python、TypeScript、PHP、SQL、JSX、reStructuredText、Rust、C、CSS、Go、C++、HTML、Vue、Ruby、Jupyter Notebook、R、Shell 该模型的训练数据集包含总计 1750 亿个标记，重复使用 3 个时代，因此 replit-code-v1-3b 已经在 5250 亿个标记上进行了训练（每个参数大约 195 个标记）。 Hugging Face：https://huggingface.co/replit/replit-code-v1-3b #21 The RWKV Language Model **RWKV 是一个可以并行运行的循环神经网络，其性能相当于 Transformer-level 的大语言模型（LLM），名称来自其四个主要参数：**R、W、K 和 V，发音为 “RwaKuv”。 GitHub：https://github.com/BlinkDL ChatRWKV：https://github.com/BlinkDL/ChatRWKV Hugging Face Demo：https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio RWKV pip package：https://pypi.org/project/rwkv/ Review：https://www.youtube.com/watch?v=B3Qa2rRsaXo #22 Segment Anything **“Segment Anything Model（SAM）”能够根据输入的提示，例如点或框，产生高质量的对象掩模，并可用于生成图像中所有对象的掩模。**它已经在一个包含 1100 万张图片和 11 亿个掩模的数据集上进行了训练，并在各种分割任务的零样本情况下表现出强大的性能。 网站：https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/ GitHub：https://github.com/facebookresearch/segment-anything #23 StableLM **StableLM 是一种新的开源语言模型，其 Alpha 版本提供了 30 亿和 70 亿参数版本，之后还会推出 150 亿到 650 亿参数的模型。**开发者可以自由地检查、使用和调整我们的 StableLM 基础模型，用于商业或研究目的，但需要遵守 CC BY-SA-4.0 许可协议的条款。 ​**StableLM 是在建立在 The Pile 上的新实验数据集上训练的，数据集大小为原来的三倍，包含了 1.5 万亿个内容单元。**这个数据集的丰富性使得 StableLM 在对话和编程任务方面表现出了出乎意料的高性能，尽管它的参数规模只有 3 到 7 亿（相比之下，GPT-3 有 1750 亿个参数）。 网站：https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models GitHub：https://github.com/stability-AI/stableLM/ Hugging Face：https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat Review：https://www.youtube.com/watch?v=0uI7SoMn0Es #24 Together’s RedPajama-INCITE 3B and 7B **我们发布了基于 RedPajama 数据集训练的第一批模型，包括 3B 和 7B 参数的基础模型，旨在尽可能精准地复制 LLaMA 模型的架构。**此外，我们还发布了完全开源的指令调优和对话模型。 网站：https://www.together.xyz/blog/redpajama-models-v1 Hugging Face：https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1 #25 XGLM XGLM 模型是在 “Few-shot Learning with Multilingual Language Models” 中提出的。 GitHub：https://github.com/facebookresearch/fairseq/tree/main/examples/xglm Hugging Face：https://huggingface.co/docs/transformers/model_doc/xglm #26 Other Repositories couchpotato888 Hugging Face：https://huggingface.co/couchpotato888 crumb Hugging Face：https://huggingface.co/crumb Knut Jägersberg Hugging Face：https://huggingface.co/KnutJaegersberg LaMini-LM：来自大规模指令的多样化压缩模型群 LaMini-LM 是一系列小型、高效的语言模型，它们是从 ChatGPT 中提取并经过训练的，训练数据集包含 258 万条指令。我们尝试了不同的模型架构、大小和检查点，并在各种 NLP 基准测试和人类评估中广泛评估了它们的性能。 Paper：https://arxiv.org/abs/2304.14402 GitHub：https://github.com/mbzuai-nlp/LaMini-LM Review：https://www.youtube.com/watch?v=TeJrG3juAL4&amp;t=42s Teknium Hugging Face: https://huggingface.co/teknium ​ ","link":"https://jobslee0.github.io/post/kai-yuan-da-yu-yan-mo-xing-hui-zong/"},{"title":"万字长文聊聊Web3的组成架构","content":" 原文链接：https://mp.weixin.qq.com/s/R3YBcN2t2VLeHZTpsGzw5Q Web3 发展至今，生态已然初具雏形，如果将当前阶段的 Web3 生态组成架构抽象出一个鸟瞰图，由下而上可划分为四个层级：区块链网络层、中间件层、应用层、访问层。下面我们来具体看看每一层级都有什么。另外，此章节会涉及到很多项目的名称，因为篇幅原因不会一一进行介绍，有兴趣的可以另外去查阅相关资料进行深入了解。 区块链网络层 最底层是「区块链网络层」，也是 Web3 的基石层，主要由各区块链网络所组成。 组成该层级的区块链网络还不少，Bitcoin、Ethereum、BNB Chain(BSC)、Polygon、Arbitrum、Polkadot、Cosmos、Celestia、Avalanche、Aptos、Sui 等等，还有很多。根据 Blockchain-Comparison 的统计，截止撰文之日的区块链至少有 150 条。这里我们主要说的是公链，联盟链不包括在内。因为区块链实在太多，会有些眼花缭乱，所以有必要进行分门别类。 首先，不同区块链之间存在着分层结构，有 Layer0、Layer1、Layer2 之分。其次，Web3 的繁荣发展，依赖于智能合约技术，而智能合约的运行环境为虚拟机。智能合约和虚拟机的关系，就和 Java 程序和 JVM 的关系类似。从不同的虚拟机维度上划分区块链，就可以分为两大类：EVM 链和 Non-EVM 链。EVM 是 Ethereum Virtual Machine，即为以太坊虚拟机的简称。EVM 链即为兼容 EVM 的区块链，而 Non-EVM 顾名思义就是不兼容 EVM 的区块链。最后，还可以根据存储的数据大小进行分类，可以分为计算型区块链和存储型区块链。 先从分层结构说起。最好理解的是 Layer1，我们所熟知的比特币、以太坊、EOS、BSC 都属于 Layer1，也称为主链。在分布式系统中，存在 CAP 定理，即一个分布式系统不可能同时满足三个特性：一致性、可用性、分区容错性。一个分布式系统只能满足三项中的两项。Layer1 的区块链本质上也是分布式系统，也同样存在不可能三角问题，只是三个特性与 CAP 不同，分别为：可扩展性、安全性、去中心化，每个区块链也只能满足三项中的两项。比特币和以太坊偏向于安全性和去中心化，所以可扩展性比较弱，TPS 比较低。EOS 和 BSC 则只依赖于少数节点来维护共识，相比于比特币和以太坊，减低了去中心化特性，但提高了可扩展性，从而能达到很高的 TPS。 为了解决比特币和以太坊的可扩展性问题，就慢慢衍生出了 Layer2。Layer2 是作为依附于主链的子链而存在，主要用于承载 Layer1 的交易量，承担执行层的角色，而 Layer1 则可变成结算层，可大大减少交易压力。目前主流的 Layer2 都是扩展以太坊的子链，包括 Arbitrum、Optimism、zkSync、StarkNet、Polygon 等。比特币也有 Layer2，主要包括闪电网络、Stacks、RSK 和 Liquid，但目前都比较小众。 Layer0 则比较抽象了，一般被定义为区块链基础设施服务层，主要由模块化区块链所构成，包括 Celestia、Polkadot、Cosmos 等。模块化区块链这个概念主要是由 Celestia 提出的，其核心设计思路就是把区块链的共识、执行、数据可用性这几个核心模块拆分开来，每个模块由一条单独的链来完成，再将几个模块组合到一起完成全部工作。这和软件架构设计中所提倡的模块化设计思想是一样的，可实现高内聚低耦合。 实现跨链通信的跨链桥或跨链协议也可以划入 Layer0。跨链桥的数量也是非常多，撰写此文时，debridges.com 上统计的跨链桥多达 113 条，其中 TVL 排名最高的三个分别为 Polygon、Arbitrum、Optimism 的官方跨链桥，这几个桥分别实现了各自的 Layer2 和以太坊之间的资产跨链。TVL 排名第四位的则是 Multichain，其前身为 Anyswap，是连接了最多条区块链的第三方跨链桥，截至今年 1 月份时，其连接的区块链多达 81 条。 聊完分层结构的划分，我们再从 EVM 的维度来梳理下不同的区块链。前面说过，从 EVM 维度上可划分为 EVM 链和 Non-EVM 链两大类。 EVM 链是目前最主流的方向，基于 EVM 链的 DApp 和用户群体是目前整个 Web3 生态里规模最大的。有些原生就是兼容 EVM 的，比如 BSC、Heco、Arbitrum、Optimism 等；有些则是后期才扩展兼容 EVM 的，比如 zkSync 1.0 并不兼容 EVM，而 zkSync 2.0 则是兼容 EVM 的。很多区块链就算早期并不兼容 EVM，但也逐渐在拥抱 EVM。比如，Polkadot 推出了 Moonbeam 平行链来兼容 EVM，Cosmos 则有 Evmos。 目前来看，排名靠前的区块链中，大部分都已经兼容 EVM，不过依然还有少部分 Non-EVM 链存在，比如 Solana、Terra、NEAR、Aptos、Sui。另外，EVM 链的智能合约主要使用 Solidity 作为开发语言，而 Non-EVM 链则主要使用 Rust 或 Move 语言开发智能合约。 以上提到的这些区块链，主要还是偏向于解决去中心化计算的区块链，这些区块链普遍不支持大数据的存储，比如文件存储。而存储型的区块链则聚焦于解决大数据存储的问题，这类区块链目前不太多，主要有 Filecoin、Arweave、Storj、Siacoin 和 EthStorage。 目前组成「区块链网络层」的区块链成员们主要就包括这些了，未来还会不断有新成员加入，但也有不少旧成员逐渐没落而被遗落在角落里。 中间件层 在区块链网络层之上的这一层，我称之为「中间件层」，主要为上层应用提供各种通用服务和功能。所提供的通用服务和功能包括但不限于：安全审计、预言机、索引查询服务、API 服务、数据分析、数据存储、基本的金融服务、数字身份、DAO 治理等。提供通用服务和功能的组件则可称为「中间件」，这些中间件也是存在多种形式，可以是链上协议，也可以是链下平台，或链下组织，包括中心化的企业或去中心化自治组织 DAO。下面就来聊聊这一层具体都有哪些中间件。 先来聊聊安全审计，这是非常核心的中间件，因为 Web3 里的区块链和应用大多都是开源的，且很多都是跟金融强相关，因此，安全性就成为了重中之重，安全审计自然也变成了刚需。安全审计的服务大多由一些安全审计公司所提供，比较知名的审计公司包括：CertiK、OpenZeppelin、ConsenSys、Hacken、Quantstamp，以及国内主要有慢雾、链安、派盾等。另外，还有不少知名度不高的小审计公司。 除了审计公司，还有一些提供 Bug Bounty 的平台，一般就是在这些平台上发布任务，让白帽黑客们来找 Bug，找到的 Bug 安全漏洞等级越高则可获得的赏金越高。目前，全球最大的 Bug Bounty 平台是 Immunefi。 接着，再来聊聊预言机（Oracle Machine，简称 Oracle），在 Web3 生态里也是扮演着非常重要的角色，是区块链系统与外部数据源之间沟通的桥梁，主要实现智能合约与链下真实世界的数据互通。因为区块链网络本身对状态一致性的限制，需要保证每个节点在给定相同输入的情况下必须获得相同的结果，所以区块链被设计成一个封闭系统，只能获取到链内的数据，而无法主动获取外部系统的数据。但很多应用场景中是需要用到外部数据的，这些外部数据就由预言机来提供，这也是目前区块链与外部数据实现互通的唯一途径。 根据预言机所提供的具体功能，目前对预言机的分类大致有：DeFi 预言机、NFT 预言机、SocialFi 预言机、跨链预言机、隐私预言机、信用预言机、去中心化预言机网络。具体的预言机项目有 CreDA、Privy、UMA、Banksea、DOS、NEST、Chainlink 等，其中，Chainlink 为预言机的龙头，其定位为去中心化预言机网络，推出了 Data Feeds、VRF、Keepers、Proof of Reserve、CCIP 等一系列产品和服务。 然后，索引查询服务也是很关键的中间件，解决了链上数据的复杂查询问题。比如要查询 Uniswap 上某一天的总交易量，如果直接在链上查询是很麻烦的。所以就有了对索引查询服务的需求，这块的主要代表为 The Graph 和 Covalent。The Graph 的实现方案主要是可定制化监听链上数据并映射成自定义的数据进行存储，从而方便查询。而 Covalent 则是将很多通用、广泛使用的数据封装成统一的 API 服务，供用户查询。 提到 API 服务，除了 Covalent，还存在解决其他不同需求的 API 提供商，比如：NFTScan，是聚焦于提供 NFT API 数据服务的；Infura 和 Alchemy，则主要提供区块链网络节点服务；API3，旨在打造去中心化 API 服务。 不管是索引查询服务还是 API 服务，都是链上数据相关的服务，数据分析也是数据相关的服务，这一版块的成员主要有 Dune Analytics、Flipside Crypto、DeBank、Chainalysis 等。 数据存储中间件则和底层几个专门做存储的区块链容易混淆，也有人将底层的 Filecoin、Arweave、Storj 等划分到这一层，但我觉得这些本质上还是底层区块链，所以我将其划入到区块链网络层。而中间件层的数据存储，目前主要就是 IPFS。IPFS 全称为 InterPlanetary File System，中文名为星际文件系统，是一个基于内容寻址、分布式、点对点的新型超媒体传输协议，其旨在取代 HTTP 协议。IPFS 与区块链网络很相似，但其实并不属于区块链网络，基于 IPFS 的 Filecoin 才是区块链网络。 接下来，看看有哪些中间件是提供基本金融服务的。这块的代表性组件主要包括 Uniswap、Curve、Compound、Aave 等，Uniswap 和 Curve 提供了链上交易功能，而 Compound 和 Aave 则是链上借贷平台。这几个本质上都是应用层的链上协议，但因为这些协议都逐渐被越来越多其他应用所依赖，类似于成为了乐高积木，可以用来组合搭建出不同的应用，于是就变成了通用性的应用协议，即下沉为了中间件的角色。 其实，任何具有可组合性的组件，不管是链上应用协议，还是链下提供不同服务的中心化实体，或者是 DAO，只要其提供的服务和功能是大部分应用都需要的，就可以划入「中间件层」。不同的中间件就和不同的乐高积木一样，通过组装不同的积木就可以创建出不同的应用。包括数字身份、DAO 治理的工具等，其实也都是同样道理。 应用层 应用层是 Web3 生态里最繁荣的一层，这一层里，充斥着各种不同的 DApps，可谓是百花齐放、百家争鸣。下面我们主要介绍几个发展得相对比较繁荣的板块。 NFT NFT 全称为 Non-Fungible Token，表示「非同质化代币」，国内也称为数字藏品，用于代表艺术品等独一无二的数字资产。 第一个真正意义上的 NFT 项目叫 CryptoPunks，于 2017 年 6 月发布，由 10,000 个 24x24 像素的头像所组成。每个头像都是由算法生成的，独一无二且所有头像都上传到了以太坊上，也是目前为止唯一一个将所有头像数据全部上链的 NFT 项目。下图为 CryptoPunks 官网展示的部分头像： 截止撰文之日，CryptoPunks 的地板价（即最低价）为 66.88 ETH，按 ETH 的价格换算成美元，大概为 $84,397.21 美元。最贵的一个 CryptoPunk，成交价达到了 8000 ETH，成交于 2022 年 2 月 12 日。一个 NFT 头像为何会这么贵，这对于很多人都是很难理解的。其中，最主要的一个原因，就是它是第一个 NFT 项目，就和比特币是第一个区块链一样，其开创性的地位所带来的价值潜力非常大。 受 CryptoPunks 的启发，一家名为 Axiom Zen（Dapper Labs 的前身）的公司于 2017 年 11 月底发行了 CryptoKitties，国内也称为加密猫、以太猫、谜恋猫。CryptoKitties 上线后便病毒式地传播开来，还造成了以太坊的拥堵，暴露出以太坊的性能问题。CryptoKitties 发行之前，Axiom Zen 的技术总监 Dieter Shirley 以 CryptoKitties 为案例，还提出了 ERC721 Token 协议作为 NFT 的通用技术标准，而随着 CryptoKitties 爆火后，以 ERC721 为主要技术标准的 NFT 被进一步采用，如今 ERC721 已经成为了所有 NFT 的基础标准之一。 继 CryptoPunks 和 CryptoKitties 之后，NFT 开始逐渐遍地开花，NFT 生态逐渐蓬勃发展。NFT 发展至今，已经涉足到了多个领域，如果对 NFT 生态的所有组成部分做详细分类的话，可以多达几十种。如果只聚焦于 NFT 本身，即 NFT 的不同用例，那大致可以做出以下分类：收藏品、艺术品、音乐、影视、游戏、体育运动、虚拟土地、金融、品牌、DID。下面主要介绍每个分类的一些代表性的 NFT 项目。 收藏品其实很难单独定义为一个类别，宽泛地讲，几乎任何东西都可以归为收藏品，包括艺术品、游戏道具、虚拟土地等。能被定义为收藏品的 NFT 主要需具备一个特性：稀缺性。比如，10000 个 CryptoPunks 中，外星人的数量最少，所以有很高的稀缺性，而男性最多，稀缺性就很低了。最知名的收藏品 NFT，除了 CryptoPunks，还有 BAYC，全称为 Bored Ape Yacht Club，也称为无聊猿。无聊猿不只是一套单独的 NFT，其实只是「无聊猿宇宙」的开端，基于无聊猿之后，背后的团队 Yuga Labs 又相继发行了无聊猿犬舍俱乐部（Bored Ape Kennel Club，BAKC)、变异猿游艇俱乐部（Mutant Ape Yacht Club，MAYC），也发行了 ApeCoin（APE）代币，还推出了 Otherside，专为元宇宙打造的虚拟土地。这些，都已经形成了「无聊猿宇宙」系列 IP，而且无聊猿不只是在加密圈内流行，在圈外的周边产品也在不断增加，比如有无聊猿的帽子、衣服、雕像、餐厅等。无聊猿的成功已超越了 CryptoPunks，Yuga Labs 之后还直接收购了 CryptoPunks。 NFT 的特性能有效保护版权的所有权，所以在艺术品领域流行开来也是理所当然。艺术品 NFT 有几个代表性的作品值得介绍一番，第一个是艺术家 Beeple 的作品，名为“每一天：第一个 5000 天（EVERYDAYS: THE FIRST 5000 DAYS）”，是将他过去 5000 天内每天创作一幅的所有作品（共 5000 幅）合成一个 NFT 图像，在 2021 年 3 月以 69,346,250 美元售出。第二个值得介绍的是生成艺术，也称为衍生艺术。生成艺术中的艺术品不是由人创作出来的，而是由编程算法自动生成的，最知名的 NFT 生成艺术平台叫 Art Blocks，是一个基于以太坊的随机生成艺术平台。艺术家们可以把自己设计的独特算法上传到 Art Blocks 平台，并设定特定数量 NFT 进行发行，NFT 会根据算法自动生成。最后再介绍目前最贵的 NFT 艺术品，叫 ”The Merge“，2021 年 12 月以 9180 万美元天价成交。与其他 NFT 不同，”The Merge“ 其实不是一个单独的作品，而是由多个「mass」代币动态组合而成的。销售的其实也是 mass 代币，当初共售出 312,686 个 mass 代币，共有 28,983 个买家，即是说，”The Merge“ 是由这 28,983 个买家共同拥有其所有权，每个买家所拥有的 mass 代币数量就代表了占有多少份额的所有权。”The Merge“ 也可以理解为是一个碎片化 NFT 作品。 音乐 NFT 的兴起和艺术品类似，主要也是因为版权。下面介绍几个具有代表性的音乐 NFT 相关人物，第一个要介绍的是 Justin David Blau，是美国 DJ 和电子舞曲制作人，以艺名 3LAU 而闻名。他是最早采用音乐 NFT 的人之一，在 2020 年秋天卖出了他的第一张 NFT。而在 2021 年 2 月底，凭借 Ultraviolet NFT 专辑为他带来了 1168 万美元的收入。2021 年 5 月又成立了 NFT 音乐平台 Royal，8 月份完成了种子轮融资 1600 万美元，有 a16z、Coinbase 等顶级机构参与。第二个要介绍的是 Don Diablo，荷兰 DJ、数字艺术家、唱片制作人、音乐家和电子舞曲创作者，他在 2021 年卖出第一部完整的音乐会电影 NFT，名为 “Destination Hexagonia”，成交价 600 ETH（当时为 126 万美元）。最后再介绍一个叫 Kingship 的摇滚乐队，这是一支由无聊猿组成的虚拟乐队，由环球音乐集团所组建。 NFT 也席卷到了影视圈，有几个知名的影视剧都陆续发行了 NFT，国外有《权力的游戏》《蝙蝠侠》《指环王》《黑客帝国》《行尸走肉》等，国内有《大话西游》《流浪地球》《我不是药神》《封神三部曲》等。 NFT 用在游戏里主要就是作为游戏资产的载体，相比于传统游戏内的资产，NFT 的形式对游戏玩家来说可以真正拥有游戏资产的所有权，且 NFT 可以在游戏外流通交易。第一个游戏 NFT 项目就是 CryptoKitties，每一只猫都是一个独立的 NFT。后面讲到 GameFi 小节再继续深入聊聊游戏这块。 体育运动领域也同样涉足了 NFT，目前最知名的两大体育 NFT 平台是 NBA Top Shot 和 Sorare。NBA Top Shot 顾名思义主要以 NBA 为主，而 Sorare 则服务于足球领域。除了 NBA 和足球，橄榄球、棒球、拳击、摔跤也都纷纷推出了各自的 NFT 纪念品。 虚拟土地类 NFT 主要由一些主打「元宇宙」概念的项目所推行，比较知名的有 Decentraland、The Sandbox、Roblox、Axie Infinity Land、Otherdeed 等。 金融和 NFT 的结合，主要就是将 NFT 应用到 DeFi 中，比如 UniswapV3 中的流动性仓位就是 NFT。另外，还有一个思路则是先将 NFT 碎片化，接着将这些碎片后的 NFT 再赋予 DeFi 功能，比如可以赋予交易、借贷、质押挖矿等功能。 品牌和 NFT 的结合，主要是作为一种新的营销方式。这两三年陆续有各种品牌加入这个阵营，比如，奢侈品品牌有 GUCCI、LV、爱马仕等，餐饮品牌有 Taco Bell、星巴克、必胜客、可口可乐等，汽车品牌有迈凯伦、雪佛兰等，运动品牌有阿迪达斯、李宁、耐克等，还有很多其他品牌。 最后，聊聊 DID，全称为 Decentralized Identity，即去中心化身份。所有人都知道 DID 非常重要，但其发展还比较缓慢，目前除了细分领域 ENS 域名之后，还没有成熟的 DID 体系形成网络效应。目前，应用最广泛的只有域名，基于以太坊的 ENS 是龙头，ENS 之于 Web3，就相当于 DNS 之于 Web2。不同的是，ENS 解析的域名，映射的不是网站 IP，而是用户的以太坊地址。比如，以太坊创始人 V 神的 ENS 为 “vitalik.eth”，映射的地址为 0xd8da6bf26964af9d7eed9e03e53415d37aa96045。 NFT 的可应用场景实在太多了，上面所列出的分类还没能覆盖到全部。因为 NFT 的特性，任何具有所有权的东西都可以指代，所以坊间有“万物皆可 NFT”的说法。 DeFi DeFi 即去中心化金融，崛起于 2020 年夏天，因此那段时间也被称为 DeFi Summer。根据 TradingView 的统计数据，2020 年夏天刚崛起时，DeFi 总市值仅 50 亿美元，随后一路飙升，在 2021 年底达到了最高峰，将近 1800 亿美元。 DeFi 有很多细分板块，主要包括：稳定币、交易所、衍生品、借贷、聚合器、保险、预测市场、指数等。 稳定币主要可分为三类：中心化稳定币、超额抵押稳定币、算法稳定币。其中，超额抵押稳定币和算法稳定币为去中心化稳定币。 中心化稳定币直接与法定货币挂钩，由中心化机构所发行，要求每单位稳定币需要有 1:1 的法币储备。目前交易量最大的两个稳定币 USDT 和 USDC，都是法币抵押稳定币，与美元 1:1 挂钩，分别由 Tether 和 Circle 两家中心化机构所发行。另外，币安，全球第一大中心化数字货币交易所，联合 Paxos 发行了自己的法币抵押稳定币 BUSD，目前也是全球交易量排名第三的稳定币，仅次于 USDT 和 USDC。 超额抵押稳定币通过超额抵押其他加密货币而锻造，抵押品会被锁定在智能合约里，智能合约会根据抵押品的价值锻造出对应数量的稳定币，智能合约依靠价格预言机来维持与法币的锚定。此类型的稳定币主要以 DAI 为代表，由 MakerDAO 推出，和美元保持 1:1 锚定，目前交易量排名第四。 算法稳定币则比较新颖，顾名思义，主要是通过算法来控制稳定币的供应。此赛道的选手也不少，包括 UST、FEI、AMPL、ESD、BAC、FRAX、CUSD、USDD、USDN 等，但目前还没有一个真正实现稳定的算法稳定币出现。 接着，来聊聊交易所，DeFi 里的交易所是指去中心化交易所，简称 DEX。DEX 是 DeFi 所有板块里市值占比最高的板块，也是 DeFi 的基石板块。如果对 DEX 再进一步细分，还可以分为现货 DEX 和衍生品 DEX，衍生品 DEX 主要交易永续合约或期权。如果从交易模式上划分，那 DEX 主要可分为两种：Orderbook 模式和 AMM 模式。Orderbook 模式的 DEX，主要包括 dYdX、apeX、0x、Loopring 等。AMM 模式的 DEX 则比较多了，主要包括 Uniswap、SushiSwap、PancakeSwap、Curve、Balancer、Bancor、GMX、Perpetual 等。 Orderbook 模式是最早出现的交易类型，交易方式和股票盘口的买卖方式一样，交易用户可选择成为挂单者（maker）或吃单者（taker），交易会根据价格优先和时间优先的规则撮合成交。采用 Orderbook 的 DEX，根据其发展历程主要还可以再分为三种模式：纯链上撮合+结算模式、链下撮合+链上结算模式、Layer2 模式。 纯链上撮合结算模式，用户提交的挂单和吃单都是直接在链上，吃单会直接和链上的挂单成交。该模式的代表为 EtherDelta，其优点是完全链上，去中心化程度高，但缺点是交易性能很低且交易成本很贵，用户挂单、撤单都需要支付燃料费。 链下撮合+链上结算模式的代表则是 0x 协议，相比于第一种模式，主要多了链下的「中继器」角色，用户通过链下签名的方式生成委托单并提交给中继器，由中继器来维护 Orderbook，撮合成功的委托单再由中继器提交到链上进行结算。因为将撮合移到了链下处理，大大提高了交易性能，但结算是一笔笔单独结算的，所以结算的性能成为了瓶颈。 Layer2 模式的代表为 dYdX，背后所使用的技术主要由 StarkWare 所提供的产品 StarkEx 所支持。其基本原理就是部署一个单独的、专用的 Layer2，用户的撮合交易和结算都发生在这个 Layer2 上，然后定时将所有交易记录（包括结算记录）全部打包生成证明并发送到 Layer1 上进行验证。与 Layer2 公链不同，Layer2 公链提供的是通用交易，而 dYdX 背后所使用的这个 Layer2 只能用于专用的交易场景，这其实算是个私有链，也可称为应用链，这也是一种新的应用模式。这种模式的交易体验和中心化交易所已经相差无几了，但中心化程度比较高。 完全去中心化且交易体验也较好的交易模式，目前主流的就是 AMM 模式了，AMM 为 Automated Market Maker 的简称，也称为自动做市商模式。引爆 AMM 模式的是 Uniswap，于 2018 年 11 月上线，之后的 SushiSwap、PancakeSwap、Curve 等都是基于 Uniswap 的模式进行改造。该模式需要流动性池作支撑，流动性提供者（简称 LP）往交易池里注入资产作为流动性，其实就是资金池，然后用户直接和流动性池进行交易，而 LP 则从中赚取用户的交易手续费。 关于交易所暂时就先聊这么多，接着来看看衍生品。DeFi 衍生品板块主要包括几个方向：永续合约、期权、合成资产、利率衍生品。 永续合约也是期货合约，加了杠杆的交易产品，前面提到的 dYdX、apeX、GMX、Perpetual 就是知名的几个永续合约 DEX。期权比期货复杂，DeFi 期权领域的玩家主要包括 Hegic、Charm、Opium、Primitive、Opyn 等，但目前期权市场还很小，被关注的不多。合成资产是由一种或多种资产/衍生品组合并进行代币化的加密资产，早期主要合成 DAI、WBTC 等数字资产，后面基于现实世界中的股票、货币、贵金属等的合成资产也越来越多，目前该赛道的龙头项目是 Synthetix，另外还有 Mirror、UMA、Linear、Duet、Coinversation 等项目。DeFi 的利率衍生品主要是基于加密资产利率开发不同类型的衍生产品，以满足 DeFi 用户对确定性收益的不同需求，主要玩家有 BarnBridge、Swivel Finance、Element Finance 等。 接着来看看借贷，这也是 TVL 很高的一个版块，和 DEX 一样也是 DeFi 的基石。这块的借贷协议主要有 Compound、Aave、Maker、Cream、Liquity、Venus、Euler、Fuse 等。目前，大部分借贷协议都是采用超额抵押的借贷模型，所谓超额抵押，举个例子，比如，要借出 80 美元的资产，那至少需要存入价值 100 美元的抵押资产，即抵押资产价值要高于借贷资产价值。 虽然超额抵押模型是主流，但也存在几个创新方向：无息贷款、资产隔离池、跨链借贷、信用贷。无息贷款的代表为 Liquity，用户在 Liquity 借出其稳定币 LUSD 的时候，用户一次性支付借款和赎回费用，借出后无需支付利息。资产隔离池就是将不同的借贷资产分开为不同的池子，每个借贷池都是独立的，避免一个不良资产或者一个池子受损导致整个平台都被连累。目前，资产隔离池差不多已经成为了标配，很多借贷协议都引入了这种模式，除了一开始就使用这种模式的 Fuse，包括 Compound、Aave、Euler 等协议也都加入了阵营。跨链借贷也是一个新趋势，Flux、Compound、Aave 等都在这个方向上进行拓展。信用贷在传统金融非常普遍，但在 DeFi 领域还比较少，主要是还缺乏有效的链上信用体系，目前的代表项目是 Wing Finance。 下一个是聚合器，DeFi 聚合器也分为好几种类型：DEX 聚合器、收益聚合器、资产管理聚合器、信息聚合器。DEX 聚合器，主要就是将多个 DEX 聚合到一起，通过算法从中寻找出最优的交易路径，主流的 DEX 聚合器包括 1inch、Matcha、ParaSwap，以及 MetaMask 钱包内置的 MetaMask Swap 等。收益聚合器主要有 Yearn Finance、Alpha Finance、Harvest Finance、Convex Finance 等，主要就是聚合各种流动性挖矿，让参与多平台的 Yield Farming（收益耕作）实现自动化。资产管理聚合器主要就是监控、跟踪和管理 DeFi 用户的资产和负债，主要以 Zapper 和 Zerion 为代表。最后是信息聚合器，主要包括 CoinMarketCap、DeFiPulse、DeBank、DeFiPrime 等平台。另外，这些其实都是中心化数据平台，但其在 DeFi 生态里依然扮演了重要角色，DeFi 生态里并非全都是去中心化的应用。 然后，再简单聊聊保险。我们知道，保险在传统金融中是非常大的一块市场，但 DeFi 里的保险发展至今，却是非常缓慢。整个 Web3 行业里，各种风险很多，协议漏洞风险、项目跑路风险、监管风险等，所以实际上对 DeFi 保险的需求市场本身很大，但因为开发设计门槛高，且流动性比较低，所以才导致整个保险赛道发展缓慢，目前依然处于非常早期的阶段，Nexus Mutual、Cover、Unslashed、Opium 等项目是该领域主要的玩家。 然后，再看看预测市场。预测市场是依托数据的市场，可用于押注和预测未来的所有事件，也是以太坊生态最早出现的应用场景之一，并在 2020 年美国大选中迎来爆发式增长，主要项目有 PolyMarket、Augur、Omen 等。 最后就是指数板块，提供一揽子资产敞口的指数基金在 DeFi 领域逐渐兴起。但广为人知的指数其实并不多，主要有：DPI、sDEFI、PIPT、DEFI++。DPI 全称为 DeFi Pulse Index，是由 DeFi Pulse 和 Set Protocol 合作创建的，是一种市值加权指数，包含了一些主流 DeFi 协议代币作为基础资产，包括 Uniswap、Aave、Maker、Synthetix、Loopring、Compound、Sushi 等。DPI 可以赎回为一揽子基础资产。sDEFI 则是由 Synthetix 所推出的指数代币，是该领域历史最悠久的指数。sDEFI 是一种合成资产，它不持有任何基础代币，而是使用预言机喂价来跟踪代币价值。PIPT 全称为 Power Index Pool Token，是由 PowerPool 所发行，由 8 种代币资产所组成。PowerPool 发行的指数除了 PIPT，另外还有 Yearn Lazy Ape Index、Yearn Ecosystem Token Index 和 ASSY Index 三个指数。DEFI++ 则是由 PieDAO 所发行，其组成有 14 种资产。PieDAO 还发行了 BCP 和 PLAY，BCP 由 WBTC、WETH、DEFI++ 三种代币组成，PLAY 则由一些元宇宙项目的代币所组成。 GameFi GameFi 从字面上理解就是 Game Finance，是游戏和金融的融合体，也是目前 Web3 游戏的代名词。GameFi 这个词语诞生之前，Web3 游戏则通常被称为区块链游戏，或简称链游。 CryptoKitties 是第一款广为人知的区块链游戏，这是一款虚拟养猫的养成类游戏，每一只猫咪都是一个独立的 NFT。初代猫咪总共有 50000 只，每只猫咪都有不同的属性。玩家购买猫咪 NFT 后，就可以开始玩繁殖小猫的游戏。生出来的小猫咪，有部分基因属性会遗传自上一代，而有些基因则随机生成。生出来的猫咪本质上就是新的 NFT，可以卖出变现。如果生成的新猫咪产生了稀有的基因属性，还可以卖到不错的价格。截止撰文之日（2023 年 1 月底），已经产生了 2,021,774 只猫咪，持有的钱包地址有 136,283。 继 CryptoKitties 之后，越来越多养成类游戏陆续出现，如加密狗、加密兔、加密青蛙等等。打破这种局面的是一款叫 Fomo3D 的游戏，这是一款公开、透明、去中心化的博彩资金盘游戏。游戏规则也简单，用户通过支付 ETH 购买 Key 参与游戏，用户支付的 ETH 会分配到奖池、分红池、空投池、官方池等。拥有 Key 则可以得到持续的分红，拥有的 Key 越多，则得到的分红会越多。且每轮游戏存在一个倒计时（24 小时），倒计时结束时，最后一个购买 Key 的玩家可以获得奖池里大部分的 ETH。但每次有用户购买 Key，则倒计时剩余时间会增加 30 秒。第一轮游戏持续了很久时间，最后被人用技术手段赢走了奖池。Fomo3D 爆火之后，也是各种优化升级版的同类游戏不断出现，但事实证明，这类游戏还是无法持久。 而之后，再次引爆市场的游戏则是 Axie Infinity，国内则被称为“阿蟹”（与 Axie 谐音）。这是一款结合了宝可梦和加密猫玩法的游戏，游戏里的 Axies 可以升级、繁殖、对战、交易等。与加密猫等游戏不同的是，Axie Infinity 的经济系统里还引入了 SLP 和 AXS 代币，玩家可通过战斗赢取 SLP 代币，而通过消耗 SLP 和 AXS 可以繁殖新的 Axies，赢取的 SLP 代币和繁殖出来的 Axies 都可以在市场上出售来赚取收入。 不过，Axie Infinity 其实在 2018 年就已经问世，但直到 2021 年才开始爆红，让其爆红的主要原因在于它的 Play-To-Earn 模式被推广开来了，即边玩边赚的特性呈病毒式传播了。其赚钱路径主要是先投入成本购买 Axies，然后通过玩游戏赚 SLP 代币和繁殖新的 Axies，再把 SLP 代币和 Axies 出售换成 ETH 或稳定币，最终将 ETH 或稳定币换成法币。这种赚钱模式一开始是从菲律宾逐渐流行起来的，当时，新冠疫情爆发，菲律宾当地许多人陷入了无收入的困境，而 Axie Infinity 的边玩边赚特性让这些人看到了希望。而且，这种赚钱模式也吸引了众多打金工作室，且逐渐从菲律宾扩展到了印度、印度尼西亚、巴西、中国等。截止撰文之日，日活用户已达 280 万。 而现在，边玩边赚模式几乎成为了 Web3 游戏的标配。 其他比较知名的游戏还有 Decentraland、The Sandbox、Illuvium、Star Atlas、Alien Worlds 等。这些就不展开说了，感兴趣的可以自行去搜索了解。 SocialFi SocialFi 顾名思义就是 Social Finance，是社交和金融在 Web3 领域的有机结合，其实就是去中心化社交，是近两年才开始流行的概念。目前，在这个赛道的知名项目还比较少，目前的龙头是 Lens Protocol。 Lens Protocol 是由 Aave 团队所开发的，在 2022 年 5 月上线。它不是一个独立的社交应用，也不是一个带有前端的完整社交产品，而是提供了一系列模块化组件的社交图谱平台，而具体的应用产品可以采用这些组件去构建。所以，Lens 的定义其实是 Web3 社交应用的基础设施。上线之初就已经拥有了 50 多个生态项目，比较热门的有 Lenster、Lenstube、ORB、Phaver、re:meme、Lensport、Lensta 等。 Lenster 是去中心化社交媒体应用，可以通过连接 Web3 钱包并使用 Lens 来登录。登录用户就可以在 Lenster 发布内容，和在微博或推特发布内容类似，不同的是，在 Lenster 上发布内容时可选择收费。也可以评论其他用户的内容，不过目前还不支持层级式的评论。 Lenstube 则是去中心化视频平台，可以理解为就是去中心化的 Youtube。 ORB 是去中心化职业社交媒体应用，具有端到端链上信誉系统。具体来说，ORB 可以通过将各种 NFT 和 POAP 与用户经验、教育、技能和项目联系起来，从而创建个人去中心化专业档案并建立链上可信度，以及探索工作机会和申请链上身份，还可以用在链上分享自己的想法，与 Web3 人士建立联系并构建社区。此外，ORB 还允许用户利用碎片化时间通过学习 Web3 知识来获取 NFT，即 Learn-to-Earn。 Phaver 是一款适用于 iOS 和 Android 的 Share-to-Earn 社交应用，用户可以发布帖子，内容可以是图片、链接、产品应用等。用户还可以浏览 Lens 内的所有内容。Lens Profile 用户连接钱包后，还可以通过 Phaver 直接发布帖子到 Lens 中。 re:meme 是一个链上 meme 生成器，允许用户上传 meme 模版，也能选择是否收费，然后其他人可以用图像编辑器添加文本、绘图和补充图像等。:meme 还可以扩展到音乐、视频和学术论文等媒体格式。 Lensport 是一个只聚焦于 Lens 协议的社交 NFT 市场，用户可以发现、发布和出售帖子，也可以投资支持创作者。 Lensta 是一个聚焦于 Lens 协议的图片流应用，可以浏览 Lens 中带有图片的最新、最热门以及 Lenster、Lensport 等上收集费用最多的帖子。 访问层 访问层是 Web3 组成架构里的最上层，也是直接面向终端用户的入口层。这一层里主要包括钱包、浏览器、聚合器等，另外，有一些 Web2 的社交媒体平台也成为了 Web3 的入口。 先来看看钱包，这也是最主要的入口。目前的钱包有多种分类，有浏览器钱包、手机钱包、硬件钱包、多签钱包、MPC 钱包、智能合约钱包等。 浏览器钱包就是通过网络浏览器使用的加密钱包，是大部分用户使用最广泛的钱包，最常用的就是 MetaMask、Coinbase Wallet、WalletConnect 等。MetaMask 是最被广泛支持的钱包之一，支持所有的 EVM 链，也已经成为了所有 DApps 的标准，目前支持的浏览器包括 Chrome、Brave、Firefox、Edge，以浏览器插件的方式存在。Coinbase Wallet 顾名思义是由交易所 Coinbase 所发行的钱包，于 2021 年 11 月推出后迅速发展，成为了与 MetaMask 旗鼓相当的对手，但浏览器还只支持 Chrome。WalletConnect 则比较特殊，它并不是一款具体的钱包应用，而是连接 DApps 和钱包的开源协议。最常用的就是用于连接手机钱包，在浏览器上的 DApp 选择连接 WalletConnect，会展示一个二维码，用你的手机钱包扫这个二维码就可以授权你的手机钱包连接上浏览器上的 DApp。而且，WalletConnect 支持所有区块链，不只是 EVM 链，也支持接入所有钱包。另外，不像 MetaMask 和 Coinbase Wallet 需要安装其浏览器插件，WalletConnect 不需要安装浏览器插件，所以可以支持所有浏览器，比如也支持 Safari，而 MetaMask 和 Coinbase Wallet 是不支持 Safari 的。因此，WalletConnect 成为了最受欢迎的钱包，也成为了所有 DApp 接入钱包的标配。 手机钱包，即移动端数字资产钱包，很多钱包都支持。MetaMask 和 Coinbase Wallet 也有手机端的钱包 App。另外，比较知名的手机钱包还有 TokenPocket、BitKeep、Rainbow、imToken、Crypto.com 等。大部分流行的手机钱包都支持多链，包括 EVM 链，也包括 Non-EVM 链，比如 TokenPocket 目前支持了 Bitcoin、Ethereum、BSC、TRON、Polygon、Arbitrum、Avalanche、Solana、Cosmos、Polkadot、Aptos 等。 硬件钱包则是把数字资产私钥存储在安全的硬件设备中，与互联网隔离，可通过 USB 即插即用。现在使用最广泛的硬件钱包是 Ledger 和 Trezor。Ledger 目前有三款不同型号的硬件钱包：Ledger Stax、Ledger Nano X、Ledger Nano S Plus。Ledger Stax 是在 2023 年才推出的新型号，支持触摸屏，而另外两款则不支持。Trezor 则有两款型号：Trezor Model T 和 Trezor Model One。Model T 支持触摸屏。除了 Ledger 和 Trezor，市面上的硬件钱包还有 SafePal、OneKey、imKey、KeepKey、ColdLar 等。 多签钱包，顾名思义，是指需要多人签名才能执行操作的钱包。最知名的多签钱包就是 Gnosis Safe，其本质上是一套链上智能合约，最常用的就是 2/3 签名，即总共有三个用户共同管理钱包，每次执行操作时，需要这三人中至少两个人的签名才能触发链上执行。 MPC 全称为 Multi-Party Computation，MPC 钱包也称为多方计算钱包，是新一代钱包类型，通过对私钥进行多方计算在链下实现多签和跨链等复杂的验证方式。简单来说，就是将私钥拆分成多个分片，然后由多方各自存储管理每个分片，签名的时候，再联合多方将分片重新拼接成完整的私钥。MPC 钱包与多签签名很类似，也可以实现 2/3 签名，不同的是，多签钱包是在智能合约层面实现签名校验，而 MPC 钱包则是通过链下计算实现的。目前已提供 MPC 钱包服务的还不多，主要有 ZenGo、Safeheron、Fordefi、OpenBlock、web3auth 等。 智能合约钱包就是使用智能合约账户作为地址的钱包，多签钱包 Gnosis Safe 也属于智能合约钱包。而近一两年对智能合约钱包最新的尝试则是结合「账户抽象（Account Abstraction）」的新一代钱包。账户抽象主要是要将签名者和账户分离开来，钱包地址不再与唯一的私钥强绑定，可以实现更换签名者，也可以实现多签，还可以实现更换签名算法。目前在这个赛道的选手除了 Gnosis Safe 还有 UniPass、Argent、Blocto 等。 钱包暂时就聊这么多，接着来说说浏览器。很多 DApp 都还是只提供了网页版本的前端，所以浏览器就成为了重要的访问入口。但因为不是所有浏览器都支持钱包扩展插件，所以也不是所有浏览器都能成为很好的 Web3 入口。最常用的浏览器是 Chrome，所有浏览器钱包都会开发 Chrome 的钱包插件。而 Safari 则很少用做 Web3 DApp 入口，因为除了 WalletConnect，没有其他浏览器钱包能够支持。还有一个值得介绍的浏览器是 Brave，这是一款内置了钱包的浏览器，其内置钱包叫 Brave Wallet。 有一些聚合器也是 Web3 的访问入口，比如 DappRadar 收集了各种 DApps，用户可以通过它浏览并连接到这些 DApps。还有 Zapper、DeBank、Zerion 之类的聚合器，可以帮助用户追踪他们在各种 Web3 应用的所有资产和操作记录。 最后，像 Twitter 和 Reddit 这类 Web3 的社交媒体平台，因为聚集了很多 Web3 社群，也逐渐变成了 Web3 的访问入口。 ","link":"https://jobslee0.github.io/post/wan-zi-chang-wen-liao-liao-web3-de-zu-cheng-jia-gou/"},{"title":"B树与B+树","content":"B树和B+树都是常见的数据结构，用于在磁盘等外存储介质上存储数据并进行高效的查询、插入和删除操作。它们的主要区别在于： 结构差异 B树的每个节点包含一个键值和一个指向子节点的指针数组。而B+树的每个非叶子节点只包含键值，而不包含指向数据的指针。所有的数据都保存在叶子节点中，并且叶子节点通过一个指针链表连接在一起。 叶子节点区别 B树的叶子节点包含指向数据的指针和数据本身，而B+树的叶子节点只包含数据本身。这使得B+树的叶子节点能够更好地利用磁盘块，因为每个节点只存储数据而不需要指针信息。 遍历方式 在B树中，由于每个节点都包含指向子节点的指针，因此在进行遍历时需要进行递归操作。而在B+树中，由于数据只存储在叶子节点中，因此在进行遍历时只需要遍历叶子节点即可。 查询性能 由于B+树的叶子节点只包含数据本身，因此在进行范围查询时只需要遍历叶子节点，而不需要进行中间节点的遍历。这使得B+树在范围查询时具有更高的查询性能。 总的来说，B+树比B树更适合于磁盘等外存储介质上的数据存储和查询，因为它具有更好的磁盘块利用率和查询性能。而B树则更适用于内存中的数据结构。 ","link":"https://jobslee0.github.io/post/b-shu-yu-bshu/"},{"title":"几种常见的IO模型","content":"传统IO Reactor事件驱动 Reactor-业务IO分离 Reactor-并发读写 ","link":"https://jobslee0.github.io/post/ji-chong-chang-jian-de-io-mo-xing/"},{"title":"PSQ队列核心原理分析","content":" PSQ-Github地址 PSQ是友缘在线早些年，由大师独自编写的一个轻量级队列，该队列开箱即用、效率高，可以达到每秒并发40000-50000个请求，并开创性提供了队列回溯消费的功能。 技术架构 核心原理 底层数据结构：最底层使用了leansoft.bigqueue，此队列基于内存映射文件，实现了LRU及页映射特性 数据结构扩展：基于原始队列，构建环形队列，并支持回溯、持久化（使用NIO）特性 网络接口：网络接口基于Netty实现，根据参数定制对应消息对象，由于NIO的模型特性，网络IO效率非常的高 服务包装：服务使用tanukisoftware.wrapper进行了包装，启动比较轻巧，可以很好的跟操作系统融合 扩展特性：系统使用JMX扩展进行了Bean的监控和管理，有效提高了系统的可操作性 磁盘与内存 在 LeanSoft BigQueue 中，Memory mapped page 是一种利用内存映射文件实现的存储方式。它允许应用程序映射一个磁盘文件到内存，从而避免从磁盘读取数据的慢速过程。通过使用 Memory mapped page，LeanSoft BigQueue 可以在内存和磁盘之间平衡读写性能，从而提高吞吐量。 Java 中可以使用 FileChannel 类刷新直接内存到磁盘。你可以使用该类打开一个文件，并使用 map 方法将该文件映射到直接内存。然后，你可以使用该内存中的数据进行操作，并通过调用 force 方法将其写入磁盘。 以下是使用直接内存和 FileChannel 类的示例代码： import java.io.File; import java.io.IOException; import java.io.RandomAccessFile; import java.nio.MappedByteBuffer; import java.nio.channels.FileChannel; public class DirectMemoryExample { public static void main(String[] args) throws IOException { File file = new File(&quot;data.dat&quot;); // Open the file and map it to memory RandomAccessFile raf = new RandomAccessFile(file, &quot;rw&quot;); FileChannel channel = raf.getChannel(); MappedByteBuffer buffer = channel.map(FileChannel.MapMode.READ_WRITE, 0, 1024); // Write data to the memory buffer.putInt(100); buffer.putInt(200); // Flush the changes to disk buffer.force(); // Close the file channel.close(); raf.close(); } } ","link":"https://jobslee0.github.io/post/psq/"},{"title":"记一次JVM泄漏问题及解决方案","content":"问题现象 前段时间，公司线上的服务器开始出现pod反复重启的现象，通过普罗米修斯监控大盘，发现是JVM内存突破了pod限制。 问题分析 现象分析 硬件配置 我们的应用服务是使用k8s来部署的，应用的实现是jdk8，出现问题的服务pod分配的总数为3台，每个pod分配3G内存和1个高性能CPU，pod的内存required和limit分别为2G和3G。 JVM参数 默认参数，未配置。 JVM内存分析 年轻代正常；老年代占用较多，可能有大对象存在；老年代内存增长同pod增长趋势相同，且有按小时增长迹象，并会触发FullGC。 问题猜测 随着业务增长，pod内存可能确实不足 JVM默认参数无法有效限制JVM的内存使用 某个小时任务存在问题，且使用了大对象 大对象触发FullGC后并没有有效释放内存 查询k8s官方Issues，是否存在pod与Java版本兼容bug 尝试解决 增加pod内存，使得required=4G limit=5G 增加JVM参数限制-Xms=4G -Xmx=4G，堆外内存也要限制，（特别注意jdk8变为了元空间）-XX:MetaspaceSize=300m -XX:MaxMetaspaceSize=300m 翻查代码，确实发现某个定时任务存在着大批量的List内存存储，且长时间不释放，但是短时间内没有更好的修改替代方案 调整年轻代大小-Xmn，使大对象在年轻代就被回收，而不进入老年代 升级jdk8的小版本，提升jvm对容器限制的感知（这块见参考文档3） 尝试结果 JVM内存增长导致的重启次数变得减少一点，部分FullGC之后pod容器依然健在，但是时间久了依然会被kill重启 大对象没有被提前回收，依然进入了老年代，且FullGC明显增多 初步结论 业务增长pod内存确实需要适当增加 JVM参数需要进行手动限制，但是年轻代大小可以不需要调整，反而可以降低FullGC次数 代码端想更好的优化方法，尽量让数据对象生命周期缩短，但是改动依然比较难 jdk8小版本有对容器限制感知的修复，有一定用处（这块见参考文档3） 问题解决 保持上面的解决参数和思路，我们的服务勉强保持了一段时间，但是告警和重启依然存在，实在令人头疼，很明显还是有一部分JVM的内存泄漏。 后来通过不断的排查监控大盘，我发现堆内内存经过FullGC后，都可以做到到达临界线后不再增长，这说明堆内参数大小和限制都起到了很好的作用；但是要知道，JVM还有一部分叫做堆外内存的东西，参数中限制了我们常知道的MetaSpace，在大盘中我却发现了non-heap memory部分总有比MetaSpace多出的300M，其增长趋势同pod内存一样一致，只是不是那么明显。 这引发了我进一步的思考，堆外内存是不是除了元数据区，还有一块未曾想到的区域？ 经过翻查资料： 发现有一块叫做Direct Memory的区域，这块区域就是我们常提到的NIO为了减少内存拷贝而直接申请的部分，这部分在常见数据库读写客户端中大量存在。 通过排查代码，果不其然，代码除了保留List大对象之外，还大量使用了RedisTemplate对象，这个客户端底层连接基于netty实现，正是NIO那部分。 于是，加上-MaxDirectMemorySize参数后，再次测试，JVM内存稳固被限制，果然没有问题了！ 问题总结 总的来看，这次的问题DirectMemory是一个主要问题，代码的大对象处理是一个次要问题，其次才应该考虑对JVM的FullGC次数进行优化。 被忽略的DirectMemory因为默认为堆大小的1/4，加上我们大量基于RedisTemplate中NIO机制的Redis读写和本身POD的大小有限，最终这部分未被考虑的内存不断增长，导致最后POD内存溢出被kill掉，而这个过程因为本身还未达到JVM本身内存大小的限制，所以也是没有看到JVM的OOM日志的原因。 这里再对DirectMemory进行一个简单的总结： Direct Memory 不是由Java虚拟机直接管理的，而是由操作系统管理。因此，Direct Memory 不会受到Java虚拟机的垃圾回收机制的影响，它的回收是由操作系统自行处理的。 虽然Direct Memory不是由Java虚拟机管理的，但是Java虚拟机会在启动时将一块内存注册到操作系统中，用于存放 Direct Memory，这块内存也可以被限制大小；同样，Direct Memory在必要时需要手动进行释放，否则会出现内存泄漏的情况。 JVM中的Direct Memory是通过Native Memory来实现的，即直接在操作系统中申请的内存空间，而不是通过Java Heap来分配的。通常，Direct Memory可以通过调用java.nio.Buffer类的clean()方法进行释放。此外，也可以通过反射来调用sun.misc.Cleaner类的clean()方法来释放Direct Memory。在JDK9中，引入了一个新的API，即jdk.internal.misc.Unsafe类的invokeCleaner()方法，该方法可以用于显式地释放Direct Memory。调用该方法会触发Cleaner机制，异步清理Direct Memory。当该内存块没有被引用时，Cleaner线程将清理该内存块。 MaxDirectMemorySize 是控制 Direct Memory 最大分配空间的参数，它的默认值与 Java 堆最大空间有关，通常为 Java 堆最大空间的1/4。当应用程序申请的 Direct Memory 超过了这个值时，将会抛出 OutOfMemoryError 异常。 JVM是一个复杂的大工程，它帮我们很好的完成了对内存的管理；出现了内存泄漏不可怕，要从现象和问题本身出发，要从JVM本身出发，万万不能被所谓的“八股文”和“权威”给束缚住，上来就考虑GC优化。 那些被刻板经验给忽略的，往往就是真相，仅此而已。 参考文档 https://blog.csdn.net/tterminator/article/details/54342666 https://zhuanlan.zhihu.com/p/370241822 https://blog.51cto.com/lookingdream/4046529 https://juejin.cn/post/6844903894863052814 ","link":"https://jobslee0.github.io/post/ji-yi-ci-jvm-xie-lou-wen-ti-ji-jie-jue-fang-an/"},{"title":"算法学习笔记","content":"常见时间复杂度 一个顺序结构的代码，时间复杂度是 O(1) 二分查找，或者更通用地说是采用分而治之的二分策略，时间复杂度都是 O(logn) 一个简单的 for 循环，时间复杂度是 O(n) 两个顺序执行的 for 循环，时间复杂度是 O(n)+O(n)=O(2n)，其实也是 O(n) 两个嵌套的 for 循环，时间复杂度是 O(n²) 时间复杂度演进 第一步，暴力解法。在没有任何时间、空间约束下，完成代码任务的开发。 第二步，无效操作处理。将代码中的无效计算、无效存储剔除，降低时间或空间复杂度。 第三步，时空转换。设计合理数据结构，完成时间复杂度向空间复杂度的转移。 3 个基本操作，增、删、查；3个基本思考，设计合理数据结构的方法论 首先，这段代码对数据进行了哪些操作？ 其次，这些操作中，哪个操作最影响效率，对时间复杂度的损耗最大？ 最后，哪种数据结构最能帮助你提高数据操作的使用效率？ 解决代码问题的方法论；宏观上，它可以分为以下 4 个步骤： 复杂度分析（估算问题中复杂度的上限和下限） 定位问题（根据问题类型，确定采用何种算法思维） 数据操作分析（根据增、删、查和数据顺序关系去选择合适的数据结构，利用空间换取时间） 编码实现 线性表 种类 单向链表 循环链表 双向链表 双向循环链表 基础操作 // 增 s.next = p.next; p.next = s; // 删 p.next = p.next.next; // 查O(n) 常见方法 // 翻转 while(curr){ next = curr.next; curr.next = prev； prev = curr; curr = next; } // 中间节点/环（快慢指针） while(fast &amp;&amp; fast.next &amp;&amp; fast.next.next){ fast = fast.next.next; slow = slow.next; } 使用场景 线性表对数据的存储方式是按照顺序的存储 当数据的元素个数不确定，且需要经常进行数据的新增和删除时，那么链表会比较合适 栈 特性 后进先出 栈顶（top） 栈底（bottom） 继承了线性表的优点与不足，是个限制版的线性表，只允许数据从栈顶进出 种类 顺序栈（借助数组来实现） 链栈（用链表的方式对栈的表示） 基础操作 push 或压栈 pop 或出栈 使用场景 需要高频使用新增、删除操作，且新增和删除操作的数据执行顺序具备后来居上的相反关系时，栈就是个不错的选择 浏览器的前进和后退，括号匹配等问题 队列 特性 先进先出 队头（front） 队尾（rear） 当队列为空时，front 和 rear 都指向头结点 有了头结点后，哪怕队列为空，头结点依然存在，能让 front 指针和 rear 指针依然有意义 继承了线性表的优点与不足，是加了限制的线性表，队列的增和删的操作只能在这个线性表的头和尾进行 种类 顺序队列（依赖数组来实现） 链式队列（依赖链表来实现） 循环队列（约瑟夫环问题） 使用场景 在可以确定队列长度最大值时，建议使用循环队列 无法确定队列长度时，应考虑使用链式队列 很像现实中人们排队买票的场景，在面对数据处理顺序非常敏感的问题时，队列一定是个不错的技术选型 数组 特性 定义简单，访问方便 所有元素类型必须相同 最大长度必须在定义时给出 使用的内存空间必须连续 基础操作 增加：若插入数据在最后，则时间复杂度为 O(1)；如果中间某处插入数据，则时间复杂度为 O(n)。 删除：对应位置的删除，扫描全数组，时间复杂度为 O(n)。 查找：如果只需根据索引值进行一次查找，时间复杂度是 O(1)；但是要在数组中查找一个数值满足指定条件的数据，则时间复杂度是 O(n) 使用场景 在数据数量确定，即较少甚至不需要使用新增数据、删除数据操作的场景下使用，这样就有效地规避了数组天然的劣势 在数据对位置敏感的场景下，比如需要高频根据索引位置查找数据时，数组就是个很好的选择了 链表存在的价值 首先，链表的长度是可变的，数组的长度是固定的，在申请数组的长度时就已经在内存中开辟了若干个空间；如果没有引用 ArrayList 时，数组申请的空间永远是我们在估计了数据的大小后才执行，所以在后期维护中也相当麻烦 其次，链表不会根据有序位置存储，进行插入数据元素时，可以用指针来充分利用内存空间；数组是有序存储的，如果想充分利用内存的空间就只能选择顺序存储，而且需要在不取数据、不删除数据的情况下才能实现 字符串 类型 顺序存储（用一组地址连续的存储单元来存储串中的字符序列，一般是用定长数组来实现，有些语言会在串值后面加一个不计入串长度的结束标记符，比如 \\0 来表示串值的终结） 链式存储（一个结点可以考虑存放多个字符，如果最后一个结点未被占满时，可以使用 &quot;#&quot; 或其他非串值字符补全） 基础操作 在线性表的基本操作中，大多以“单个元素”作为操作对象 在字符串的基本操作中，通常以“串的整体”作为操作对象 增O(n) 删O(n) 查 子串查找（字符串匹配）（这种匹配算法需要从主串中找到跟模式串的第 1 个字符相等的位置，然后再去匹配后续字符是否与模式串相等O(nm)） 树 特性 结点和边 不存在环 深度 层 一对多 类型 链式存储法 顺序存储法（结点 X 的下标为 i，那么 X 的左子结点总是存放在 2 * i 的位置，X 的右子结点总是存放在 2 * i + 1 的位置） 二叉树（每个结点最多有两个分支，即每个结点最多有两个子结点，分别称作左子结点和右子结点）O(n) 满二叉树，定义为除了叶子结点外，所有结点都有 2 个子结点 完全二叉树，定义为除了最后一层以外，其他层的结点个数都达到最大，并且最后一层的叶子结点都靠左排列（对于一棵完全二叉树而言，仅仅浪费了下标为 0 的存储位置。而如果是一棵非完全二叉树，则会浪费大量的存储空间） 二叉查找树（也称作二叉搜索树）O(logn) 左小右大 规避相等 中序有序 基础操作 前序遍历（根左右） 中序遍历（左根右） 后序遍历（左右根） 二叉查找树（删除） 情况一，如果要删除的结点是某个叶子结点，则直接删除，将其父结点指针指向 null 即可 情况二，如果要删除的结点只有一个子结点，只需要将其父结点指向的子结点的指针换成其子结点的指针即可 情况三，如果要删除的结点有两个子结点，则有两种可行的操作方式 第一种，找到这个结点的左子树中最大的结点，替换要删除的结点 第二种，找到这个结点的右子树中最小的结点，替换要删除的结点 哈希表 特性 哈希表的核心思想：如果有一种方法，可以实现“地址 = f (关键字)”的映射关系，那么就可以快速完成基于数据的数值的查找了O(1） hash方法 直接定制法（H (key) = a*key + b。 这里，a 和 b 是设置好的常数） 数字分析法（每个关键字 key 都是由 s 位数字组成（k1,k2,…,Ks），并从中提取分布均匀的若干位组成哈希地址） 平方取中法（先求关键字的平方值，通过平方扩大差异，然后取中间几位作为最终存储地址） 折叠法（取它们的叠加和的值（舍去进位）作为哈希地址） 除留余数法（key mod p） 解决冲突 开放定址法 链地址法 基础操作 对于给定的 key，通过哈希函数计算哈希地址 H (key) 使用场景 如果不需要有序遍历数据，并且可以提前预测数据量的大小，那么哈希表在速度和易用性方面是无与伦比的 递归（Recursion） 递归的基本思想：把规模大的问题转化为规模小的相同的子问题来解决 这个解决问题的函数必须有明确的结束条件，否则就会导致无限递归的情况 递归的实现包含了两个部分，一个是递归主体，另一个是终止条件 数学归纳法 当一个问题同时满足以下 2 个条件时，就可以使用递归的方法求解 可以拆解为除了数据规模以外，求解思路完全相同的子问题 存在终止条件 写出递归代码的关键在于 写出递推公式 找出终止条件 递归的应用非常广泛，很多数据结构和算法的编码实现都要用到递归，例如分治策略、快速排序等 分治 分而治之 互相独立 形式相同 原问题都需要具备以下几个特征： 难度在降低 问题可分 解可合并 相互独立（各个子问题之间相互独立，某个子问题的求解不会影响到另一个子问题） 分治法在每轮递归上，都包含了分解问题、解决问题和合并结果这 3 个步骤 二分查找O(logn) 输入的数列是有序的 选择一个标志 i 将集合 L 分为二个子集合，一般可以使用中位数 判断标志 L(i) 是否能与要查找的值 des 相等，相等则直接返回结果 如果不相等，需要判断 L(i) 与 des 的大小 基于判断的结果决定下步是向左查找还是向右查找；如果向某个方向查找的空间为 0，则返回结果未查到 回到初始步骤 二分查找的时间复杂度是 O(logn)（当你面对某个代码题，而且约束了时间复杂度是 O(logn) 或者是 O(nlogn) 时，可以想一下分治法是否可行） 二分查找的循环次数并不确定，一般是达到某个条件就跳出循环（编码的时候，多数会采用 while 循环加 break 跳出的代码结构） 二分查找处理的原问题必须是有序的（当你在一个有序数据环境中处理问题时，可以考虑分治法；相反，如果原问题中的数据并不是有序的，则使用分治法的可能性就会很低了） 在面对陌生问题时，需要注意原问题的数据是否有序，预期的时间复杂度是否带有 logn 项，是否可以通过小问题的答案合并出原问题的答案。如果这些先决条件都满足，你就应该第一时间想到分治法 排序 冒泡排序 插入排序 归并排序 快速排序 动态规划 问题的解决难度与数据规模有关 原问题可被分解 子问题的解可以合并为原问题的解 所有的子问题相互独立 运筹学方法，多轮决策过程中的最优方法 最短路径问题 状态 通用的方法论（k 表示多轮决策的第 k 轮） 分阶段，将原问题划分成几个子问题（一个子问题就是多轮决策的一个阶段，它们可以是不满足独立性的） 找状态，选择合适的状态变量 Sk（它需要具备描述多轮决策过程的演变，更像是决策可能的结果） 做决策，确定决策变量 uk（每一轮的决策就是每一轮可能的决策动作，例如 D2 的可能的决策动作是 D2 -&gt; E2 和 D2 -&gt; E3） 状态转移方程（这个步骤是动态规划最重要的核心，即 sk+1= uk(sk) ） 定目标（写出代表多轮决策目标的指标函数 Vk,n） 寻找终止条件 补充概念 策略（每轮的动作是决策，多轮决策合在一起常常被称为策略） 策略集合（由于每轮的决策动作都是一个变量，这就导致合在一起的策略也是一个变量；我们通常会称所有可能的策略为策略集合；动态规划的目标，也可以说是从策略集合中，找到最优的那个策略） 如下几个特征的问题，可以采用动态规划求解 最优子结构（它的含义是，原问题的最优解所包括的子问题的解也是最优的；例如，某个策略使得 A 到 G 是最优的。假设它途径了 Fi，那么它从 A 到 Fi 也一定是最优的） 无后效性（某阶段的决策，无法影响先前的状态；可以理解为今天的动作改变不了历史） 有重叠子问题（也就是，子问题之间不独立；**这个性质是动态规划区别于分治法的条件；**如果原问题不满足这个特征，也是可以用动态规划求解的，无非就是杀鸡用了宰牛刀） ","link":"https://jobslee0.github.io/post/suan-fa-xue-xi-bi-ji/"},{"title":"Trouble Shoot","content":"Swagger2 and Spring-Healthy-Check Integration Issues KeyWords: org.springframework.context.ApplicationContextException: Failed to start bean 'documentationPluginsBootstrapper'; nested exception is java.lang.NullPointerException Version: &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; Describe: After the two components are integrated, we open the spring-healthy-check and will see the &quot;Exception&quot; due to the healthy-check bug. Solution: /** * 解决swagger配合健康检查时，无法启动的问题 * @param webEndpointsSupplier * @param servletEndpointsSupplier * @param controllerEndpointsSupplier * @param endpointMediaTypes * @param corsProperties * @param webEndpointProperties * @param environment * @return */ @Bean public WebMvcEndpointHandlerMapping webEndpointServletHandlerMapping(WebEndpointsSupplier webEndpointsSupplier, ServletEndpointsSupplier servletEndpointsSupplier, ControllerEndpointsSupplier controllerEndpointsSupplier, EndpointMediaTypes endpointMediaTypes, CorsEndpointProperties corsProperties, WebEndpointProperties webEndpointProperties, Environment environment) { List&lt;ExposableEndpoint&lt;?&gt;&gt; allEndpoints = new ArrayList&lt;&gt;(); Collection&lt;ExposableWebEndpoint&gt; webEndpoints = webEndpointsSupplier.getEndpoints(); allEndpoints.addAll(webEndpoints); allEndpoints.addAll(servletEndpointsSupplier.getEndpoints()); allEndpoints.addAll(controllerEndpointsSupplier.getEndpoints()); String basePath = webEndpointProperties.getBasePath(); EndpointMapping endpointMapping = new EndpointMapping(basePath); boolean shouldRegisterLinksMapping = webEndpointProperties.getDiscovery().isEnabled() &amp;&amp; (org.springframework.util.StringUtils.hasText(basePath) || ManagementPortType.get(environment).equals(ManagementPortType.DIFFERENT)); return new WebMvcEndpointHandlerMapping(endpointMapping, webEndpoints, endpointMediaTypes, corsProperties.toCorsConfiguration(), new EndpointLinksResolver(allEndpoints, basePath), shouldRegisterLinksMapping, null); } Reference: https://github.com/springfox/springfox/issues/3462 Sharding-JDBC and Spring-Healthy-Check Integration Issues KeyWords: WARN o.s.boot.actuate.jdbc.DataSourceHealthIndicator:94 - DataSource health check failed org.springframework.dao.InvalidDataAccessApiUsageException: ConnectionCallback; isValid; nested exception is java.sql.SQLFeatureNotSupportedException: isValid Version: &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;4.1.1&lt;/version&gt; &lt;/dependency&gt; Describe: After the two components are integrated, we open the spring-healthy-check and will see the &quot;Exception&quot; due to the sharding-jdbc was not implementing the healthy-check api. Solution: /** * 数据库检查配置 */ @Configuration public class DbCheckConfig { /** * 对数据库检测做默认处理（健康健康检查用到sharding-jdbc时，该组件没有完全实现MySQL驱动会导致问题） */ @Bean public DataSourcePoolMetadataProvider dataSourcePoolMetadataProvider() { return dataSource -&gt; new DefaultDataSourcePoolMetadata(); } /** * 默认的数据源池元数据 */ private static class DefaultDataSourcePoolMetadata implements DataSourcePoolMetadata { @Override public Float getUsage() { return null; } @Override public Integer getActive() { return null; } @Override public Integer getMax() { return null; } @Override public Integer getMin() { return null; } @Override public String getValidationQuery() { // 用于检查的简单查询语句 return &quot;select 1 from dual&quot;; } @Override public Boolean getDefaultAutoCommit() { return null; } } } // or @SpringBootApplication(exclude = {DataSourceHealthContributorAutoConfiguration.class}) Reference: https://www.cnblogs.com/laeni/p/16089788.html Swagger2 and Spring-Healthy-Check Integration Issues KeyWords: org.springframework.context.ApplicationContextException: Failed to start bean 'documentationPluginsBootstrapper'; nested exception is java.lang.NullPointerException Version: &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; Describe: After the two components are integrated, we open the spring-healthy-check and will see the &quot;Exception&quot; due to the healthy-check bug. Solution: /** * 解决swagger配合健康检查时，无法启动的问题 * @param webEndpointsSupplier * @param servletEndpointsSupplier * @param controllerEndpointsSupplier * @param endpointMediaTypes * @param corsProperties * @param webEndpointProperties * @param environment * @return */ @Bean public WebMvcEndpointHandlerMapping webEndpointServletHandlerMapping(WebEndpointsSupplier webEndpointsSupplier, ServletEndpointsSupplier servletEndpointsSupplier, ControllerEndpointsSupplier controllerEndpointsSupplier, EndpointMediaTypes endpointMediaTypes, CorsEndpointProperties corsProperties, WebEndpointProperties webEndpointProperties, Environment environment) { List&lt;ExposableEndpoint&lt;?&gt;&gt; allEndpoints = new ArrayList&lt;&gt;(); Collection&lt;ExposableWebEndpoint&gt; webEndpoints = webEndpointsSupplier.getEndpoints(); allEndpoints.addAll(webEndpoints); allEndpoints.addAll(servletEndpointsSupplier.getEndpoints()); allEndpoints.addAll(controllerEndpointsSupplier.getEndpoints()); String basePath = webEndpointProperties.getBasePath(); EndpointMapping endpointMapping = new EndpointMapping(basePath); boolean shouldRegisterLinksMapping = webEndpointProperties.getDiscovery().isEnabled() &amp;&amp; (org.springframework.util.StringUtils.hasText(basePath) || ManagementPortType.get(environment).equals(ManagementPortType.DIFFERENT)); return new WebMvcEndpointHandlerMapping(endpointMapping, webEndpoints, endpointMediaTypes, corsProperties.toCorsConfiguration(), new EndpointLinksResolver(allEndpoints, basePath), shouldRegisterLinksMapping, null); } Reference: https://github.com/springfox/springfox/issues/3462 Sharding-JDBC and Spring-Healthy-Check Integration Issues KeyWords: WARN o.s.boot.actuate.jdbc.DataSourceHealthIndicator:94 - DataSource health check failed org.springframework.dao.InvalidDataAccessApiUsageException: ConnectionCallback; isValid; nested exception is java.sql.SQLFeatureNotSupportedException: isValid Version: &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator-autoconfigure&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;4.1.1&lt;/version&gt; &lt;/dependency&gt; Describe: After the two components are integrated, we open the spring-healthy-check and will see the &quot;Exception&quot; due to the sharding-jdbc was not implementing the healthy-check api. Solution: /** * 数据库检查配置 */ @Configuration public class DbCheckConfig { /** * 对数据库检测做默认处理（健康健康检查用到sharding-jdbc时，该组件没有完全实现MySQL驱动会导致问题） */ @Bean public DataSourcePoolMetadataProvider dataSourcePoolMetadataProvider() { return dataSource -&gt; new DefaultDataSourcePoolMetadata(); } /** * 默认的数据源池元数据 */ private static class DefaultDataSourcePoolMetadata implements DataSourcePoolMetadata { @Override public Float getUsage() { return null; } @Override public Integer getActive() { return null; } @Override public Integer getMax() { return null; } @Override public Integer getMin() { return null; } @Override public String getValidationQuery() { // 用于检查的简单查询语句 return &quot;select 1 from dual&quot;; } @Override public Boolean getDefaultAutoCommit() { return null; } } } // or @SpringBootApplication(exclude = {DataSourceHealthContributorAutoConfiguration.class}) Reference: https://www.cnblogs.com/laeni/p/16089788.html Sharding-JDBC Load Meta Data Slowly KeyWords: ShardingSphere-metadata : Meta data load finished, cost 471600 milliseconds. Version: &lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;4.1.1&lt;/version&gt; &lt;/dependency&gt; Describe: When spring application starts, the application always waits for a long time and then shows the log above. Actually, the load time could be shortened by config. Solution: spring.shardingsphere.props.max.connections.size.per.query = 50 Reference: https://github.com/apache/shardingsphere/issues/6212 The Apollo-Config is Unable to Update the Configuration of Application KeyWords: com.ctrip.framework.apollo jasypt-spring-boot-starter Unable to Update the Configuration Version: &lt;dependency&gt; &lt;groupId&gt;com.ctrip.framework.apollo&lt;/groupId&gt; &lt;artifactId&gt;apollo-client&lt;/artifactId&gt; &lt;version&gt;1.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.ulisesbocchio&lt;/groupId&gt; &lt;artifactId&gt;jasypt-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.0.4&lt;/version&gt; &lt;/dependency&gt; Describe: After using jasypt-spring-boot-starter component, I found that the modifies of Apollo-Config could not been updated in Spring-Application. It's because the high version of jasypt-spring-boot-starter conflicts with com.ctrip.framework.apollo, the detail in the Reference below. Solution: Use the version less than 2.2.0 for jasypt-spring-boot-starter(Please pay attention to the difference between the version 2.0.0 and the version 3.3.0 for the encryption algorithm). Implement the ConfigChangeListener in Apollo by yourself. Reference: https://github.com/apolloconfig/apollo/issues/2162 https://blog.csdn.net/qingquanyingyue/article/details/109197386 ","link":"https://jobslee0.github.io/post/trouble-shoot/"},{"title":"MongoDB实战问答","content":"MongoDB底层实现是B树还是B+树？ B+树 这个问题国内论坛真的是难以找到靠谱的答案，大部分给出的都是不准确的B树，当然B+树确实是B树的一种，但是准确的说当前版本MongoDB底层实现应该是B+树才是准确的 下面来给出具体的说明： 我们知道MongoDB当下支持两种引擎，一个是默认的WiredTiger，另一个是In-Memory，In-Memory是保存在内存中，我们平时讨论和使用的是默认的WiredTiger（本讨论从MongoDB3.2将WiredTiger作为默认引擎的前提下展开，至于老版本的数据结构这里忽略掉） WiredTiger文档中有这么一句话： 大概意思就是使用称为B-Tree（具体为B+Tree）的数据结构在内存中维护表的数据，这直接说明了具体的数据结构应该为B+树，只不过B+树是B树的一种变形而已 MongoDB大数据量分页怎么做？ count语法会导致全局扫表，不建议使用count做页码总数的统计 对于不带条件的全局数据统计，建议使用estimatedDocumentCount 使用自定义带索引的标识或者_id中包含的时间戳，作为范围查询的条件，并往后limit有限条数据 MongoDB是否需要自行实现_id来提升性能？ 不需要，_id基于MongoDB自定义的规则实现，插入时直接生成，无需检查重复，效率已经足够 具体可参照前两篇MongoDB文章 MongoDB读写分离有延迟怎么办？ 检查服务器间网络是否正常 检查服务器磁盘是否正常 通过节点opLog日志排查是否同步正常 升级4.4版本使用流复制提升效率 读写分离这种主从部署的方式，在新版本中已经不建议使用，建议使用副本集进行读写 MongoDB大数据量写索引慢有什么办法解决？ MongoDB在写入索引的时候存在阻塞，所以最开始设计就要考虑合理的索引设计 对于不太关注历史数据的数据库，可以使用部分索引，减少索引构建的数据量 定期清理数据，减少数据量 MongoDB新版本有哪些需要关注的新特性？ 6.0版本-上限集合 5.0版本-时间序列 4.4版本-流复制 ","link":"https://jobslee0.github.io/post/mongodb-shi-zhan-wen-da/"},{"title":"MongoDB与MySQL的插入、查询性能测试","content":" 原文链接：https://www.cnblogs.com/wodeboke-y/p/10828985.html 7.1 平均每条数据的插入时间 先上张图，来点直观感受： 图上数据横坐标是平均每插入1000条数据所需要的时间，单位是秒。记住，是每1000条数据，不是每条数据哦。 总结： 数据库的平均插入速率：MongoDB不指定_id插入 &gt; MySQL不指定主键插入 &gt; MySQL指定主键插入 &gt; MongoDB指定_id插入。 MongoDB在指定_id与不指定_id插入时速度相差很大，而MySQL的差别却小很多。 分析： 在指定_id或主键时，两种数据库在插入时要对索引值进行处理，并查找数据库中是否存在相同的键值，这会减慢插入的速率。 在MongoDB中，指定索引插入比不指定慢很多，这是因为，MongoDB里每一条数据的_id值都是唯一的。当在不指定_id插入数据的时候，其_id是系统自动计算生成的。MongoDB通过计算机特征值、时间、进程ID与随机数来确保生成的_id是唯一的。而在指定_id插入时，MongoDB每插一条数据，都需要检查此_id可不可用，当数据库中数据条数太多的时候，这一步的查询开销会拖慢整个数据库的插入速度。 MongoDB会充分使用系统内存作为缓存，这是一种非常优秀的特性。我们的测试机的内存有64G，在插入时，MongoDB会尽可能地在内存快写不进去数据之后，再将数据持久化保存到硬盘上。这也是在不指定_id插入的时候，MongoDB的效率遥遥领先的原因。但在指定_id插入时，当数据量一大内存装不下时，MongoDB就需要将磁盘中的信息读取到内存中来查重，这样一来其插入效率反而慢了。 MySQL不愧是一种非常稳定的数据库，无论在指定主键还是在不指定主键插入的情况下，其效率都差不了太多。 7.2 插入稳定性分析 插入稳定性是指，随着数据量的增大，每插入一定量数据时的插入速率情况。 在本次测试中，我们把这个指标的规模定在10w，即显示的数据是在每插入10w条数据时，在这段时间内每秒钟能插入多少条数据。 先呈现四张图上来： MongoDB指定_id插入： MongoDB不指定_id插入： MySQL指定PRIMARY KEY插入： MySQL不指定PRIMARY KEY插入： 总结： 整体上的插入速度还是和上一回的统计数据类似：MongoDB不指定_id插入 &gt; MySQL不指定主键插入 &gt; MySQL指定主键插入 &gt; MongoDB指定_id插入。 从图中可以看出，在指定主键插入数据的时候，MySQL与MongoDB在不同数据数量级时，每秒插入的数据每隔一段时间就会有一个波动，在图表中显示成为规律的毛刺现象。而在不指定插入数据时，在大多数情况下插入速率都比较平均，但随着数据库中数据的增多，插入的效率在某一时段有瞬间下降，随即又会变稳定。 整体上来看，MongoDB的速率波动比MySQL的严重，方差变化较大。 MongoDB在指定_id插入时，当插入的数据变多之后，插入效率有明显地下降。在其他三种的插入测试中，从开始到结束，其插入的速率在大多数的时候都固定在一个标准上。 分析： 毛刺现象是因为，当插入的数据太多的时候，MongoDB需要将内存中的数据写进硬盘，MySQL需要重新分表。这些操作每当数据库中的数据达到一定量级后就会自动进行，因此每隔一段时间就会有一个明显的毛刺。 MongoDB毕竟还是新生事物，其稳定性没有已应用多年的MySQL优秀。 MongoDB在指定_id插入的时候，其性能的下降还是很厉害的。 7.3 MySQL与MongoDB读取性能的简单测试 这是一个附加的测试，也并没有测试得非常完整，但还是很能说明一些问题的。 测试方法： 先在1 – 100, 000, 000这一亿个数中，分别随机取1w, 5w, 10w, 20w, 50w个互不相同的数字，再计算其md5值，并保存。 至于为什么最高只选到50w这个规模，这是因为我在随机生成100w个互不相同的数字的时候，写的脚本跑了一晚上都没有跑出来，估计是我生成的算法写得太烂了。我不想重新再弄了，暂就以50w为上限吧。 在上述带主键插入的两个数据库里，分别以上一步生成的md5源为输入进行查询操作。同样，每查询1000条数据在日志文件中将当前系统时间写入。 测试结果： 以下三张图的横坐标是每查询1000条数据所需要的时间，单位为s；纵坐标是查询的规模，分为1w, 5w,10w, 20w, 50w五个等级。 这张图是详细对比，可以看出MySQL与MongoDB之间的差异了吗…… 总结： 在读取的数据规模不大时，MongoDB的查询速度真是一骑绝尘，甩开MySQL好远好远。 在查询的数据量逐渐增多的时候，MySQL的查询速度是稳步下降的，而MongoDB的查询速度却有些起伏。 分析： 如果MySQL没有经过查询优化的话，其查询速度就不要跟MongoDB比了。MongoDB可以充分利用系统的内存资源，我们的测试机器内存是64GB的，内存越大MongoDB的查询速度就越快，毕竟磁盘与内存的I/O效率不是一个量级的。 本次实验的查询的数据也是随机生成的，因此所有待查询的数据都存在MongoDB的内存缓存中的概率是很小的。在查询时，MongoDB需要多次将内存中的数据与磁盘进行交互以便查找，因此其查询速率取决于其交互的次数。这样就存在这样一种可能性，尽管待查询的数据数目较多，但这段随机生成的数据被MongoDB以较少的次数从磁盘中取出。因此，其查询的平均速度反而更快一些。这样看来，MongoDB的查询速度波动也处在一个合理的范围内。 MySQL的稳定性还是毋庸置疑的。 8. 测试总结 8.1 测试结论 相比较MySQL，MongoDB数据库更适合那些读作业较重的任务模型。MongoDB能充分利用机器的内存资源。如果机器的内存资源丰富的话，MongoDB的查询效率会快很多。 在带”_id”插入数据的时候，MongoDB的插入效率其实并不高。如果想充分利用MongoDB性能的话，推荐采取不带”_id”的插入方式，然后对相关字段作索引来查询。 8.2 测试需要进一步注意的问题 对MongoDB的读取测试考虑不周，虽然这只是一个额外的测试。在这个测试中，随机生成大量待测试的数据很有必要，但生成大量互不相同的数据就没有必要了。正是这一点，把我的读取测试规模限定在了50w条，没能进一步进行分析。 8.3 MongoDB的优势 MongoDB适合那些对数据库具体数据格式不明确或者数据库数据格式经常变化的需求模型，而且对开发者十分友好。 MongoDB官方就自带一个分布式文件系统，可以很方便地部署到服务器机群上。MongoDB里有一个Shard的概念，就是方便为了服务器分片使用的。每增加一台Shard，MongoDB的插入性能也会以接近倍数的方式增长，磁盘容量也很可以很方便地扩充。 MongoDB还自带了对map-reduce运算框架的支持，这也很方便进行数据的统计。 其他方面的优势还在发掘中，本人也是刚刚接触这个不久。 8.4 MongoDB的缺陷 事务关系支持薄弱。这也是所有NoSQL数据库共同的缺陷，不过NoSQL并不是为了事务关系而设计的，具体应用还是很需求。 稳定性有些欠缺，这点从上面的测试便可以看出。 MongoDB一方面在方便开发者的同时，另一方面对运维人员却提出了相当多的要求。业界并没有成熟的MongoDB运维经验，MongoDB中数据的存放格式也很随意，等等问题都对运维人员的考验。 ","link":"https://jobslee0.github.io/post/mongodb-yu-mysql-de-cha-ru-cha-xun-xing-neng-ce-shi/"},{"title":"MongoDB之ObjectId详解","content":"在MongoDB默认的document中，有一个叫做_id的字段，这个字段乍一看是一个随机的字符串，其实在MongoDB中却是含有特殊含义的且具有文档数据唯一标识意义的主键。在默认情况下，这个id可以拿来做很多有趣的事情，在某些情况下，我们甚至可以人工干预_id的生成。 ObjectId组成解析 _id在MongoDB中对应的其实是ObjectId（在MongoDB的Shell模式下，描述为ObjectId(&quot;HEX_STR&quot;)，在扩展JSON模式下，描述为$oid:&quot;HEX_STR&quot;）。其本质是一个12字节的数据串，分别由4字节时间戳+5字节随机值+3字节递增计数组成。 举一个具体例子： ObjectId(&quot;507f1f77bcf86cd799439011&quot;) 基础计算公式：12byte*8bit=96bit 96bit/4bit=24个十六进制字符 十六进制字符串：507f1f77 bcf86cd799 439011 前4字节（秒级别的unix时间戳）（4byte*8bit/4bit=8个十六进制字符）：507f1f77 转换为十进制的秒时间戳就是 1350508407 也就是时间戳 2012-10-18 05:13:27 中间5字节（生成的唯一的机器码和进程码）（5byte*8bit/4bit=10个十六进制字符）：bcf86cd799 后3字节（初始化随机的递增计数值）（3byte*8bit/4bit=6个十六进制字符）：439011 ObjectId核心作用 MongoDB 在创建集合期间在_id字段上创建唯一索引。_id索引可防止客户端插入两个具有相同_id字段值的文档。您不能在_id字段上删除此索引。 从官方文档可以看出，_id具有主键意义的存在，具有唯一性，会自动生成，同时自带唯一索引的特性，相对于普通索引，查询性能上较快。 ObjectId的妙用 由以上的解析和文档参考，我们完全可以使用_id进行时间范围的数据查询。在极端业务场景中，若某一时间字段不存在或者不具备索引查询的特性，且临时创建索引的代价巨大之时，巧妙使用_id自带时间戳的特性，可以有效实现按时间范围的数据查询，而且效率很高。具体的方法如下： 1.获取指定时间的时间戳（精确到秒）; 2.把10位的时间戳转成16进制字符，得到4个字节； 3.对4个字节（时间戳）后追加&quot;0000000000000000&quot;(8个字节)； 举个实际查询的例子： db.record.find({&quot;_id&quot;:{$gte:ObjectId(&quot;62ffa5700000000000000000&quot;),$lt:ObjectId(&quot;62ffb3800000000000000000&quot;)}}); 实现了按照_id查询2022-08-19 23:00:00-2022-08-20 00:00:00时间范围的数据。 自定义生成及注意事项 使用 ObjectId 如果可用，请使用自然唯一标识符。这节省了空间并避免了额外的索引 生成一个自动递增的数字 在您的应用程序代码中生成一个 UUID（为了在集合和 _id 索引中更有效地存储 UUID 值，请将 UUID 存储为 BSON BinData 类型的值） BinData 类型的索引键在以下情况下更有效地存储在索引中： 二进制子类型值在 0-7 或 128-135 的范围内，并且字节数组的长度为：0、1、2、3、4、5、6、7、8、10、12、14、16、20、24或32 使用驱动程序的 BSON UUID 工具生成 UUID（请注意，驱动程序实现可能会以不同的方式实现 UUID 序列化和反序列化逻辑，这可能与其他驱动程序不完全兼容） 在分片集群中，如果您不使用_id字段作为分片键，那么您的应用程序必须确保_id字段中的值的唯一性以防止错误。这通常通过使用标准的自动生成的ObjectId来完成。 参考文档： MongoDB Manual - ObjectId MongoDB Manual - default-_id-index MongoDB Manual - std-label-document-id-field CSDN博客 ","link":"https://jobslee0.github.io/post/mongodb-zhi-objectid-xiang-jie/"},{"title":"实现web实时消息推送的7种方案","content":" 原文链接：https://juejin.cn/post/7122014462181113887 什么是消息推送（push） 推送的场景比较多，比如有人关注我的公众号，这时我就会收到一条推送消息，以此来吸引我点击打开应用。 消息推送(push)通常是指网站的运营工作等人员，通过某种工具对用户当前网页或移动设备APP进行的主动消息推送。 消息推送一般又分为web端消息推送和移动端消息推送。 上边的这种属于移动端消息推送，web端消息推送常见的诸如站内信、未读邮件数量、监控报警数量等，应用的也非常广泛。 在具体实现之前，咱们再来分析一下前边的需求，其实功能很简单，只要触发某个事件（主动分享了资源或者后台主动推送消息），web页面的通知小红点就会实时的+1就可以了。 通常在服务端会有若干张消息推送表，用来记录用户触发不同事件所推送不同类型的消息，前端主动查询（拉）或者被动接收（推）用户所有未读的消息数。 消息推送无非是推（push）和拉（pull）两种形式，下边我们逐个了解下。 短轮询 轮询(polling)应该是实现消息推送方案中最简单的一种，这里我们暂且将轮询分为短轮询和长轮询。 短轮询很好理解，指定的时间间隔，由浏览器向服务器发出HTTP请求，服务器实时返回未读消息数据给客户端，浏览器再做渲染显示。 一个简单的JS定时器就可以搞定，每秒钟请求一次未读消息数接口，返回的数据展示即可。 setInterval(() =&gt; { // 方法请求 messageCount().then((res) =&gt; { if (res.code === 200) { this.messageCount = res.data } }) }, 1000); 复制代码 效果还是可以的，短轮询实现固然简单，缺点也是显而易见，由于推送数据并不会频繁变更，无论后端此时是否有新的消息产生，客户端都会进行请求，势必会对服务端造成很大压力，浪费带宽和服务器资源。 长轮询 长轮询是对上边短轮询的一种改进版本，在尽可能减少对服务器资源浪费的同时，保证消息的相对实时性。长轮询在中间件中应用的很广泛，比如Nacos和apollo配置中心，消息队列kafka、RocketMQ中都有用到长轮询。 Nacos配置中心交互模型是push还是pull？一文中我详细介绍过Nacos长轮询的实现原理，感兴趣的小伙伴可以瞅瞅。 这次我使用apollo配置中心实现长轮询的方式，应用了一个类DeferredResult，它是在servelet3.0后经过Spring封装提供的一种异步请求机制，直意就是延迟结果。 DeferredResult可以允许容器线程快速释放占用的资源，不阻塞请求线程，以此接受更多的请求提升系统的吞吐量，然后启动异步工作线程处理真正的业务逻辑，处理完成调用DeferredResult.setResult(200)提交响应结果。 下边我们用长轮询来实现消息推送。 因为一个ID可能会被多个长轮询请求监听，所以我采用了guava包提供的Multimap结构存放长轮询，一个key可以对应多个value。一旦监听到key发生变化，对应的所有长轮询都会响应。前端得到非请求超时的状态码，知晓数据变更，主动查询未读消息数接口，更新页面数据。 @Controller @RequestMapping(&quot;/polling&quot;) public class PollingController { // 存放监听某个Id的长轮询集合 // 线程同步结构 public static Multimap&lt;String, DeferredResult&lt;String&gt;&gt; watchRequests = Multimaps.synchronizedMultimap(HashMultimap.create()); /** * 公众号：程序员小富 * 设置监听 */ @GetMapping(path = &quot;watch/{id}&quot;) @ResponseBody public DeferredResult&lt;String&gt; watch(@PathVariable String id) { // 延迟对象设置超时时间 DeferredResult&lt;String&gt; deferredResult = new DeferredResult&lt;&gt;(TIME_OUT); // 异步请求完成时移除 key，防止内存溢出 deferredResult.onCompletion(() -&gt; { watchRequests.remove(id, deferredResult); }); // 注册长轮询请求 watchRequests.put(id, deferredResult); return deferredResult; } /** * 公众号：程序员小富 * 变更数据 */ @GetMapping(path = &quot;publish/{id}&quot;) @ResponseBody public String publish(@PathVariable String id) { // 数据变更 取出监听ID的所有长轮询请求，并一一响应处理 if (watchRequests.containsKey(id)) { Collection&lt;DeferredResult&lt;String&gt;&gt; deferredResults = watchRequests.get(id); for (DeferredResult&lt;String&gt; deferredResult : deferredResults) { deferredResult.setResult(&quot;我更新了&quot; + new Date()); } } return &quot;success&quot;; } 复制代码 当请求超过设置的超时时间，会抛出AsyncRequestTimeoutException异常，这里直接用@ControllerAdvice全局捕获统一返回即可，前端获取约定好的状态码后再次发起长轮询请求，如此往复调用。 @ControllerAdvice public class AsyncRequestTimeoutHandler { @ResponseStatus(HttpStatus.NOT_MODIFIED) @ResponseBody @ExceptionHandler(AsyncRequestTimeoutException.class) public String asyncRequestTimeoutHandler(AsyncRequestTimeoutException e) { System.out.println(&quot;异步请求超时&quot;); return &quot;304&quot;; } } 复制代码 我们来测试一下，首先页面发起长轮询请求/polling/watch/10086监听消息更变，请求被挂起，不变更数据直至超时，再次发起了长轮询请求；紧接着手动变更数据/polling/publish/10086，长轮询得到响应，前端处理业务逻辑完成后再次发起请求，如此循环往复。 长轮询相比于短轮询在性能上提升了很多，但依然会产生较多的请求，这是它的一点不完美的地方。 iframe流 iframe流就是在页面中插入一个隐藏的&lt;iframe&gt;标签，通过在src中请求消息数量API接口，由此在服务端和客户端之间创建一条长连接，服务端持续向iframe传输数据。 传输的数据通常是HTML、或是内嵌的javascript脚本，来达到实时更新页面的效果。 这种方式实现简单，前端只要一个&lt;iframe&gt;标签搞定了 &lt;iframe src=&quot;/iframe/message&quot; style=&quot;display:none&quot;&gt;&lt;/iframe&gt; 复制代码 服务端直接组装html、js脚本数据向response写入就行了 @Controller @RequestMapping(&quot;/iframe&quot;) public class IframeController { @GetMapping(path = &quot;message&quot;) public void message(HttpServletResponse response) throws IOException, InterruptedException { while (true) { response.setHeader(&quot;Pragma&quot;, &quot;no-cache&quot;); response.setDateHeader(&quot;Expires&quot;, 0); response.setHeader(&quot;Cache-Control&quot;, &quot;no-cache,no-store&quot;); response.setStatus(HttpServletResponse.SC_OK); response.getWriter().print(&quot; &lt;script type=\\&quot;text/javascript\\&quot;&gt;\\n&quot; + &quot;parent.document.getElementById('clock').innerHTML = \\&quot;&quot; + count.get() + &quot;\\&quot;;&quot; + &quot;parent.document.getElementById('count').innerHTML = \\&quot;&quot; + count.get() + &quot;\\&quot;;&quot; + &quot;&lt;/script&gt;&quot;); } } } 复制代码 但我个人不推荐，因为它在浏览器上会显示请求未加载完，图标会不停旋转，简直是强迫症杀手。 SSE (我的方式) 很多人可能不知道，服务端向客户端推送消息，其实除了可以用WebSocket这种耳熟能详的机制外，还有一种服务器发送事件(Server-sent events)，简称SSE。 SSE它是基于HTTP协议的，我们知道一般意义上的HTTP协议是无法做到服务端主动向客户端推送消息的，但SSE是个例外，它变换了一种思路。 SSE在服务器和客户端之间打开一个单向通道，服务端响应的不再是一次性的数据包而是text/event-stream类型的数据流信息，在有数据变更时从服务器流式传输到客户端。 整体的实现思路有点类似于在线视频播放，视频流会连续不断的推送到浏览器，你也可以理解成，客户端在完成一次用时很长（网络不畅）的下载。 SSE与WebSocket作用相似，都可以建立服务端与浏览器之间的通信，实现服务端向客户端推送消息，但还是有些许不同： SSE 是基于HTTP协议的，它们不需要特殊的协议或服务器实现即可工作；WebSocket需单独服务器来处理协议。 SSE 单向通信，只能由服务端向客户端单向通信；webSocket全双工通信，即通信的双方可以同时发送和接受信息。 SSE 实现简单开发成本低，无需引入其他组件；WebSocket传输数据需做二次解析，开发门槛高一些。 SSE 默认支持断线重连；WebSocket则需要自己实现。 SSE 只能传送文本消息，二进制数据需要经过编码后传送；WebSocket默认支持传送二进制数据。 SSE 与 WebSocket 该如何选择？ 技术并没有好坏之分，只有哪个更合适 SSE好像一直不被大家所熟知，一部分原因是出现了WebSockets，这个提供了更丰富的协议来执行双向、全双工通信。对于游戏、即时通信以及需要双向近乎实时更新的场景，拥有双向通道更具吸引力。 但是，在某些情况下，不需要从客户端发送数据。而你只需要一些服务器操作的更新。比如：站内信、未读消息数、状态更新、股票行情、监控数量等场景，SEE不管是从实现的难易和成本上都更加有优势。此外，SSE 具有WebSockets在设计上缺乏的多种功能，例如：自动重新连接、事件ID和发送任意事件的能力。 前端只需进行一次HTTP请求，带上唯一ID，打开事件流，监听服务端推送的事件就可以了 &lt;script&gt; let source = null; let userId = 7777 if (window.EventSource) { // 建立连接 source = new EventSource('http://localhost:7777/sse/sub/'+userId); setMessageInnerHTML(&quot;连接用户=&quot; + userId); /** * 连接一旦建立，就会触发open事件 * 另一种写法：source.onopen = function (event) {} */ source.addEventListener('open', function (e) { setMessageInnerHTML(&quot;建立连接。。。&quot;); }, false); /** * 客户端收到服务器发来的数据 * 另一种写法：source.onmessage = function (event) {} */ source.addEventListener('message', function (e) { setMessageInnerHTML(e.data); }); } else { setMessageInnerHTML(&quot;你的浏览器不支持SSE&quot;); } &lt;/script&gt; 复制代码 服务端的实现更简单，创建一个SseEmitter对象放入sseEmitterMap进行管理 private static Map&lt;String, SseEmitter&gt; sseEmitterMap = new ConcurrentHashMap&lt;&gt;(); /** * 创建连接 * * @date: 2022/7/12 14:51 * @auther: 公众号：程序员小富 */ public static SseEmitter connect(String userId) { try { // 设置超时时间，0表示不过期。默认30秒 SseEmitter sseEmitter = new SseEmitter(0L); // 注册回调 sseEmitter.onCompletion(completionCallBack(userId)); sseEmitter.onError(errorCallBack(userId)); sseEmitter.onTimeout(timeoutCallBack(userId)); sseEmitterMap.put(userId, sseEmitter); count.getAndIncrement(); return sseEmitter; } catch (Exception e) { log.info(&quot;创建新的sse连接异常，当前用户：{}&quot;, userId); } return null; } /** * 给指定用户发送消息 * * @date: 2022/7/12 14:51 * @auther: 公众号：程序员小富 */ public static void sendMessage(String userId, String message) { if (sseEmitterMap.containsKey(userId)) { try { sseEmitterMap.get(userId).send(message); } catch (IOException e) { log.error(&quot;用户[{}]推送异常:{}&quot;, userId, e.getMessage()); removeUser(userId); } } } 复制代码 我们模拟服务端推送消息，看下客户端收到了消息，和我们预期的效果一致。 注意： SSE不支持IE浏览器，对其他主流浏览器兼容性做的还不错。 MQTT 什么是 MQTT协议？ MQTT 全称(Message Queue Telemetry Transport)：一种基于发布/订阅（publish/subscribe）模式的轻量级通讯协议，通过订阅相应的主题来获取消息，是物联网（Internet of Thing）中的一个标准传输协议。 该协议将消息的发布者（publisher）与订阅者（subscriber）进行分离，因此可以在不可靠的网络环境中，为远程连接的设备提供可靠的消息服务，使用方式与传统的MQ有点类似。 TCP协议位于传输层，MQTT 协议位于应用层，MQTT 协议构建于TCP/IP协议上，也就是说只要支持TCP/IP协议栈的地方，都可以使用MQTT协议。 为什么要用 MQTT协议？ MQTT协议为什么在物联网（IOT）中如此受偏爱？而不是其它协议，比如我们更为熟悉的 HTTP协议呢？ 首先HTTP协议它是一种同步协议，客户端请求后需要等待服务器的响应。而在物联网（IOT）环境中，设备会很受制于环境的影响，比如带宽低、网络延迟高、网络通信不稳定等，显然异步消息协议更为适合IOT应用程序。 HTTP是单向的，如果要获取消息客户端必须发起连接，而在物联网（IOT）应用程序中，设备或传感器往往都是客户端，这意味着它们无法被动地接收来自网络的命令。 通常需要将一条命令或者消息，发送到网络上的所有设备上。HTTP要实现这样的功能不但很困难，而且成本极高。 具体的MQTT协议介绍和实践，这里我就不再赘述了，大家可以参考我之前的两篇文章，里边写的也都很详细了。 MQTT协议的介绍 我也没想到 springboot + rabbitmq 做智能家居，会这么简单 MQTT实现消息推送 未读消息（小红点），前端 与 RabbitMQ 实时消息推送实践，贼简单~ Websocket websocket应该是大家都比较熟悉的一种实现消息推送的方式，上边我们在讲SSE的时候也和websocket进行过比较。 WebSocket是一种在TCP连接上进行全双工通信的协议，建立客户端和服务器之间的通信渠道。浏览器和服务器仅需一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。 springboot整合websocket，先引入websocket相关的工具包，和SSE相比额外的开发成本。 &lt;!-- 引入websocket --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-websocket&lt;/artifactId&gt; &lt;/dependency&gt; 复制代码 服务端使用@ServerEndpoint注解标注当前类为一个websocket服务器，客户端可以通过ws://localhost:7777/webSocket/10086来连接到WebSocket服务器端。 @Component @Slf4j @ServerEndpoint(&quot;/websocket/{userId}&quot;) public class WebSocketServer { //与某个客户端的连接会话，需要通过它来给客户端发送数据 private Session session; private static final CopyOnWriteArraySet&lt;WebSocketServer&gt; webSockets = new CopyOnWriteArraySet&lt;&gt;(); // 用来存在线连接数 private static final Map&lt;String, Session&gt; sessionPool = new HashMap&lt;String, Session&gt;(); /** * 公众号：程序员小富 * 链接成功调用的方法 */ @OnOpen public void onOpen(Session session, @PathParam(value = &quot;userId&quot;) String userId) { try { this.session = session; webSockets.add(this); sessionPool.put(userId, session); log.info(&quot;websocket消息: 有新的连接，总数为:&quot; + webSockets.size()); } catch (Exception e) { } } /** * 公众号：程序员小富 * 收到客户端消息后调用的方法 */ @OnMessage public void onMessage(String message) { log.info(&quot;websocket消息: 收到客户端消息:&quot; + message); } /** * 公众号：程序员小富 * 此为单点消息 */ public void sendOneMessage(String userId, String message) { Session session = sessionPool.get(userId); if (session != null &amp;&amp; session.isOpen()) { try { log.info(&quot;websocket消: 单点消息:&quot; + message); session.getAsyncRemote().sendText(message); } catch (Exception e) { e.printStackTrace(); } } } } 复制代码 前端初始化打开WebSocket连接，并监听连接状态，接收服务端数据或向服务端发送数据。 &lt;script&gt; var ws = new WebSocket('ws://localhost:7777/webSocket/10086'); // 获取连接状态 console.log('ws连接状态：' + ws.readyState); //监听是否连接成功 ws.onopen = function () { console.log('ws连接状态：' + ws.readyState); //连接成功则发送一个数据 ws.send('test1'); } // 接听服务器发回的信息并处理展示 ws.onmessage = function (data) { console.log('接收到来自服务器的消息：'); console.log(data); //完成通信后关闭WebSocket连接 ws.close(); } // 监听连接关闭事件 ws.onclose = function () { // 监听整个过程中websocket的状态 console.log('ws连接状态：' + ws.readyState); } // 监听并处理error事件 ws.onerror = function (error) { console.log(error); } function sendMessage() { var content = $(&quot;#message&quot;).val(); $.ajax({ url: '/socket/publish?userId=10086&amp;message=' + content, type: 'GET', data: { &quot;id&quot;: &quot;7777&quot;, &quot;content&quot;: content }, success: function (data) { console.log(data) } }) } &lt;/script&gt; 复制代码 页面初始化建立websocket连接，之后就可以进行双向通信了，效果还不错 自定义推送 上边我们给我出了6种方案的原理和代码实现，但在实际业务开发过程中，不能盲目的直接拿过来用，还是要结合自身系统业务的特点和实际场景来选择合适的方案。 推送最直接的方式就是使用第三推送平台，毕竟钱能解决的需求都不是问题，无需复杂的开发运维，直接可以使用，省时、省力、省心，像goEasy、极光推送都是很不错的三方服务商。 一般大型公司都有自研的消息推送平台，像我们本次实现的web站内信只是平台上的一个触点而已，短信、邮件、微信公众号、小程序凡是可以触达到用户的渠道都可以接入进来。 消息推送系统内部是相当复杂的，诸如消息内容的维护审核、圈定推送人群、触达过滤拦截（推送的规则频次、时段、数量、黑白名单、关键词等等）、推送失败补偿非常多的模块，技术上涉及到大数据量、高并发的场景也很多。所以我们今天的实现方式在这个庞大的系统面前只是小打小闹。 ","link":"https://jobslee0.github.io/post/shi-xian-web-shi-shi-xiao-xi-tui-song-de-7-chong-fang-an/"},{"title":"关于时间国际化的一点整理总结","content":" 原文链接：https://juejin.cn/post/6844904134257147917 先说结论： 总原则：显示跟存储分离。 存储，以及后端服务之间交互时最好都使用时间戳； 前端显示时根据用户本地时区对时间戳转换。 后端定时任务： 如果涉及当地重要时间的，使用当地时区配置，否则可以默认使用北京时间，方便国内研发同学辨识。 后端服务之间交互: 使用时间戳； 或者时间格式 + 时区。 基于上述几个结论，下面看几个交互环节的建议。 1 MySQL中的时间存储 MySQL支持几种时间类型，datetime、timestamp，但是最好都使用bigint, 存储时间戳。 因为dateTime 跟TimeStamp 存在如下的问题： DateTime类型字段，MySQL存储时不存时区信息，并且怎么存就怎么取，不做任何处理和转换。所以时区timeZone1的server1插入MySQL一条记录后，时区timeZone2的server2读取出来的时间就不对了。这里只能将所有的server的时区设置为一样的，或者在数据库表中添加一个字段存储时区信息 TimeStamp类型字段，这个比较特殊。当server创建connection时，可以在数据库URL中手动指定时区信息，即不同时区的server连接MySQL时，指定connection时区使用自己所在时区。当MySQL处理不同的connection时，就有了时间字符串和发出请求的时区，然后转换为UTC时间进行存储。从MySQL中读取时也是基于connection的时区设置进行转换。但是如果不指定connection时区，那么MySQL就将存储的UTC时间，按MySQL服务器所在时区进行转换和展示或者传输，此时若MySQL服务器和server的时区不一致，就会出现时区问题。 2 服务器到客户端 一般这种情况下，服务器会把时间戳给到客户端，然后客户端根据用户本地时区进行时间显示。 如果需要在客户端强制显示某个时区的内容，也可以把时区一并传给客户端。 3 客户端到服务器 客户端给服务器时，建议直接转换为时间戳给到服务端。至于服务端怎么处理时间戳，由服务端自己决定。 4 服务A跟服务B交互 最好也是使用时间戳。如果使用时间，而服务A跟B的时区不一致，则就会产生问题。而使用时间戳就不会有这个问题。 如果非得传时间，则需要指定时区。 5 服务器到MySQL 同服务之间的传输，也是建议使用时间戳。 6 定时任务 如果涉及当地重要时间的，使用当地时区配置，比如印度某个特定的节日，需要针对节日做某些任务的定时导入； 如果不涉及当地重要时间，则可以默认使用北京时间，方便国内研发同学辨识。 7 多个时区转换 以UTC时间为基准，然后加减时区。本质：时区概念是上层人为转换的概念，程序的逻辑不要依赖于他，要有个统一的时刻值概念来衡量真实的时间（例如UNIX时间戳），然后在上层做转换。 ","link":"https://jobslee0.github.io/post/guan-yu-shi-jian-guo-ji-hua-de-yi-dian-zheng-li-zong-jie/"},{"title":"Groovy闭包","content":"闭包 本章节主要讲解Groovy的闭包。Groovy中的闭包是一个开放、匿名、可以携带参数的代码块，同时可以返回一个指定变量的值。一个闭包可以引用在它周围声明的变量。与闭包的正式定义相反的，Closure在Groovy语言中还可以包含在其周围范围之外定义的自由变量。虽然打破了闭包的正式概念，但它提供了本章中描述的各种优点。 1. 语法 1.1. 定义一个闭包 一个闭包的定义遵循以下语法： { [closureParameters -&gt; ] statements } [closureParameters-&gt;]是一个可选的逗号分隔的参数列表，语句是0个或者多个Groovy语句。这些参数看起来像方法的参数列表，这些参数类型可有可无。 当一个参数列表被指定，-&gt;符号是必须的，它能够在闭包体中提供分割参数的功能。语句部分由0个、1个或者许多个Groovy语句组成。 一些有效的闭包定义示例： { item++ } (1) { -&gt; item++ } (2) { println it } (3) { it -&gt; println it } (4) { name -&gt; println name } (5) { String x, int y -&gt; (6) println &quot;hey ${x} the value is ${y}&quot; }{ reader -&gt; (7) def line = reader.readLine() line.trim() } 1 引用名为item的变量的闭包 2 可以通过添加箭头 (-&gt;) 将闭包参数与代码显式分开 3 使用隐式参数 (it) 的闭包 4 it是显式参数的替代版本 5 在这种情况下，通常最好为参数使用显式名称 6 一个接受两个类型参数的闭包 7 一个闭包可以包含多个语句 1.2. 闭包是一个对象 闭包是groovy.lang.Closure类的一个实例，使它可以像任何其他变量一样分配给变量或字段，尽管它是一个代码块： def listener = { e -&gt; println &quot;Clicked on $e.source&quot; } (1) assert listener instanceof Closure Closure callback = { println 'Done!' } (2) Closure&lt;Boolean&gt; isTextFile = { File it -&gt; it.name.endsWith('.txt') (3) } 1 你可以将闭包分配给变量，它是groovy.lang.Closure的一个实例 2 如果不使用def或var，请使用groovy.lang.Closure作为类型 3 或者，你可以使用groovy.lang.Closure的泛型类型指定闭包的返回类型 1.3. 调用一个闭包 一个闭包，作为一个匿名的代码块，可以像其他任何方法一样被调用。如果你定义一个不带任何参数的闭包，像这样： def code = { 123 } 闭包中的代码只有当你调用的时候才会执行，这可以使用变量来实现，就像它是一个常规方法一样： assert code() == 123 或者，你可以显式地使用call方法： assert code.call() == 123 如果闭包接受参数，原则是一样的： def isOdd = { int i -&gt; i%2 != 0 } (1) assert isOdd(3) == true (2) assert isOdd.call(2) == false (3) def isEven = { it%2 == 0 } (4) assert isEven(3) == false (5) assert isEven.call(2) == true (6) 1 定义一个接受int作为参数的闭包 2 可以直接调用 3 或者使用call方法 4 带有隐式参数（it）的闭包也是如此 5 可以使用 (arg) 直接调用 6 或使用call 与方法不同，闭包在调用时总是返回一个值。下一节讨论如何声明闭包参数，何时使用它们以及什么是隐式“it”参数。 2. 参数 2.1. 正常参数 闭包的参数遵循与常规方法的参数相同的原则： 可选类型 一个名字 可选的默认值 参数用逗号分隔： def closureWithOneArg = { str -&gt; str.toUpperCase() } assert closureWithOneArg('groovy') == 'GROOVY'def closureWithOneArgAndExplicitType = { String str -&gt; str.toUpperCase() } assert closureWithOneArgAndExplicitType('groovy') == 'GROOVY'def closureWithTwoArgs = { a,b -&gt; a+b } assert closureWithTwoArgs(1,2) == 3def closureWithTwoArgsAndExplicitTypes = { int a, int b -&gt; a+b } assert closureWithTwoArgsAndExplicitTypes(1,2) == 3def closureWithTwoArgsAndOptionalTypes = { a, int b -&gt; a+b } assert closureWithTwoArgsAndOptionalTypes(1,2) == 3def closureWithTwoArgAndDefaultValue = { int a, int b=2 -&gt; a+b } assert closureWithTwoArgAndDefaultValue(1) == 3 2.2. 隐式参数 当一个闭包没有显示的声明参数列表（使用-&gt;），一个闭包总是定义了一个隐式的参数，叫做it。这意味着以下代码： def greeting = { &quot;Hello, $it!&quot; } assert greeting('Patrick') == 'Hello, Patrick!' 严格等同于这个： def greeting = { it -&gt; &quot;Hello, $it!&quot; } assert greeting('Patrick') == 'Hello, Patrick!' 如果你想声明一个不接受参数并且必须限制为不带参数调用的闭包，那么你必须用一个显式的空参数列表声明它： def magicNumber = { -&gt; 42 }// this call will fail because the closure doesn't accept any argument magicNumber(11) 2.3. Varargs 闭包可以像任何其他方法一样声明变量参数。如果最后一个参数是可变长度（或数组），则Vargs方法是可以接受可变数量参数的方法，如下例所示： def concat1 = { String... args -&gt; args.join('') } (1) assert concat1('abc','def') == 'abcdef' (2) def concat2 = { String[] args -&gt; args.join('') } (3) assert concat2('abc', 'def') == 'abcdef'def multiConcat = { int n, String... args -&gt; (4) args.join('')*n } assert multiConcat(2, 'abc','def') == 'abcdefabcdef' 1 接受可变数量的字符串作为第一个参数的闭包 2 可以使用任意数量的参数调用它，而无需将它们显式包装到数组中 3 如果将args参数声明为数组，则可以直接使用相同的行为 4 只要最后一个参数是数组或显式 vargs 类型 3. 委托策略 3.1. Groovy闭包 vs lambda表达式 Groovy将闭包定义为Closure类的实例。这使得它跟lambda expressions in Java 8非常的不一样。委托在Groovy闭包中是一个跟lambda不相同的重要概念。更改委托或更改闭包的委托策略的能力使得在 Groovy 中设计漂亮的领域特定语言 (DSL) 成为可能。 3.2. Owner, delegate和this 为了理解delegate的概念，我们首先必须解释下闭包中this的含义。一个闭包实际上定义了3个不同的东西： this对应于定义闭包的封闭类 owner对应于定义闭包的封闭对象，可以是类也可以是闭包 delegate对应于第三方对象，只要未定义消息的接收者，就会解析方法调用或属性 3.2.1. this的含义 在闭包中，调用getThisObject将会返回闭包定义的封闭类。它等效于使用显式this： class Enclosing { void run() { def whatIsThisObject = { getThisObject() } (1) assert whatIsThisObject() == this (2) def whatIsThis = { this } (3) assert whatIsThis() == this (4) } } class EnclosedInInnerClass { class Inner { Closure cl = { this } (5) } void run() { def inner = new Inner() assert inner.cl() == inner (6) } } class NestedClosures { void run() { def nestedClosures = { def cl = { this } (7) cl() } assert nestedClosures() == this (8) } } 1 在Enclosure类中定义了一个闭包，并返回getThisObject 2 调用闭包将返回定义闭包的Enclosure实例 3 通常，你只想使用this表示法的快捷方式 4 它返回完全相同的对象 5 如果闭包是在内部类中定义的 6 闭包中的this将返回内部类，而不是顶级类 7 在嵌套闭包的情况下，就像这里cl被定义在NestedClosures的范围内 8 那么this对应于最近的外部类，而不是封闭的闭包！ 当然可以这样调用闭包类中的方法： class Person { String name int age String toString() { &quot;$name is $age years old&quot; } String dump() { def cl = { String msg = this.toString() (1) println msg msg } cl() } } def p = new Person(name:'Janice', age:74) assert p.dump() == 'Janice is 74 years old' 1 闭包在this上调用toString，这实际上会在封闭对象上调用toString方法，即Person实例 3.2.2. 闭包的所有者Owner 闭包的owner与闭包中this的定义非常相似，但有细微差别：它将返回直接封闭的对象，无论是闭包还是类： class Enclosing { void run() { def whatIsOwnerMethod = { getOwner() } (1) assert whatIsOwnerMethod() == this (2) def whatIsOwner = { owner } (3) assert whatIsOwner() == this (4) } } class EnclosedInInnerClass { class Inner { Closure cl = { owner } (5) } void run() { def inner = new Inner() assert inner.cl() == inner (6) } } class NestedClosures { void run() { def nestedClosures = { def cl = { owner } (7) cl() } assert nestedClosures() == nestedClosures (8) } } 1 在Enclosure类中定义了一个闭包，并返回getOwner 2 调用闭包将返回定义闭包的Enclosure实例 3 通常，你只想使用快捷方式owener表示法 4 它返回完全相同的对象 5 如果闭包是在内部类中定义的 6 闭包中的owener将返回内部类，而不是顶级类 7 但是在nestedClosures的情况下，就像这里cl被定义在嵌套闭包的范围内 8 那么owner对应于封闭的闭包，因此与this不同的对象！ 3.2.3. 闭包的委托Delegate 可以使用委托属性或调用getDelegate方法来访问闭包的委托。它是在 Groovy 中构建特定领域语言的强大概念。虽然this和owner指的是闭包的词法范围，但委托是闭包将使用的用户定义对象。默认情况下，委托设置为owener： class Enclosing { void run() { def cl = { getDelegate() } (1) def cl2 = { delegate } (2) assert cl() == cl2() (3) assert cl() == this (4) def enclosed = { { -&gt; delegate }.call() (5) } assert enclosed() == enclosed (6) } } 1 你可以通过调用getDelegate方法获取闭包的委托 2 或使用delegate属性 3 两者都返回相同的对象 4 这是封闭类或闭包 5 特别是在嵌套闭包的情况下 6 delegate将对应于owner 闭包的委托可以更改为任何对象。让我们通过创建两个类来说明这一点，它们不是彼此的子类，但都定义了一个名为name的属性： class Person { String name } class Thing { String name }def p = new Person(name: 'Norman') def t = new Thing(name: 'Teapot') 然后让我们定义一个闭包来获取委托上的name属性： def upperCasedName = { delegate.name.toUpperCase() } 然后通过更改闭包的委托，可以看到目标对象会发生变化： upperCasedName.delegate = p assert upperCasedName() == 'NORMAN' upperCasedName.delegate = t assert upperCasedName() == 'TEAPOT' 此时，行为与在闭包的词法范围中定义target变量没有什么不同： def target = p def upperCasedNameUsingVar = { target.name.toUpperCase() } assert upperCasedNameUsingVar() == 'NORMAN' 但是，有主要区别： 在最后一个示例中，target是从闭包内引用的局部变量 委托可以透明地使用，也就是说，无需在方法调用前加上delegate。如下一段所述。 3.2.4. 委托策略 每当在闭包中访问一个属性而不显式设置接收器对象时，就会涉及到委托策略： class Person { String name } def p = new Person(name:'Igor') def cl = { name.toUpperCase() } (1) cl.delegate = p (2) assert cl() == 'IGOR' (3) 1 name没有引用闭包词法范围内的变量 2 我们可以将闭包的委托更改为Person的实例 3 并且方法调用会成功 此代码有效的原因是name属性将在delegate对象上透明地解析！这是解决闭包内的属性或方法调用的一种非常强大的方法。无需设置显式delegate。接收者：将进行调用，因为闭包的默认委托策略使其如此。闭包实际上定义了多种解决策略，你可以选择： Closure.OWNER_FIRST是默认策略。如果owner上存在属性/方法，那么它将在owner上调用。如果不是，则使用delegate。 Closure.DELEGATE_FIRST反转逻辑：先使用delegate，然后使用owner Closure.OWNER_ONLY只会解析所有者的属性/方法查找：委托将被忽略。 Closure.DELEGATE_ONLY只会解析委托上的属性/方法查找：所有者将被忽略。 Closure.TO_SELF可供需要高级元编程技术并希望实现自定义解析策略的开发人员使用：解析不会针对所有者或委托，而只会针对闭包类本身。如果你实现自己的Closure子类，则使用它才有意义。 让我们用这段代码来说明默认的“所有者优先”策略： class Person { String name def pretty = { &quot;My name is $name&quot; } (1) String toString() { pretty() } } class Thing { String name (2) }def p = new Person(name: 'Sarah') def t = new Thing(name: 'Teapot')assert p.toString() == 'My name is Sarah' (3) p.pretty.delegate = t (4) assert p.toString() == 'My name is Sarah' (5) 1 为了说明，我们定义了一个引用“name”的闭包成员 2 Person和Thing类都定义了一个name属性 3 使用默认策略，首先在所有者上解析name属性 4 因此，如果我们将delegate更改为t，它是Thing的一个实例 5 结果没有变化：name首先在闭包的owner上解析 但是，可以更改闭包的解析策略： p.pretty.resolveStrategy = Closure.DELEGATE_FIRST assert p.toString() == 'My name is Teapot' 通过更改resolveStrategy，我们正在修改Groovy解析“隐式 this”引用的方式：在这种情况下，name将首先在委托中查找，如果未找到，则在所有者中查找。由于name是在委托中定义的，它是Thing的一个实例，因此使用此值。 如果其中一个委托人（或所有者）没有这样的方法或属性，则可以说明“委托优先”和“仅委托”或“所有者优先”和“仅所有者”之间的区别： class Person { String name int age def fetchAge = { age } } class Thing { String name }def p = new Person(name:'Jessica', age:42) def t = new Thing(name:'Printer') def cl = p.fetchAge cl.delegate = p assert cl() == 42 cl.delegate = t assert cl() == 42 cl.resolveStrategy = Closure.DELEGATE_ONLY cl.delegate = p assert cl() == 42 cl.delegate = t try { cl() assert false } catch (MissingPropertyException ex) { // &quot;age&quot; is not defined on the delegate } 在这个例子中，我们定义了两个类，它们都有一个name属性，但只有Person类声明了一个age。Person类还声明了一个引用age的闭包。我们可以将默认的解决策略从“所有者优先”更改为“仅代表”。由于闭包的所有者是Person类，那么我们可以检查，如果委托是Person的实例，则调用闭包是成功的，但是如果我们以委托作为Thing的实例来调用它，它会失败并出现groovy .lang.MissingPropertyException。尽管在Person类中定义了闭包，但没有使用所有者。 4. GStrings中的闭包 采取以下代码： def x = 1 def gs = &quot;x = ${x}&quot; assert gs == 'x = 1' 代码的行为与你预期的一样，但是如果你添加以下内容会发生什么： x = 2 assert gs == 'x = 2' 你会看到断言失败了！有两个原因： GString仅懒惰地执行值的toString表示 GString 中的语法${x}不代表闭包，而是$x的表达式，在创建GString时进行执行。 在我们的示例中，GString是使用引用x的表达式创建的。创建GString时，x的值为 1，因此创建的GString的值为 1。触发断言时，执行GString并使用toString将 1 转换为String。当我们将x更改为 2 时，我们确实更改了x的值，但它是一个不同的对象，并且GString仍然引用旧的对象。 只有当GString引用的值发生变化时，GString才会更改其toString表示。如果引用发生变化，什么都不会发生。 如果你需要 GString 中的真正闭包，例如强制对变量进行惰性求值，则需要使用替代语法${→ x}，如固定示例中所示： def x = 1 def gs = &quot;x = ${-&gt; x}&quot; assert gs == 'x = 1' x = 2 assert gs == 'x = 2' 让我们用这段代码来说明它与变体有何不同： class Person { String name String toString() { name } (1) } def sam = new Person(name:'Sam') (2) def lucy = new Person(name:'Lucy') (3) def p = sam (4) def gs = &quot;Name: ${p}&quot; (5) assert gs == 'Name: Sam' (6) p = lucy (7) assert gs == 'Name: Sam' (8) sam.name = 'Lucy' (9) assert gs == 'Name: Lucy' (10) 1 Person类有一个toString方法返回name属性 2 我们创建了一个名为Sam的Person 3 我们创建另一个名为 Lucy 的Person 4 p变量设置为Sam 5 并创建一个闭包，引用p的值，也就是说Sam 6 所以当我们评估字符串时，它会返回Sam 7 如果我们将p更改为 Lucy 8 该字符串仍然计算为Sam，因为它是创建GString时p的值 9 所以如果我们把name的值Sam改成Lucy 10 这次GString被正确地改变了 因此，如果你不想依赖变异对象或包装对象，则必须通过显式声明一个空参数列表来在GString中使用闭包： class Person { String name String toString() { name } } def sam = new Person(name:'Sam') def lucy = new Person(name:'Lucy') def p = sam // Create a GString with lazy evaluation of &quot;p&quot; def gs = &quot;Name: ${-&gt; p}&quot; assert gs == 'Name: Sam' p = lucy assert gs == 'Name: Lucy' 5. 强制闭包 闭包可以转换为接口或单抽象方法类型。有关完整说明，请参阅手册的这一部分。 6. 函数式编程 闭包，如Java 8中的lambda表达式，是Groovy中函数式编程范式的核心。函数的一些函数式编程操作可以直接在Closure类上使用，如本节所示。 6.1. 柯里化 在Groovy中，柯里化是指部分应用的概念。由于Groovy在闭包上应用的不同作用域规则，它不符合函数式编程中柯里化的真正概念。 Groovy中的柯里化将允许你设置闭包的一个参数的值，并且它将返回一个接受少一个参数的新闭包。 6.1.1. 左柯里化 左柯里化是设置闭包最左边的参数，如下例所示： def nCopies = { int n, String str -&gt; str*n } (1) def twice = nCopies.curry(2) (2) assert twice('bla') == 'blabla' (3) assert twice('bla') == nCopies(2, 'bla') (4) 1 nCopies闭包定义了两个参数 2 curry将第一个参数设置为 2 ，创建一个接受单个String的新闭包（函数） 3 所以只用一个String调用新的函数调用 4 并且相当于用两个参数调用nCopies 6.1.2. 右柯里化 与左柯里化类似，可以设置闭包的最右侧参数： def nCopies = { int n, String str -&gt; str*n } (1) def blah = nCopies.rcurry('bla') (2) assert blah(2) == 'blabla' (3) assert blah(2) == nCopies(2, 'bla') (4) 1 nCopies闭包定义了两个参数 2 rcurry会将最后一个参数设置为bla，创建一个接受单个int的新闭包（函数） 3 所以只用一个int调用新的函数调用 4 并且相当于用两个参数调用nCopies 6.1.3. 基于索引的柯里化 如果闭包接受超过 2 个参数，则可以使用ncurry设置任意参数： def volume = { double l, double w, double h -&gt; l*w*h } (1) def fixedWidthVolume = volume.ncurry(1, 2d) (2) assert volume(3d, 2d, 4d) == fixedWidthVolume(3d, 4d) (3) def fixedWidthAndHeight = volume.ncurry(1, 2d, 4d) (4) assert volume(3d, 2d, 4d) == fixedWidthAndHeight(3d) (5) 1 volume函数定义了3个参数 2 ncurry将第二个参数 (index = 1) 设置为2d，创建一个接受长度和高度的新体积函数 3 该功能相当于调用volume省略宽度 4 也可以设置多个参数，从指定的索引开始 5 结果函数接受与初始参数一样多的参数减去ncurry设置的参数数量 6.2. 记忆 记忆化允许缓存调用闭包的结果。如果一个函数（闭包）完成的计算很慢，这很有趣，但你知道这个函数会经常用相同的参数调用。一个典型的例子是斐波那契套件。一个简单的实现可能如下所示： def fib fib = { long n -&gt; n&lt;2?n:fib(n-1)+fib(n-2) } assert fib(15) == 610 // slow! 这是一个幼稚的实现，因为“fib”通常使用相同的参数递归调用，从而导致指数算法： 计算fib(15)需要fib(14)和fib(13)的结果 计算fib(14)需要fib(13)和fib(12)的结果 由于调用是递归的，你已经可以看到我们将一次又一次地计算相同的值，尽管它们可以被缓存。这个幼稚的实现可以通过使用memoize缓存调用结果来“修复”： fib = { long n -&gt; n&lt;2?n:fib(n-1)+fib(n-2) }.memoize() assert fib(25) == 75025 // fast! 缓存使用参数的实际值工作。这意味着，如果你将memoization与原始类型或盒装原始类型以外的其他内容一起使用，你应该非常小心。 可以使用其他方法调整缓存的行为： memoizeAtMost将生成一个新的闭包，最多缓存n个值 memoizeAtLeast将生成一个缓存至少n个值的新闭包 memoizeBetween将生成一个新的闭包，它缓存至少n个值和最多n个值 所有memoize变体中使用的缓存都是LRU缓存。 6.3. 组合 闭包组合对应于函数组合的概念，即通过组合两个或多个函数（链式调用）来创建一个新函数，如下例所示： def plus2 = { it + 2 } def times3 = { it * 3 }def times3plus2 = plus2 &lt;&lt; times3 assert times3plus2(3) == 11 assert times3plus2(4) == plus2(times3(4))def plus2times3 = times3 &lt;&lt; plus2 assert plus2times3(3) == 15 assert plus2times3(5) == times3(plus2(5))// reverse composition assert times3plus2(3) == (times3 &gt;&gt; plus2)(3) 6.4. 蹦床Trampoline 递归算法通常受到物理限制的限制：最大堆栈高度。例如，如果你调用一个递归调用自身太深的方法，你最终会收到一个StackOverflowException。 在这些情况下有帮助的一种方法是使用Closure及其trampoline功能。 闭包被包裹在TrampolineClosure中。调用时，蹦床Closure将调用原始Closure等待其结果。如果调用的结果是TrampolineClosure的另一个实例，可能是作为调用trampoline()方法的结果而创建的，则将再次调用Closure。返回的蹦床闭包实例的这种重复调用将继续，直到返回蹦床Closure以外的值。该值将成为蹦床的最终结果。这样，调用是连续进行的，而不是填充堆栈。 下面是使用trampoline()实现阶乘函数的示例： def factorial factorial = { int n, def accu = 1G -&gt; if (n &lt; 2) return accu factorial.trampoline(n - 1, n * accu) } factorial = factorial.trampoline() assert factorial(1) == 1 assert factorial(3) == 1 * 2 * 3 assert factorial(1000) // == 402387260.. plus another 2560 digits 6.5. 方法指针 能够使用常规方法作为闭包通常很实用。例如，你可能想要使用闭包的柯里化功能，但这些功能不适用于普通方法。在 Groovy 中，你可以使用方法指针运算符从任何方法中获取闭包。 ","link":"https://jobslee0.github.io/post/groovy-bi-bao/"},{"title":"Groovy应用集成","content":"Groovy 语言提出了几种在运行时将自身集成到应用程序（Java 甚至 Groovy）中的方法，从最基本的简单代码执行到最完整的集成缓存和编译器定制。 本节中编写的所有示例都使用 Groovy，但可以在 Java 中使用相同的集成机制。 1.1. Eval 这个groovy.util.Eval类是最简单的方式在运行时动态的执行Groovy。可以使用me方法： import groovy.util.Evalassert Eval.me('33*3') == 99 assert Eval.me('&quot;foo&quot;.toUpperCase()') == 'FOO' Eval支持多种参数接收变体来支持简易的表达式： assert Eval.x(4, '2*x') == 8 (1) assert Eval.me('k', 4, '2*k') == 8 (2) assert Eval.xy(4, 5, 'x*y') == 20 (3) assert Eval.xyz(4, 5, 6, 'x*y+z') == 26 (4) 1 一个绑定了名称为x参数的简易表达式 2 跟上面的一样，一个绑定了名称为k参数的简易表达式 3 一个绑定了x和y两个绑定参数的简易表达式 4 一个绑定了x、y和z三个绑定参数的简易表达式 Eval类使得执行简易的脚本变得容易，但是对于大规模的脚本不行：没有脚本缓存，而且不能执行超过一行的表达式。 1.2. GroovyShell 1.2.1. 多种来源 groovy.lang.GroovyShell类是执行脚本的首选方式，能够缓存生成的脚本实例。尽管Eval类能够返回编译后脚本的执行结果，GroovyShell可以提供更多的选择。 def shell = new GroovyShell() (1) def result = shell.evaluate '3*5' (2) def result2 = shell.evaluate(new StringReader('3*5')) (3) assert result == result2 def script = shell.parse '3*5' (4) assert script instanceof groovy.lang.Script assert script.run() == 15 (5) 1 创建一个新的GroovyShell实例 2 可以作为Eval直接执行代码 3 能够从很多源读取（ String , Reader , File , InputStream ） 4 可以延迟脚本的执行。使用parse方法返回一个Script实例 5 Script定义了一个run方法 1.2.2. 在脚本和应用间共享数据 通过使用groovy.lang.Binding去在脚本和应用间共享数据： def sharedData = new Binding() (1) def shell = new GroovyShell(sharedData) (2) def now = new Date() sharedData.setProperty('text', 'I am shared data!') (3) sharedData.setProperty('date', now) (4) String result = shell.evaluate('&quot;At $date, $text&quot;') (5) assert result == &quot;At $now, I am shared data!&quot; 1 创建一个包含共享数据的新Binding 2 使用共享数据创建一个GroovyShell 3 添加字符串到绑定中 4 添加一个日期到绑定中（你可以不必限制于简单的类型） 5 执行脚本 注意，也有可以在脚本中写入绑定： def sharedData = new Binding() (1) def shell = new GroovyShell(sharedData) (2) shell.evaluate('foo=123') (3) assert sharedData.getProperty('foo') == 123 (4) 1 创建一个Binding实例 2 使用共享数据创建一个新的GroovyShell 3 在绑定中使用一个未声明的变量去存储结果 4 在回调中读取结果 如果你想使用绑定，一个未声明变量是很重要的。像下面例子一样使用def或者explicit类型将会失败，因为你将会创建一个本地变量： def sharedData = new Binding() def shell = new GroovyShell(sharedData) shell.evaluate('int foo=123')try { assert sharedData.getProperty('foo') } catch (MissingPropertyException e) { println &quot;foo is defined as a local variable&quot; } 你在多线程中使用共享变量时必须非常小心。传递给GroovyShell的Binding实例不是线程安全的，会被所有的脚本共享。 可以通过利用parse返回的Script实例来解决Binding的共享实例： def shell = new GroovyShell()def b1 = new Binding(x:3) (1) def b2 = new Binding(x:4) (2) def script = shell.parse('x = 2*x') script.binding = b1 script.run() script.binding = b2 script.run() assert b1.getProperty('x') == 6 assert b2.getProperty('x') == 8 assert b1 != b2 1 将x变量存储在b1中 2 将x变量存储在b2中 但是，你必须知道你仍在共享同一脚本实例。因此，如果你有两个线程处理同一个脚本，则无法使用此技术。在这种情况下，你必须确保创建两个不同的脚本实例： def shell = new GroovyShell()def b1 = new Binding(x:3) def b2 = new Binding(x:4) def script1 = shell.parse('x = 2*x') (1) def script2 = shell.parse('x = 2*x') (2) assert script1 != script2 script1.binding = b1 (3) script2.binding = b2 (4) def t1 = Thread.start { script1.run() } (5) def t2 = Thread.start { script2.run() } (6) [t1,t2]*.join() (7) assert b1.getProperty('x') == 6 assert b2.getProperty('x') == 8 assert b1 != b2 1 为线程1创建一个实例 2 为线程2创建一个实例 3 将第一个绑定分配给脚本1 4 将第一个绑定分配给脚本2 5 在单独的线程中启动第一个脚本 6 在单独的线程中启动第二个脚本 7 等待完成 如果你需要像这里这样的线程安全，建议直接使用 GroovyClassLoader。 1.2.3. 自定义脚本类 我们可以看到parse方法返回一个groovy.lang.Script的实例，但是有可能需要去使用一个自定义的类去扩展Script本身。它可用于为脚本提供额外的行为，如下例所示： abstract class MyScript extends Script { String name String greet() { &quot;Hello, $name!&quot; } } 这个自定义类定义了一个叫做name的参数和一个叫做greet的方法。这个类可以通过一个自定义的配置来被用作脚本的基础类： import org.codehaus.groovy.control.CompilerConfigurationdef config = new CompilerConfiguration() (1) config.scriptBaseClass = 'MyScript' (2) def shell = new GroovyShell(this.class.classLoader, new Binding(), config) (3) def script = shell.parse('greet()') (4) assert script instanceof MyScript script.setName('Michel') assert script.run() == 'Hello, Michel!' 1 创建一个CompilerConfiguration实例 2 指定MyScript作为脚本的基础类 3 然后在创建 shell 时使用编译器配置 4 该脚本现在可以访问新方法greet 你不仅局限于唯一的scriptBaseClass配置。你能够调整任意的编译器配置，包括compilation customizers。 1.3. GroovyClassLoader 在上一节，我们已经展示了GroovyShell是一个执行脚本的简单工具，但是除了脚本之外，编译任何东西都变得很复杂。在内部，它使用groovy.lang.GroovyClassLoader ，这是运行时编译和加载类的核心。 通过使用GroovyClassLoader替代GroovyShell，你将能够加载类，而不是脚本实例： import groovy.lang.GroovyClassLoaderdef gcl = new GroovyClassLoader() (1) def clazz = gcl.parseClass('class Foo { void doIt() { println &quot;ok&quot; } }') (2) assert clazz.name == 'Foo' (3) def o = clazz.newInstance() (4) o.doIt() (5) 1 创建一个新的GroovyClassLoader 2 parseClass将会返回Class的实例 3 你可以检查返回的类是否真的是脚本中定义的类 4 你可以创建一个新的类实例，它不是脚本 5 然后调用它的任何方法 GroovyClassLoader保留了它创建的所有类的引用，因此很容易造成内存泄漏。特别是，如果你执行两次相同的脚本，如果它是一个字符串，那么你将获得两个不同的类！ import groovy.lang.GroovyClassLoaderdef gcl = new GroovyClassLoader() def clazz1 = gcl.parseClass('class Foo { }') (1) def clazz2 = gcl.parseClass('class Foo { }') (2) assert clazz1.name == 'Foo' (3) assert clazz2.name == 'Foo' assert clazz1 != clazz2 (4) 1 动态创建一个名为“Foo”的类 2 使用单独的parseClass调用创建一个外观相同的类 3 确保两个类具有相同的名称 4 但它们实际上是不同的！ 原因是GroovyClassLoader不跟踪源文本。如果你想拥有相同的实例，则源必须是一个文件，如下例所示： def gcl = new GroovyClassLoader() def clazz1 = gcl.parseClass(file) (1) def clazz2 = gcl.parseClass(new File(file.absolutePath)) (2) assert clazz1.name == 'Foo' (3) assert clazz2.name == 'Foo' assert clazz1 == clazz2 (4) 1 从文件中解析一个类 2 从不同的文件实例解析一个类，但指向同一个物理文件 3 确保我们的类具有相同的名称 4 但现在，它们是同一个实例 使用一个File作为输入，GroovyClassLoader是有能力缓存生成的类文件的，这样能够避免在运行时创建多个同源类。 1.4. GroovyScriptEngine groovy.util.GroovyScriptEngine类为有脚本依赖的应用程序提供了灵活的脚本重载基础。虽然GroovyShell专注于独立脚本，而GroovyClassLoader处理任何 Groovy 类的动态编译和加载，但是GroovyScriptEngine将在GroovyClassLoader之上添加一个层来处理脚本依赖关系和重新加载。 为了说明这一点，我们将创建一个脚本引擎并在无限循环中执行代码。首先，你需要创建一个目录，其中包含以下脚本： ReloadingTest.groovy class Greeter { String sayHello() { def greet = &quot;Hello, world!&quot; greet } }new Greeter() 然后你可以使用GroovyScriptEngine执行此代码： def binding = new Binding() def engine = new GroovyScriptEngine([tmpDir.toURI().toURL()] as URL[]) (1)while (true) { def greeter = engine.run('ReloadingTest.groovy', binding) (2) println greeter.sayHello() (3) Thread.sleep(1000) } 1 创建一个脚本引擎，它将在我们的源目录中查找源 2 执行脚本，它将返回一个Greeter的实例 3 打印问候语 此时，你应该会看到每秒打印一条消息： Hello, world!Hello, world!... 在不中断脚本执行的情况下，现在将ReloadingTest文件的内容替换为： ReloadingTest.groovy class Greeter { String sayHello() { def greet = &quot;Hello, Groovy!&quot; greet } }new Greeter() 消息应更改为： Hello, world!...Hello, Groovy!Hello, Groovy!... 但也可能依赖于另一个脚本。为了说明这一点，在同一目录中创建以下文件，而不中断正在执行的脚本： Dependency.groovy class Dependency { String message = 'Hello, dependency 1' } 并像这样更新ReloadingTest脚本： ReloadingTest.groovy import Dependencyclass Greeter { String sayHello() { def greet = new Dependency().message greet } }new Greeter() 这一次，消息应该变为： Hello, Groovy!...Hello, dependency 1!Hello, dependency 1!... 作为最后一个测试，你可以更新Dependency.groovy文件，而无需触及ReloadingTest文件： Dependency.groovy class Dependency { String message = 'Hello, dependency 2' } 你应该观察到依赖文件已重新加载： Hello, dependency 1!...Hello, dependency 2!Hello, dependency 2! 1.5. CompilationUnit 最终，通过直接依赖org.codehaus.groovy.control.CompilationUnit类，可以在编译期间执行更多操作。该类负责确定编译的各个步骤，并允许你引入新步骤，甚至在各个阶段停止编译。例如，对于联合编译器，存根生成是如何完成的。 但是，不建议覆盖CompilationUnit，只有在没有其他标准解决方案有效的情况下才应该这样做。 ","link":"https://jobslee0.github.io/post/groovy-ying-yong-ji-cheng/"},{"title":"Groovy编程风格指南","content":"一个踏上了Groovy冒险之旅的Java开发者，会先基于烂熟于心的Java，然后再逐步的学习Groovy，一次一个特性的，最后将能够有效、习惯的使用Groovy编写代码。 这个文档旨在给在这条路上的开发者一个指导，传授一些常用的Groovy语法格式、新的操作和新的特性，譬如闭包等。这个指导文档不是完善的，仅仅作为一个进阶的快速介绍和基础指南，你可以对文档进行贡献和改进。 1. 无分号 当你是C / C++ / C# / Java背景的程序员时，已经习惯了处处使用分号。糟糕的是，Groovy支持了99%的Java语法，有时候复制Java代码到Groovy程序是十分容易的，这样就导致了分号大量的出现。可是，分号在Groovy里是可选的，你可以忽略它们，甚至于习惯移除它们。 2. 可选的Return关键字 在Groovy中，方法体最后一行的表达式能够被返回而无需return关键字。尤其是在闭包的短方法中，忽略关键字使它看起来更优雅： String toString() { return &quot;a server&quot; } String toString() { &quot;a server&quot; } 但是当你使用变量时，它看起来不太友好，仿佛在两行里出现了两次： def props() { def m1 = [a: 1, b: 2] m2 = m1.findAll { k, v -&gt; v % 2 == 0 } m2.c = 3 m2 } 通过这个例子，要么在最后一个表达式前换行，要么使用return关键字来提升可读性。 我自己有时候用return关键字，有时候不用，这是个人喜好问题。但是举例来讲，在闭包内部，我通常会更多的忽略它。所以即使这个关键词是可选的，也并不意味着在它阻碍了代码可读性时，还要强制去掉它。 请注意，当使用def关键字取代指定具体的类型去定义一个方法时，你可能惊讶的发现最后一个表达式会被返回。所以通常更倾向于使用一个特别指定返回值类型譬如void或者其他类型。在上面的例子中，假设我们忘记了通过最后一个表达式去返回m2，导致最后的表达式变成了m2.c = 3，最后会返回3，而不是你所期望的值。 if / else , try / catch 这些声明同样可以返回一个值，因为这里有“最后一个表达式”在这些声明中： def foo(n) { if(n == 1) { &quot;Roshan&quot; } else { &quot;Dawrani&quot; } }assert foo(1) == &quot;Roshan&quot; assert foo(2) == &quot;Dawrani&quot; 3. Def和类型 当我们讨论def和类型，我经常看到开发者同时使用def和类型。其实def在这里是多余的。所以我们需要对def和类型进行二选一。 不要这样写： def String name = &quot;Guillaume&quot; 而是这样： String name = &quot;Guillaume&quot; 当在Groovy中使用def，对象实际的类型是Object（所以使用def定义的变量可以分配任何类型的对象，同时如果一个方法声明返回def则可以返回任何类型的对象）。 当使用无类型参数定义一个方法，你可以使用def关键字，但是不是必须的，我们更倾向于忽略它。以下方法可以被替代： void doSomething(def param1, def param2) { } 更建议： void doSomething(param1, param2) { } 但是就像上部分文档提到的，通常为了帮助规范化你的代码，为了帮助IDE进行代码提示，或者为了利用Groovy的静态类型检查和静态类型编译，指定方法参数类型是更好的选择。 另外一点，定义构造函数时应该避免使用def： class MyClass { def MyClass() {} } 取而代之，应该移除def： class MyClass { MyClass() {} } 4. 默认的Public 默认情况下，Groovy认为类和方法都是public的。所以你不需要到处重复使用public修饰符。只有当不是public时，你需要指定可见性修饰符。 可以取代以下代码： public class Server { public String toString() { return &quot;a server&quot; } } 更简洁的表达： class Server { String toString() { &quot;a server&quot; } } 你可能想知道包范围的可见性，事实上，Groovy允许忽略'public'意味着这个范围默认情况下不再被支持，这里有一个特殊的Groovy注解允许你使用这个可见性修饰符： class Server { @PackageScope Cluster cluster } 5. 可忽略的括号 Groovy允许你忽略顶级表达式中的括号，譬如println指令： println &quot;Hello&quot; method a, b 对照： println(&quot;Hello&quot;) method(a, b) 当一个闭包是一个方法调用的最后一个参数时，就像使用Groovy的each{}迭代器一样，你可以把闭包放在闭合的括号内，甚至也可以忽略括号： list.each( { println it } ) list.each(){ println it } list.each { println it } 第三种方式是一直推崇的，更自然，一对空括号就像是语句中无用的废话。 在某些情况下括号是必须的，例如在进行嵌套方法调用或调用不带参数的方法时。 def foo(n) { n } def bar() { 1 } println foo 1 // won't work def m = bar // won't work 6. Classes是一等公民 在Groovy中.class后缀不是必须的，有点像Java的instanceof。 举个例子： connection.doPost(BASE_URI + &quot;/modify.hqu&quot;, params, ResourcesResponse.class) 使用GStrings的例子，并使用一等公民的方式替代上面的写法： connection.doPost(&quot;${BASE_URI}/modify.hqu&quot;, params, ResourcesResponse) 7. Getters和Setters方法 在Groovy中，getter和setter形成了我们所说的“属性”，并提供访问和设置此类属性的快捷方法。因此替代Java的调用方式，可以使用类似的字段获取方法： resourceGroup.getResourcePrototype().getName() == SERVER_TYPE_NAME resourceGroup.resourcePrototype.name == SERVER_TYPE_NAME resourcePrototype.setName(&quot;something&quot;) resourcePrototype.name = &quot;something&quot; 当你在Groovy中写你的beans（经常被叫做POGO (Plain Old Groovy Object)）时，你不必再自行创建字段的getter/setter的方法，这些将由Groovy编译器来帮你做。 可以替代这些： class Person { private String name String getName() { return name } void setName(String name) { this.name = name } } 简化为： class Person { String name } 正如你所看到的，一个没有可见修饰符的字段实际上会由Groovy编译器去生成一个私有的字段以及getter/setter方法。 在Java中使用此类POGO时，getter和setter方法实际存在，可以照常使用。 尽管编译器创建了通常的getter/setter逻辑，但如果你希望在这些getter/setter中做任何额外或不同的事情，你仍然可以自由地提供它们，编译器将使用你的逻辑，而不是默认生成的逻辑。 8. 使用命名参数和默认构造函数初始化 bean 举一个bean的例子： class Server { String name Cluster cluster } 而不是在后续语句中设置每个 setter，如下所示： def server = new Server() server.name = &quot;Obelix&quot; server.cluster = aCluster 你可以将命名参数与默认构造函数一起使用（首先调用构造函数，然后按照在映射中指定的顺序调用设置）： def server = new Server(name: &quot;Obelix&quot;, cluster: aCluster) 9. 使用 with() 和 tap() 对同一个 bean 进行重复操作 当你创建一个新的实例的时候，使用名称-参数的默认构造方法是有趣的，但是如果你在更新一个实例的时候，你是否不得不重复'server'前缀？不，感谢Groovy为所有类型对象提供的with()和tap()方法： server.name = application.name server.status = status server.sessionCount = 3 server.start() server.stop() 对比： server.with { name = application.name status = status sessionCount = 3 start() stop() } 与 Groovy 中的任何闭包一样，最后一条语句被视为返回值。在上面的例子中，返回的结果是stop()。想要使用这种方式构建只返回传入的对象，可以使用tap()方法： def person = new Person().with { name = &quot;Ada Lovelace&quot; it // Note the explicit mention of it as the return value } 对比： def person = new Person().tap { name = &quot;Ada Lovelace&quot; } 注意：你也可以使用 with(true) 代替 tap() 和 with(false) 代替 with() 。 10. Equals和== Java的==实际上等同于Groovy的is()方法，Groovy的==是更智能的equals()！ 在比较对象引用的过程中，需要使用a.is(b)去替代==。 但是比起使用equals()比较，你应该更喜欢Groovy的==，并且它对NullPointerException进行了规避处理，与为null变量在左边和在右边无关。 为了替代如下： status != null &amp;&amp; status.equals(ControlConstants.STATUS_COMPLETED) 可以这么做： status == ControlConstants.STATUS_COMPLETED 11. GStrings（插值，多行） 我们在Java中经常使用字符串和变量连接，导致有许多开闭的双引号、加号、以及\\n换行符。通过插值字符串（叫做GStrings），这样的字符串看起来更友好，输入起来也不那么痛苦： throw new Exception(&quot;Unable to convert resource: &quot; + resource) 对比： throw new Exception(&quot;Unable to convert resource: ${resource}&quot;) 在大括号里面，你可以放入任何类型的表达式，而不仅仅是变量。譬如，简单的变量、variable.property，你甚至可以去掉大括号： throw new Exception(&quot;Unable to convert resource: $resource&quot;) 你甚至可以通过闭包${-&gt; resource }来延迟执行这些表达式。当GString被强制转换为String时，它将执行闭包然后调用toString()方法来表示返回值。 举例： int i = 3def s1 = &quot;i's value is: ${i}&quot; def s2 = &quot;i's value is: ${-&gt; i}&quot; i++assert s1 == &quot;i's value is: 3&quot; // eagerly evaluated, takes the value on creation assert s2 == &quot;i's value is: 4&quot; // lazily evaluated, takes the new value into account 当字符串及其连接的表达式在Java中很长时： throw new PluginException(&quot;Failed to execute command list-applications:&quot; + &quot; The group with name &quot; + parameterMap.groupname[0] + &quot; is not compatible group of type &quot; + SERVER_TYPE_NAME) 您可以使用\\连续字符（这不是多行字符串）： throw new PluginException(&quot;Failed to execute command list-applications: \\ The group with name ${parameterMap.groupname[0]} \\ is not compatible group of type ${SERVER_TYPE_NAME}&quot;) 或者使用带三引号的多行字符串： throw new PluginException(&quot;&quot;&quot;Failed to execute command list-applications: The group with name ${parameterMap.groupname[0]} is not compatible group of type ${SERVER_TYPE_NAME)}&quot;&quot;&quot;) 您还可以通过在该字符串上调用 .stripIndent() 来去除出现在多行字符串左侧的缩进。 还要注意单引号和双引号在Groovy中的不同：单引号一直被拿来创建Java不包含插值的字符串，然而当存在插值时，双引号将会创建Java字符串或者GStrings。 对于多行字符串，你可以使用三重引号：即 GStrings 的三重双引号和纯字符串的三重单引号。 如果你需要写正则表达式，你可以使用“斜线”字符串表示法： assert &quot;foooo/baaaaar&quot; ==~ /fo+\\/ba+r/ “斜线”表示法的优点是您不需要双转义反斜杠，使使用正则表达式更简单。 最后，当你需要字符串常量的时候更建议使用单引号，当你需要依赖字符串插值的时候要使用双引号。 12. 数据结构的原生语法 Groovy为lists、maps、regex或ranges提供了原生的数据结构语法。确保在你的Groovy程序中去使用它。 这些原生数据结构的例子： def list = [1, 4, 6, 9]// by default, keys are Strings, no need to quote them // you can wrap keys with () like [(variableStateAcronym): stateName] to insert a variable or object as a key. def map = [CA: 'California', MI: 'Michigan']// ranges can be inclusive and exclusive def range = 10..20 // inclusive assert range.size() == 11 // use brackets if you need to call a method on a range definition assert (10..&lt;20).size() == 10 // exclusivedef pattern = ~/fo*/// equivalent to add() list &lt;&lt; 5// call contains() assert 4 in list assert 5 in list assert 15 in range // subscript notation assert list[1] == 4// add a new key value pair map &lt;&lt; [WA: 'Washington'] // subscript notation assert map['CA'] == 'California' // property notation assert map.WA == 'Washington'// matches() strings against patterns assert 'foo' ==~ pattern 13. Groovy开发工具包 继续谈论这些数据结构，当你去遍历集合时，Groovy提供了很多的附加方法来装饰Java代码的数据结构，譬如each{} , find{} , findAll{} , every{} , collect{} , inject{}。这些方法为编程语言提供了新的功能特性，能够更简单的来实现复杂的算法。由于语言的动态特性，许多新方法通过修饰应用于各种类型。你可以发现很多非常有用的方法在String, Files, Streams, Collections等数据结构中： http://groovy-lang.org/gdk.html 14. 强大的switch Groovy的switch比起直接只受原语和同化的C-ish语言更加强大。Groovy的switch接收几乎任何类型的参数。 def x = 1.23 def result = &quot;&quot; switch (x) { case &quot;foo&quot;: result = &quot;found foo&quot; // lets fall through case &quot;bar&quot;: result += &quot;bar&quot; case [4, 5, 6, 'inList']: result = &quot;list&quot; break case 12..30: result = &quot;range&quot; break case Integer: result = &quot;integer&quot; break case Number: result = &quot;number&quot; break case { it &gt; 3 }: result = &quot;number &gt; 3&quot; break default: result = &quot;default&quot; } assert result == &quot;number&quot; 更进一步来讲，带有 isCase() 方法的类型也可以决定一个值是否与一个case相对应。 15. Import别名 在Java中，当你使用两个不同包但同名的类时，譬如java.util.List和java.awt.List，你只能导入一个类，然后不得不通过使用完全限定名来使用另外一个类。 有时候在代码中，多个长类名的调用可能显得代码冗长且降低了代码的清晰度。 为了改善这些场景，Groovy提供了导入别名的特性： import java.util.List as UtilList import java.awt.List as AwtList import javax.swing.WindowConstants as WC UtilList list1 = [WC.EXIT_ON_CLOSE] assert list1.size() instanceof Integer def list2 = new AwtList() assert list2.size() instanceof java.awt.Dimension 你也可以在导入静态方法时使用别名： import static java.lang.Math.abs as mabs assert mabs(-4) == 4 16. Groovy的真值 所有的对象都可以被强转为boolean值：所有的可能值为null , void , 等于0或者空则为false , 否则为true。 替代下面的写法： if (name != null &amp;&amp; name.length &gt; 0) {} 你可以仅仅这样做： if (name) {} 集合也是如此。 因此你可以在while() , if() , 三元运算符, Elvis表达式等里面使用一些简短的表达。 甚至于，通过在你的类中添加一个boolean类型的asBoolean()方法，去自定义Groovy的真值！ 17. 安全视图导航 Groovy支持对象.操作的安全变体。 在Java中，当你对对象中一个子字段感兴趣，必须通过校验null来避免空指针，最后不得不便携复杂的if或者if嵌套： if (order != null) { if (order.getCustomer() != null) { if (order.getCustomer().getAddress() != null) { System.out.println(order.getCustomer().getAddress()); } } } 通过?.来安全的访问子对象，你可以像如下一样简化代码： println order?.customer?.address 在调用链中，空指针将被校验，如果有任何元素为null将不会有NullPointerException异常抛出。如果有某个元素为null，则结果返回为null。 18. Assert断言 你可以使用assert语句来校验你的参数、返回值或者更多场景。 跟Java的assert相反，assert不需要被激活便能够被使用，所以assert总是可以被检查的。 def check(String name) { // name non-null and non-empty according to Groovy Truth assert name // safe navigation + Groovy Truth to check assert name?.size() &gt; 3 } 你还会注意到 Groovy 的“Power Assert”语句提供的良好输出，其中包含每个被断言的子表达式的各种值的图形视图。 19. 使用Elvis表达式处理默认值 Elvis是一个用来处理默认值的简短的三元表达式。 我们通常这样写代码： def result = name != null ? name : &quot;Unknown&quot; 真心感谢Groovy，null的检查可以简短到只有'name'。 更进一步，既然你无论如何都返回 'name'，而不是在这个三元表达式中重复name两次，我们可以通过使用Elvis运算符以某种方式删除问号和冒号之间的内容，这样上面就变成了： def result = name ?: &quot;Unknown&quot; 20. Catch任何的异常 如果你不是真的很在意代码try块中的异常的类型，你可以简单地捕获它们中的任何一个并简单地省略捕获的异常的类型。所以替代下面的代码： try { // ... } catch (Exception t) { // something bad happens } 使用catch ('any' 或者 'all', 或者你认为的任何异常)： try { // ... } catch (any) { // something bad happens } 请注意，它捕获所有异常，而不是 Throwable。如果你需要真正捕捉“所有东西”，你必须明确地说你想捕捉 Throwable。 21. 对可选类型的建议 我将完成一些关于怎么用和什么时候用可选类型的内容。Groovy让你决定是否去使用强类型还是def。 我有一个相当简单的经验法则：每当你的代码被当作公有API使用时，你应该总是青睐使用强类型，这将帮助你确定更健壮的约束，以避免可能通过错误的参数类型，也能够提供更好的文档，还能帮助IDE去进行代码补全。每当代码仅供你自己使用时，像私有方法，或者当IDE可以轻易推断类型时，你就可以更自由地决定何时使用可选类型。 原文档链接 ","link":"https://jobslee0.github.io/post/groovy-bian-cheng-feng-ge-zhi-nan/"}]}